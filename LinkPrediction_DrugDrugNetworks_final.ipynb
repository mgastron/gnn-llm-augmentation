{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOnGH8jsl5NL"
      },
      "source": [
        "[ Related Medium post: https://medium.com/stanford-cs224w/gnn-based-link-prediction-in-drug-drug-interaction-networks-c0e2136e4a72 ]\n",
        "\n",
        "# GNN-Based Link Prediction in Drug-Drug Interaction Networks\n",
        "Welcome! This colab serves as a tutorial for using Graph Machine Learning to perform Link Prediction in the drug-drug interaction dataset. Particularly, we focus on applying **GraphSage**, a type of **Graph Neural Network (GNN)**, to the `ogbl-ddi` dataset.\n",
        "\n",
        "Over the course of this colab, we will begin development from the ground-up: from downloading the dataset, to writing training code to prepare a base model using [PyTorch Geometric (PyG)](https://pytorch-geometric.readthedocs.io/en/latest/), to implementing increasingly advanced techniques to try and improve our performance. We would strongly encourage you to make use of GPU runtime as you go through this notebook. Now, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W57vvc3jEscl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzB9tYiDGcts"
      },
      "source": [
        "### Installation\n",
        "\n",
        "We first will install PyG as well as [ogb](https://github.com/snap-stanford/ogb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T9Tgj6bFGf5E",
        "outputId": "19e0131f-3694-4fd7-cd6f-cef388278c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch has version 2.0.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aNe4asLBGtGn",
        "outputId": "876f43ea-0a45-4976-e86f-3de2e17b2ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1.html\n",
            "Requirement already satisfied: torch-scatter in ./venv_llm_clean/lib/python3.11/site-packages (2.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1.html\n",
            "Requirement already satisfied: torch-sparse in ./venv_llm_clean/lib/python3.11/site-packages (0.6.18)\n",
            "Requirement already satisfied: scipy in ./venv_llm_clean/lib/python3.11/site-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./venv_llm_clean/lib/python3.11/site-packages (from scipy->torch-sparse) (1.24.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: torch-geometric in ./venv_llm_clean/lib/python3.11/site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (3.11.18)\n",
            "Requirement already satisfied: fsspec in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (1.24.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (7.0.0)\n",
            "Requirement already satisfied: pyparsing in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./venv_llm_clean/lib/python3.11/site-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv_llm_clean/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv_llm_clean/lib/python3.11/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->torch-geometric) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: ogb in ./venv_llm_clean/lib/python3.11/site-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (1.24.1)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: six>=1.12.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (2.4.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in ./venv_llm_clean/lib/python3.11/site-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in ./venv_llm_clean/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (78.1.0)\n",
            "Requirement already satisfied: littleutils in ./venv_llm_clean/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in ./venv_llm_clean/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv_llm_clean/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv_llm_clean/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./venv_llm_clean/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./venv_llm_clean/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./venv_llm_clean/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv_llm_clean/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in ./venv_llm_clean/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in ./venv_llm_clean/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (4.13.2)\n",
            "Requirement already satisfied: sympy in ./venv_llm_clean/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (1.13.3)\n",
            "Requirement already satisfied: networkx in ./venv_llm_clean/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./venv_llm_clean/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv_llm_clean/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv_llm_clean/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv_llm_clean/lib/python3.11/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP4RsBSkG6uA"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LHZchSNMEG7U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matigasstron/Documents/TFM/venv_llm_clean/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import random\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import negative_sampling, convert, to_dense_adj\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EpUQKewJr3V"
      },
      "source": [
        "## Dataset\n",
        "As mentioned above, the dataset that we are working with is [ogbl-ddi](https://ogb.stanford.edu/docs/linkprop/#ogbl-ddi). Briefly, in the homogenous, feature-less, undirected graph, each node represents a drug. Edges between nodes represent interactions between the drugs, where the joint effect of taking both drugs is markedly different than the expected effects if either drug was taken independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktfQQiDzJuUp"
      },
      "source": [
        "### Installation and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgmXT6IaSOyN"
      },
      "source": [
        "The `ogb` library makes downloading an official ogb dataset, like ogbl-ddi, incredibly easy.\n",
        "\n",
        "Below, we download the (non-sparse) version of the dataset and explore it using NetworkX, a popular Python library for working with graphs/networks. To do so, we leverage PyG utilities to convert the dataset into a NetworkX graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tOlunvuJtMs",
        "outputId": "096aaf19-5bbb-407d-b1e8-ff89dc7c22ab"
      },
      "outputs": [],
      "source": [
        "dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
        "data = dataset[0]\n",
        "G = convert.to_networkx(data, to_undirected=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xicAHoShr6t4"
      },
      "source": [
        "We know gather some statistics about the graph using NetworkX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whiT-BQ9Tnh0",
        "outputId": "db16d618-9960-4aee-e337-9309dc317705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2135822\n",
            "ogbl-ddi has 4267 nodes and 1067911 edges (*), with an average node degree of 501\n"
          ]
        }
      ],
      "source": [
        "num_nodes, num_edges = G.number_of_nodes(), G.number_of_edges()\n",
        "print(data.num_edges)\n",
        "# (*) means that this number refers to the edges that are considered as train\n",
        "# in the split (80% train - 10% valid - 10% test).\n",
        "# This total number of edges is 100/80 times this number\n",
        "print(f'ogbl-ddi has {num_nodes} nodes and {num_edges} edges (*), with an average node degree of {round(2 * num_edges / num_nodes)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVFhrsILsJOv"
      },
      "source": [
        "We wrap up our exploration of this graph by visualizing a subgraph of it (feel free to adjust `num_nodes_to_sample` to see more or less of the graph, though anything above 1000 will take a bit of time to render)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "sNNN3VpQLGVI",
        "outputId": "359ed686-4ac2-4c12-de0e-39bb8d8934f8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_nodes_to_sample = 400\n",
        "sampled_nodes = random.sample(list(G.nodes), num_nodes_to_sample)\n",
        "sampled_graph = G.subgraph(sampled_nodes)\n",
        "nx.draw(sampled_graph, with_labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5r83nHlZCGR"
      },
      "source": [
        "Now, it's time to download the sparse version of the graph that we are actually going to make use of when developing models. We will use this version of the dataset to get our edge splits (train, validation, test).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdctAmeL4gt",
        "outputId": "0055fbb6-74c5-4f25-cb7b-6d17e9e5122e"
      },
      "outputs": [],
      "source": [
        "dataset = PygLinkPropPredDataset(name='ogbl-ddi',\n",
        "                                     transform=T.ToSparseTensor())\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = dataset[0]\n",
        "adj_t = data.adj_t.to(device)\n",
        "\n",
        "# From https://ogb.stanford.edu/docs/linkprop/:\n",
        "# We develop a protein-target split, meaning that we split drug edges according\n",
        "# to what proteins those drugs target in the body. As a result, the test set\n",
        "# consists of drugs that predominantly bind to different proteins from drugs in\n",
        "# the train and validation sets. This means that drugs in the test set work\n",
        "# differently in the body, and have a rather different biological mechanism of\n",
        "# action than drugs in the train and validation sets. The protein-target split\n",
        "# thus enables us to evaluate to what extent the models can generate practically\n",
        "# useful predictions, i.e., non-trivial predictions that are not hindered by the\n",
        "# assumption that there exist already known and very similar medications\n",
        "# available for training.\n",
        "\n",
        "# According to the documentation, the total number of edges is 1334889, which is\n",
        "# exactly 1067911+133489+133489 (train+valid+test)\n",
        "# However, the graph stats (the num_edges attribute) indicate that the TOTAL\n",
        "# number of edges (not just the train split) is 1067911\n",
        "# My interpretation is that the grph as it is downloaded only contains the\n",
        "# training edges; however, the get_edge_split function somehow adds the valid\n",
        "# and test positive edges, together with the valid and test negative edges.\n",
        "split_edge = dataset.get_edge_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr-xTH1WC7TP",
        "outputId": "6a2e2c45-7fec-4ee7-a4fa-d2de50960782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ogbl-ddi has 4267 nodes and 2135822 (directed) edges, with an average node degree of 501\n",
            "Number of training edges: 1067911\n",
            "Number of validation edges: 133489 positive, 101882 negative\n",
            "Number of test edges: 133489 positive, 95599 negative\n"
          ]
        }
      ],
      "source": [
        "print(f'ogbl-ddi has {data.num_nodes} nodes and {data.num_edges} (directed) edges, with an average node degree of {round(data.num_edges / data.num_nodes)}')\n",
        "print(f\"Number of training edges: {split_edge['train']['edge'].shape[0]}\")\n",
        "print(f\"Number of validation edges: {split_edge['valid']['edge'].shape[0]} positive, {split_edge['valid']['edge_neg'].shape[0]} negative\")\n",
        "print(f\"Number of test edges: {split_edge['test']['edge'].shape[0]} positive, {split_edge['test']['edge_neg'].shape[0]} negative\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Negativos generados: torch.Size([1067911, 2])\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "train_edge_index = split_edge['train']['edge'].t()\n",
        "neg_edge_index = negative_sampling(\n",
        "    edge_index=train_edge_index,\n",
        "    num_nodes=data.num_nodes,\n",
        "    num_neg_samples=train_edge_index.size(1)\n",
        ")\n",
        "\n",
        "split_edge['train']['edge_neg'] = neg_edge_index.t()\n",
        "\n",
        "print(f\"✅ Negativos generados: {split_edge['train']['edge_neg'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSuG-U_mHBID"
      },
      "source": [
        "Finally, as ogbl-ddi has no node features, we will initialize our own (constant) initial embeddings for all the nodes. We will make use of `emb` once we get into the thick of training our own model below. Note, to be more inductive, we have opted not to make our embeddings learned parameters, unlike prior work with ogbl-ddi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rqFrZatjZH4H"
      },
      "outputs": [],
      "source": [
        "emb = torch.ones(num_nodes, 1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKPW29aNAC3Z"
      },
      "source": [
        "## Base GraphSage Model\n",
        "\n",
        "Now that we have our dataset downloaded, we can begin building our baseline model using PyG!\n",
        "\n",
        "Our base model consists of (1) the GraphSage GNN for generating node embeddings (2) a basic LinkPredictor that simply does a dot product between the two node embeddings followed by a sigmoid. Notice that we are leveraging the built-in [SAGEConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SAGEConv) layer from PyG, which allows us to quickly get our model up and running. Our base model is heavily inspired by the ogb sample code for ogbl-ddi, which can be found here: https://github.com/snap-stanford/ogb/blob/master/examples/linkproppred/ddi/gnn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zPbkWxBrAF-d"
      },
      "outputs": [],
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "              dropout, aggr=\"add\"):\n",
        "    super(SAGE, self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.convs.append(SAGEConv(in_channels, hidden_channels, normalize=True, aggr=aggr))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(SAGEConv(hidden_channels, hidden_channels, normalize=True, aggr=aggr))\n",
        "    self.convs.append(SAGEConv(hidden_channels, out_channels, normalize=True, aggr=aggr))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, adj_t):\n",
        "    for conv in self.convs[:-1]:\n",
        "      x = conv(x, adj_t)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, adj_t)\n",
        "    return x\n",
        "\n",
        "class DotProductLinkPredictor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DotProductLinkPredictor, self).__init__()\n",
        "\n",
        "  def forward(self, x_i, x_j):\n",
        "    out = (x_i*x_j).sum(-1)\n",
        "    return torch.sigmoid(out)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhUkI6C-Ho9F"
      },
      "source": [
        "To help familiarize readers with how the GNN and LinkPredictor work, below we apply the untrained model to the training graph and use the output node embeddings to make link predictions. While the results will be terrible, it does give a good sense of how to make use of these PyG-based models.\n",
        "\n",
        "We have chosen a `hidden_dimension` / embedding dimensionality of 256. As the diameter of the graph (computed using NetworkX [diameter](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.distance_measures.diameter.html) method; not included in this Colab as it takes a long time to run) is 5 and a general rule of thumb says to keep the receptive field of the GNN slightly higher than that, we have chosen a 7-layer GraphSage model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qq7xYcDf7X",
        "outputId": "bec4291f-4512-4cee-d572-5632ec117a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAGE(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(1, 256, aggr=add)\n",
            "    (1-6): 6 x SAGEConv(256, 256, aggr=add)\n",
            "  )\n",
            ")\n",
            "DotProductLinkPredictor()\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matigasstron/Documents/TFM/venv_llm_clean/lib/python3.11/site-packages/torch_sparse/tensor.py:574: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:56.)\n",
            "  return torch.sparse_csr_tensor(rowptr, col, value, self.sizes())\n"
          ]
        }
      ],
      "source": [
        "# Initialize our model and LinkPredictor\n",
        "hidden_dimension = 256\n",
        "model = SAGE(1, hidden_dimension, hidden_dimension, 7, 0.5).to(device)\n",
        "print(model)\n",
        "\n",
        "predictor = DotProductLinkPredictor().to(device)\n",
        "print(predictor)\n",
        "\n",
        "# Run our initial \"node features\" through the GNN to get node embeddings\n",
        "model.eval()\n",
        "predictor.eval()\n",
        "h = model(emb, adj_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBCZGBy_3S0G"
      },
      "source": [
        "##Training and Evaluation\n",
        "\n",
        "Now we will define functions that we will use for training and evaluating our models! Once again, we took much inspiration from https://github.com/snap-stanford/ogb/blob/master/examples/linkproppred/ddi/gnn.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgm4b9LgjNpg"
      },
      "source": [
        "### Training Drivers\n",
        "We leverage a fairly standard PyTorch training loop. For each batch of positive edges from our initial training split, we leverage PyG's [negative_sampling](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=negative_sampling#torch_geometric.utils.negative_sampling) utility to create an equivalently sized batch of negative edges (i.e. edges NOT present in the training graph). The complete training batch thus contains an equal amount of positive and negative edges.\n",
        "\n",
        "For every batch, we generate node embeddings using all the training edges. We then make link predictions for the current batch and compute our loss, from which we backpropagate  to update our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TqXKRXKqI5Gn"
      },
      "outputs": [],
      "source": [
        "def create_train_batch(all_pos_train_edges, perm, edge_index):\n",
        "  # First, we get our positive edges, reshaping them to the form (2, hidden_dimension)\n",
        "  pos_edges = all_pos_train_edges[perm].t().to(device)\n",
        "\n",
        "  # We then sample the negative edges using PyG functionality\n",
        "  neg_edges = negative_sampling(edge_index, num_nodes=num_nodes,\n",
        "                                num_neg_samples=perm.shape[0], method='dense').to(device)\n",
        "\n",
        "  # Our training batch is just the positive edges concatanted with the negative ones\n",
        "  train_edges = torch.cat([pos_edges, neg_edges], dim=1)\n",
        "\n",
        "  # Our labels are all 1 for the positive edges and 0 for the negative ones\n",
        "  pos_labels = torch.ones(pos_edges.shape[1], )\n",
        "  neg_labels = torch.zeros(neg_edges.shape[1], )\n",
        "  train_labels = torch.cat([pos_labels, neg_labels], dim=0).to(device)\n",
        "\n",
        "  return train_edges, train_labels\n",
        "\n",
        "def train(model, predictor, x, adj_t, split_edge, loss_fn, optimizer, batch_size, num_epochs, edge_model=False, spd=None):\n",
        "  # adj_t isn't used everywhere in PyG yet, so we switch back to edge_index for negative sampling\n",
        "  row, col, edge_attr = adj_t.t().coo()\n",
        "  edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "  model.train()\n",
        "  predictor.train()\n",
        "\n",
        "  model.reset_parameters()\n",
        "  predictor.reset_parameters()\n",
        "\n",
        "  all_pos_train_edges = split_edge['train']['edge']\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_total_loss = 0\n",
        "    for perm in DataLoader(range(all_pos_train_edges.shape[0]), batch_size, shuffle=True):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_edge, train_label = create_train_batch(all_pos_train_edges, perm, edge_index)\n",
        "\n",
        "      # Use the GNN to generate node embeddings\n",
        "      if edge_model:\n",
        "        h = model(x, edge_index, spd)\n",
        "      else:\n",
        "        h = model(x, adj_t)\n",
        "\n",
        "      # Get predictions for our batch and compute the loss\n",
        "      preds = predictor(h[train_edge[0]], h[train_edge[1]])\n",
        "      loss = loss_fn(preds, train_label)\n",
        "\n",
        "      epoch_total_loss += loss.item()\n",
        "\n",
        "      # Update our parameters\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch} has loss {round(epoch_total_loss, 4)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyJykFrgjRu5"
      },
      "source": [
        "### Evaluation Drivers\n",
        "Our evaluation code is also fairly standard PyTorch. As our validation and test splits from ogb already come with negative edges, we are able to easily leverage them instead of generating our own.\n",
        "\n",
        "Like with the training code, we use the training graph to generate node embeddings using our GNN. We then calculate predictions using the LinkPredictor for our positive and negative edges. Although we calculate accuracy ourselves, we leverage the [Evaluator](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/linkproppred/evaluate.py#L11) provided by the ogb library for ogbl-ddi to calculate Hits@K. Though we demonstrate how to calculate Hits@K for several values of K, in keeping with prior work, we focus on Hits@20 as our primary metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1c94k9T5F71i",
        "outputId": "b5b7aed9-75af-48a2-e88b-3d540a97aa9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Expected input format of Evaluator for ogbl-ddi\n",
            "{'y_pred_pos': y_pred_pos, 'y_pred_neg': y_pred_neg}\n",
            "- y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\n",
            "- y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\n",
            "y_pred_pos is the predicted scores for positive edges.\n",
            "y_pred_neg is the predicted scores for negative edges.\n",
            "Note: As the evaluation metric is ranking-based, the predicted scores need to be different for different edges.\n"
          ]
        }
      ],
      "source": [
        "def accuracy(pred, label):\n",
        "  pred_rounded = torch.round(pred)\n",
        "  accu = torch.eq(pred_rounded, label).sum() / label.shape[0]\n",
        "  accu = round(accu.item(), 4)\n",
        "  return accu\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, x, adj_t, split_edge, evaluator, batch_size, edge_model=False, spd=None):\n",
        "  model.eval()\n",
        "  predictor.eval()\n",
        "\n",
        "  if model is None or isinstance(model, DummyModel):\n",
        "    # Ya tenemos los embeddings precomputados en x\n",
        "    h = x\n",
        "  elif edge_model:\n",
        "      row, col, edge_attr = adj_t.t().coo()\n",
        "      edge_index = torch.stack([row, col], dim=0)\n",
        "      h = model(x, edge_index, spd)\n",
        "  else:\n",
        "      h = model(x, adj_t)\n",
        "\n",
        "\n",
        "  pos_eval_edge = split_edge['edge'].to(device) # ALL positive edges? Just validation (look at the actual parameter)\n",
        "  neg_eval_edge = split_edge['edge_neg'].to(device) # ALL negative edges? Just validation\n",
        "\n",
        "  pos_eval_preds = []\n",
        "  for perm in DataLoader(range(pos_eval_edge.shape[0]), batch_size):\n",
        "    edge = pos_eval_edge[perm].t()\n",
        "    pos_eval_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "  pos_eval_pred = torch.cat(pos_eval_preds, dim=0)\n",
        "\n",
        "  neg_eval_preds = []\n",
        "  for perm in DataLoader(range(neg_eval_edge.size(0)), batch_size):\n",
        "    edge = neg_eval_edge[perm].t()\n",
        "    neg_eval_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "  neg_eval_pred = torch.cat(neg_eval_preds, dim=0)\n",
        "\n",
        "  total_preds = torch.cat((pos_eval_pred, neg_eval_pred), dim=0)\n",
        "  labels = torch.cat((torch.ones_like(pos_eval_pred), torch.zeros_like(neg_eval_pred)), dim=0)\n",
        "  acc = accuracy(total_preds, labels)\n",
        "\n",
        "  results = {}\n",
        "  for K in [10, 20, 30, 40, 50]:\n",
        "    evaluator.K = K\n",
        "    valid_hits = evaluator.eval({\n",
        "      'y_pred_pos': pos_eval_pred,\n",
        "      'y_pred_neg': neg_eval_pred,\n",
        "    })[f'hits@{K}']\n",
        "    results[f'Hits@{K}'] = (valid_hits)\n",
        "    print(f\"  {round(valid_hits*len(pos_eval_pred))} positive edges (out of {len(pos_eval_pred)}) are ranked into the {K} (out of {len(neg_eval_pred)}) top-ranked negative edges\")\n",
        "  results['Accuracy'] = acc\n",
        "  # This metric does not seem to be supported by the Evaluator\n",
        "  #results['Rocauc'] = evaluator.eval({\n",
        "  #    'y_pred_pos': pos_eval_pred,\n",
        "  #    'y_pred_neg': neg_eval_pred,\n",
        "  #  })['rocauc']\n",
        "\n",
        "  return results\n",
        "\n",
        "eval = Evaluator(name='ogbl-ddi')\n",
        "# ogb Evaluators can be invoked to get their expected format\n",
        "print(eval.expected_input_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MCZ15ttonx1R"
      },
      "outputs": [],
      "source": [
        "# Prints the prediction for n_pred random positive edges and n_pred random negative edges\n",
        "def pick_random_predictions(model,n_pred):\n",
        "  model.eval()\n",
        "  predictor.eval()\n",
        "\n",
        "  n_correct_pos = 0\n",
        "  n_incorrect_pos = 0\n",
        "  n_correct_neg = 0\n",
        "  n_incorrect_neg = 0\n",
        "\n",
        "  h = model(emb, adj_t)\n",
        "\n",
        "  # Randomly sample some validation edges and pass them through our basic predictor\n",
        "  idx = torch.randperm(split_edge['valid']['edge'].size(0))[:n_pred]\n",
        "  edges = split_edge['valid']['edge'][idx].t()\n",
        "\n",
        "  print(f\"Prediction for {n_pred} random positive validation edges:\")\n",
        "  for i in idx:\n",
        "    e0 = split_edge['valid']['edge'][i].t()[0]\n",
        "    e1 = split_edge['valid']['edge'][i].t()[1]\n",
        "    y = predictor(h[e0], h[e1]).item()\n",
        "    if y > 0.5:\n",
        "      n_correct_pos += 1\n",
        "    else:\n",
        "      n_incorrect_pos += 1\n",
        "    print(f\"    ({e0}, {e1}) -> {y}\")\n",
        "\n",
        "  print(f\"  Correct: {n_correct_pos}; Incorrect: {n_incorrect_pos}; Accuracy: {n_correct_pos/(n_correct_pos+n_incorrect_pos)}\")\n",
        "\n",
        "  # Randomly sample some validation edges and pass them through our basic predictor\n",
        "  idx = torch.randperm(split_edge['valid']['edge_neg'].size(0))[:n_pred]\n",
        "  edges = split_edge['valid']['edge_neg'][idx].t()\n",
        "\n",
        "  print(f\"Prediction for {n_pred} random negative validation edges:\")\n",
        "  for i in idx:\n",
        "    e0 = split_edge['valid']['edge_neg'][i].t()[0]\n",
        "    e1 = split_edge['valid']['edge_neg'][i].t()[1]\n",
        "    y = predictor(h[e0], h[e1]).item()\n",
        "    if y > 0.5:\n",
        "      n_incorrect_neg += 1\n",
        "    else:\n",
        "      n_correct_neg += 1\n",
        "    print(f\"    ({e0}, {e1}) -> {y}\")\n",
        "\n",
        "  print(f\"  Correct: {n_correct_neg}; Incorrect: {n_incorrect_neg}; Accuracy: {n_correct_neg/(n_correct_neg+n_incorrect_neg)}\")\n",
        "\n",
        "  print()\n",
        "  print(f\"  Correct: {n_correct_pos+n_correct_neg}; Incorrect: {n_incorrect_pos+n_incorrect_neg}; Accuracy: {(n_correct_pos+n_correct_neg)/(n_correct_pos+n_correct_neg+n_incorrect_pos+n_incorrect_neg)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODWt6H9ja4y"
      },
      "source": [
        "### Train and Evaluate Baseline Model\n",
        "Now we are ready to kick-off training of our base model and evaluate it with our validation set. As we will do with all of our models, we make use of the Adam optimizer and Binary Cross-Entropy loss. Readers should feel free to experiment with other optimizers/loss functions as they see fit.\n",
        "\n",
        "*This cell will take around 2-3 minutes to run.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhlaEN5i7Ac",
        "outputId": "984d574f-588d-4d05-a197-31e11ffecae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8905\n",
            "Epoch 1 has loss 10.3882\n",
            "Epoch 2 has loss 10.2227\n",
            "Epoch 3 has loss 10.1212\n",
            "Epoch 4 has loss 10.0427\n",
            "Epoch 5 has loss 9.998\n",
            "Epoch 6 has loss 9.8534\n",
            "Epoch 7 has loss 9.795\n",
            "Epoch 8 has loss 9.6787\n",
            "Epoch 9 has loss 9.7671\n",
            "Epoch 10 has loss 9.6125\n",
            "Epoch 11 has loss 9.5647\n",
            "Epoch 12 has loss 9.5835\n",
            "Epoch 13 has loss 9.5524\n",
            "Epoch 14 has loss 9.5112\n",
            "Epoch 15 has loss 9.5074\n",
            "Epoch 16 has loss 9.5075\n",
            "Epoch 17 has loss 9.4914\n",
            "Epoch 18 has loss 9.5171\n",
            "Epoch 19 has loss 9.5017\n",
            "Epoch 20 has loss 9.4727\n",
            "Epoch 21 has loss 9.4602\n",
            "Epoch 22 has loss 9.4674\n",
            "Epoch 23 has loss 9.4538\n",
            "Epoch 24 has loss 9.4679\n",
            "Epoch 25 has loss 9.4459\n",
            "Epoch 26 has loss 9.4415\n",
            "Epoch 27 has loss 9.5168\n",
            "Epoch 28 has loss 9.4694\n",
            "Epoch 29 has loss 9.4313\n",
            "Epoch 30 has loss 9.4317\n",
            "Epoch 31 has loss 9.4226\n",
            "Epoch 32 has loss 9.4419\n",
            "Epoch 33 has loss 9.4218\n",
            "Epoch 34 has loss 9.4409\n",
            "Epoch 35 has loss 9.4268\n",
            "Epoch 36 has loss 9.4281\n",
            "Epoch 37 has loss 9.4087\n",
            "Epoch 38 has loss 9.4197\n",
            "Epoch 39 has loss 9.4144\n",
            "Epoch 40 has loss 9.4081\n",
            "Epoch 41 has loss 9.4206\n",
            "Epoch 42 has loss 9.4387\n",
            "Epoch 43 has loss 9.4257\n",
            "Epoch 44 has loss 9.4265\n",
            "Epoch 45 has loss 9.4209\n",
            "Epoch 46 has loss 9.4166\n",
            "Epoch 47 has loss 9.3963\n",
            "Epoch 48 has loss 9.4068\n",
            "Epoch 49 has loss 9.4111\n",
            "  0 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "{'Hits@10': 0.0, 'Hits@20': 0.0, 'Hits@30': 0.0, 'Hits@40': 0.0, 'Hits@50': 2.9965015844002127e-05, 'Accuracy': 0.7764}\n",
            "Prediction for 30 random positive validation edges:\n",
            "    (1278, 908) -> 0.7196424007415771\n",
            "    (1108, 1622) -> 0.634114146232605\n",
            "    (173, 1585) -> 0.6942532658576965\n",
            "    (1111, 747) -> 0.7271525859832764\n",
            "    (4176, 1098) -> 0.7254911661148071\n",
            "    (2953, 2526) -> 0.7265273332595825\n",
            "    (3811, 390) -> 0.6928901076316833\n",
            "    (2651, 2883) -> 0.7210003733634949\n",
            "    (3454, 2610) -> 0.7289901971817017\n",
            "    (44, 572) -> 0.5499069690704346\n",
            "    (3028, 2050) -> 0.6926028728485107\n",
            "    (336, 2595) -> 0.7158300876617432\n",
            "    (206, 1573) -> 0.7052926421165466\n",
            "    (3889, 231) -> 0.7273657321929932\n",
            "    (2938, 480) -> 0.7069321870803833\n",
            "    (831, 2016) -> 0.6661205291748047\n",
            "    (2562, 2407) -> 0.5632131099700928\n",
            "    (2313, 586) -> 0.7008708715438843\n",
            "    (178, 2235) -> 0.720923900604248\n",
            "    (2172, 504) -> 0.7294908761978149\n",
            "    (2553, 2945) -> 0.7134515643119812\n",
            "    (456, 3933) -> 0.7258622050285339\n",
            "    (1041, 1379) -> 0.7305914163589478\n",
            "    (1662, 2943) -> 0.723665177822113\n",
            "    (401, 848) -> 0.5968444347381592\n",
            "    (3814, 3481) -> 0.7300223112106323\n",
            "    (227, 4043) -> 0.5168306827545166\n",
            "    (4054, 228) -> 0.7196170687675476\n",
            "    (1492, 2312) -> 0.6686824560165405\n",
            "    (1601, 402) -> 0.7259882688522339\n",
            "  Correct: 30; Incorrect: 0; Accuracy: 1.0\n",
            "Prediction for 30 random negative validation edges:\n",
            "    (1434, 3939) -> 0.6667165756225586\n",
            "    (24, 68) -> 0.3828630745410919\n",
            "    (364, 633) -> 0.6288483142852783\n",
            "    (384, 1117) -> 0.6472781300544739\n",
            "    (360, 1807) -> 0.44152340292930603\n",
            "    (913, 4107) -> 0.37254634499549866\n",
            "    (2279, 2521) -> 0.6796960234642029\n",
            "    (1929, 2185) -> 0.6102749109268188\n",
            "    (918, 1409) -> 0.31403279304504395\n",
            "    (2565, 3960) -> 0.37631189823150635\n",
            "    (2216, 3284) -> 0.48277023434638977\n",
            "    (622, 893) -> 0.6195290684700012\n",
            "    (1117, 2268) -> 0.3482016921043396\n",
            "    (2770, 4117) -> 0.4576738476753235\n",
            "    (2348, 2803) -> 0.607566773891449\n",
            "    (1206, 1649) -> 0.3170030415058136\n",
            "    (1876, 2070) -> 0.46476441621780396\n",
            "    (1164, 2107) -> 0.38386985659599304\n",
            "    (1980, 2827) -> 0.5755329728126526\n",
            "    (1252, 3238) -> 0.6071413159370422\n",
            "    (95, 2565) -> 0.5715566873550415\n",
            "    (338, 3422) -> 0.3892545998096466\n",
            "    (2560, 2609) -> 0.6828773021697998\n",
            "    (1120, 1402) -> 0.2905370593070984\n",
            "    (21, 385) -> 0.6537150740623474\n",
            "    (624, 1778) -> 0.6730311512947083\n",
            "    (894, 2403) -> 0.604671061038971\n",
            "    (686, 2866) -> 0.6540243625640869\n",
            "    (59, 2106) -> 0.43629077076911926\n",
            "    (752, 1687) -> 0.7303550243377686\n",
            "  Correct: 14; Incorrect: 16; Accuracy: 0.4666666666666667\n",
            "\n",
            "  Correct: 44; Incorrect: 16; Accuracy: 0.7333333333333333\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "            list(model.parameters())  +\n",
        "            list(predictor.parameters()), lr=0.05)\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "print(test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64*1024))\n",
        "pick_random_predictions(model,30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🎯 Resultados del modelo base:\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0000\n",
            "Hits@40: 0.0000\n",
            "Hits@50: 0.0000\n",
            "Accuracy: 0.7764\n"
          ]
        }
      ],
      "source": [
        "results_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🎯 Resultados del modelo base:\")\n",
        "for k, v in results_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GPT Response ===\n",
            "\n",
            "1. In a drug-drug interaction graph like the one described, several topological patterns and structures could be useful for link prediction:\n",
            "   - Hubs: Nodes with a high degree of connections, representing drugs that interact with many other drugs. These hubs could play a crucial role in link prediction as they are likely to be involved in multiple interactions.\n",
            "   - Communities: Groups of nodes that are more densely connected within the group than with the rest of the graph. Identifying communities can help in understanding clusters of drugs that interact closely with each other, potentially indicating similar mechanisms of action or shared pharmacological properties.\n",
            "   - Motifs: Small, recurring patterns in the network that may provide insights into specific interaction patterns between drugs, such as feedback loops or feedforward structures.\n",
            "\n",
            "2. From this type of interaction network, high-level reasoning or semantic cues that could be inferred include:\n",
            "   - Drug similarity: Drugs that are connected in the graph may share similar mechanisms of action, targets, or side effects.\n",
            "   - Drug combinations: Pairs of drugs that frequently co-occur in interactions may indicate potential synergistic or antagonistic effects when used together.\n",
            "   - Network proximity: Drugs that are close in the network may have a higher likelihood of interacting with each other due to shared targets or pathways.\n",
            "\n",
            "3. Semantically describing the types of connections or clusters expected in a drug-drug interaction graph:\n",
            "   - Target-specific clusters: Groups of drugs that interact with a common target or set of targets, forming clusters based on their mechanism of action.\n",
            "   - Side effect clusters: Drugs that are linked due to shared adverse effects or interactions that lead to specific clinical outcomes.\n",
            "   - Pharmacological classes: Clusters representing drugs belonging to the same therapeutic class or sharing similar chemical structures, indicating potential similarities in their interactions and effects.\n",
            "\n",
            "By leveraging these topological patterns, semantic cues, and expected connections in the drug-drug interaction graph, you can generate meaningful embeddings that capture the underlying relationships between drugs and enhance the performance of your graph neural network for link prediction.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=\"sk-...\")  # ← poné aquí tu clave\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant specialized in graph-based machine learning.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"\n",
        "You are an expert in graph-based machine learning and biomedical networks.\n",
        "\n",
        "We are working with a biomedical interaction graph where:\n",
        "- Each node represents a drug.\n",
        "- Each undirected edge represents a known interaction between two drugs.\n",
        "- The graph contains 4,267 nodes and 2,135,822 directed edges (equivalent to 1,067,911 undirected edges).\n",
        "- The average node degree is approximately 501.\n",
        "- There are no node features available. Only the graph structure is used.\n",
        "\n",
        "Our goal is to perform link prediction: predicting whether a link exists between a given pair of nodes.\n",
        "\n",
        "Please analyze the characteristics of this graph and describe:\n",
        "1. What kind of topological patterns or structures (e.g., hubs, communities, motifs) might be useful for link prediction?\n",
        "2. What high-level reasoning or semantic cues could be inferred from this kind of interaction network?\n",
        "3. How would you semantically describe the types of connections or clusters you expect in a drug–drug interaction graph like this?\n",
        "\n",
        "This output will be transformed into an embedding and used as additional semantic input to enhance the performance of a graph neural network for link prediction.\n",
        "\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"=== GPT Response ===\\n\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Respuesta guardada en 'gpt_ddi_summary_run2.txt'\n"
          ]
        }
      ],
      "source": [
        "gpt_response = response.choices[0].message.content\n",
        "\n",
        "\n",
        "with open(\"gpt_ddi_summary_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(gpt_response)\n",
        "\n",
        "print(\"✅ Respuesta guardada en 'gpt_ddi_summary_run2.txt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensión del embedding: (384,)\n",
            "Primeros valores: [ 0.0432975  -0.07707488 -0.05820531 -0.00986097  0.04603406  0.00062814\n",
            " -0.00918771  0.09237258  0.05976453 -0.08219253]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "with open(\"gpt_ddi_summary_run2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt_response_text = f.read()\n",
        "\n",
        "# Cargar el modelo BERT compacto\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Obtener el embedding vectorial del texto\n",
        "embedding = model.encode(gpt_response_text)\n",
        "\n",
        "print(\"Dimensión del embedding:\", embedding.shape)\n",
        "print(\"Primeros valores:\", embedding[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding guardado en 'gpt_ddi_embedding_run2.npy'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\"gpt_ddi_embedding_run2.npy\", embedding)\n",
        "print(\"✅ Embedding guardado en 'gpt_ddi_embedding_run2.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_global = np.load(\"gpt_ddi_embedding_run2.npy\")\n",
        "embedding_global = torch.tensor(embedding_global, dtype=torch.float32).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "aug_emb = torch.cat([emb, embedding_global.unsqueeze(0).expand(emb.size(0), -1)], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1601\n",
            "Epoch 1 has loss 10.7623\n",
            "Epoch 2 has loss 10.2396\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m predictor = DotProductLinkPredictor().to(device)\n\u001b[32m      4\u001b[39m optimizer = torch.optim.Adam(\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mlist\u001b[39m(model.parameters()) + \u001b[38;5;28mlist\u001b[39m(predictor.parameters()), lr=\u001b[32m0.05\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_edge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBCELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, predictor, x, adj_t, split_edge, loss_fn, optimizer, batch_size, num_epochs, edge_model, spd)\u001b[39m\n\u001b[32m     48\u001b[39m epoch_total_loss += loss.item()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Update our parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     53\u001b[39m torch.nn.utils.clip_grad_norm_(predictor.parameters(), \u001b[32m1.0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TFM/venv_llm_clean/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    479\u001b[39m         Tensor.backward,\n\u001b[32m    480\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m         inputs=inputs,\n\u001b[32m    486\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TFM/venv_llm_clean/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    195\u001b[39m     retain_graph = create_graph\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "model = SAGE(385,hidden_dimension, hidden_dimension, 7, 0.5).to(device)\n",
        "predictor = DotProductLinkPredictor().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.05)\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  20 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "💊 Resultados del modelo CON embedding GPT:\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0000\n",
            "Hits@40: 0.0000\n",
            "Hits@50: 0.0001\n",
            "Accuracy: 0.7724\n"
          ]
        }
      ],
      "source": [
        "results_with_gpt = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n💊 Resultados del modelo CON embedding GPT:\")\n",
        "for k, v in results_with_gpt.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Post-Processing Layers\n",
        "We can also try including post-processing layers in our GNN that do not pass messages, but simply apply a neural network to the embeddings. This can be convenient when the embeddings need to be used in a downstream task, such as the link prediction we wish to perform.\n",
        "\n",
        "Below, we augment our base `SAGE` model with additional post-processing linear layers that are applied to the output of the GNN layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copied from above (\"Post-Processing Layers\") in order to be able to run this cell without running the previous ones\n",
        "class PostProcessSAGE(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_dimension, out_channels, num_conv_layers,\n",
        "                num_linear_layers, dropout):\n",
        "    super(PostProcessSAGE, self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.lins = torch.nn.ModuleList()\n",
        "\n",
        "    self.convs.append(SAGEConv(in_channels, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "    for _ in range(num_conv_layers - 1):\n",
        "      self.convs.append(SAGEConv(hidden_dimension, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "\n",
        "    for _ in range(num_linear_layers - 1):\n",
        "      self.lins.append(torch.nn.Linear(hidden_dimension, hidden_dimension))\n",
        "    self.lins.append(torch.nn.Linear(hidden_dimension, out_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, adj_t):\n",
        "    for conv in self.convs[:-1]:\n",
        "      x = conv(x, adj_t)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, adj_t)\n",
        "\n",
        "    # Post-process\n",
        "    for lin in self.lins[:-1]:\n",
        "      x = lin(x)\n",
        "      x = F.relu(x)\n",
        "    x = self.lins[-1](x)\n",
        "    return x\n",
        "\n",
        "# copied from above (\"Base GraphSage Model\") in order to be able to run this cell without running the previous ones\n",
        "class DotProductLinkPredictor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DotProductLinkPredictor, self).__init__()\n",
        "\n",
        "  def forward(self, x_i, x_j):\n",
        "    # dot product of node embeddings, that is meant to be the edge embedding\n",
        "    out = (x_i*x_j).sum(-1)\n",
        "    return torch.sigmoid(out)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    pass\n",
        "\n",
        "class NeuralLinkPredictor(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "               dropout):\n",
        "    super(NeuralLinkPredictor, self).__init__()\n",
        "\n",
        "    self.lins = torch.nn.ModuleList()\n",
        "    self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "    self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for lin in self.lins:\n",
        "      lin.reset_parameters()\n",
        "\n",
        "  def forward(self, x_i, x_j):\n",
        "    x = x_i * x_j\n",
        "    for lin in self.lins[:-1]:\n",
        "      x = lin(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.lins[-1](x)\n",
        "    return torch.sigmoid(x).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PostProcessSAGE(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(1, 256, aggr=add)\n",
            "    (1-6): 6 x SAGEConv(256, 256, aggr=add)\n",
            "  )\n",
            "  (lins): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            ")\n",
            "NeuralLinkPredictor(\n",
            "  (lins): ModuleList(\n",
            "    (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 0 has loss 11.7486\n",
            "Epoch 1 has loss 10.7223\n",
            "Epoch 2 has loss 24.6591\n",
            "Epoch 3 has loss 9.6686\n",
            "Epoch 4 has loss 9.1936\n",
            "Epoch 5 has loss 8.5739\n",
            "Epoch 6 has loss 7.991\n",
            "Epoch 7 has loss 7.568\n",
            "Epoch 8 has loss 7.453\n",
            "Epoch 9 has loss 7.3757\n",
            "Epoch 10 has loss 7.1929\n",
            "Epoch 11 has loss 7.1749\n",
            "Epoch 12 has loss 6.8195\n",
            "Epoch 13 has loss 6.373\n",
            "Epoch 14 has loss 6.9648\n",
            "Epoch 15 has loss 6.8372\n",
            "Epoch 16 has loss 6.5514\n",
            "Epoch 17 has loss 6.3502\n",
            "Epoch 18 has loss 5.9041\n",
            "Epoch 19 has loss 6.0472\n",
            "Epoch 20 has loss 5.8084\n",
            "Epoch 21 has loss 5.6413\n",
            "Epoch 22 has loss 5.7261\n",
            "Epoch 23 has loss 5.5874\n",
            "Epoch 24 has loss 5.4459\n",
            "Epoch 25 has loss 5.2109\n",
            "Epoch 26 has loss 5.5083\n",
            "Epoch 27 has loss 5.3081\n",
            "Epoch 28 has loss 5.3973\n",
            "Epoch 29 has loss 5.1248\n",
            "Epoch 30 has loss 5.0521\n",
            "Epoch 31 has loss 5.1048\n",
            "Epoch 32 has loss 5.0763\n",
            "Epoch 33 has loss 5.0381\n",
            "Epoch 34 has loss 4.946\n",
            "Epoch 35 has loss 4.9471\n",
            "Epoch 36 has loss 4.8581\n",
            "Epoch 37 has loss 4.8734\n",
            "Epoch 38 has loss 4.8279\n",
            "Epoch 39 has loss 4.8381\n",
            "Epoch 40 has loss 4.7439\n",
            "Epoch 41 has loss 4.782\n",
            "Epoch 42 has loss 4.7133\n",
            "Epoch 43 has loss 4.8131\n",
            "Epoch 44 has loss 4.8125\n",
            "Epoch 45 has loss 4.7026\n",
            "Epoch 46 has loss 4.5797\n",
            "Epoch 47 has loss 4.6354\n",
            "Epoch 48 has loss 4.5916\n",
            "Epoch 49 has loss 4.629\n",
            "Epoch 50 has loss 4.6268\n",
            "Epoch 51 has loss 4.7693\n",
            "Epoch 52 has loss 4.5928\n",
            "Epoch 53 has loss 4.5393\n",
            "Epoch 54 has loss 4.6571\n",
            "Epoch 55 has loss 4.5846\n",
            "Epoch 56 has loss 4.4653\n",
            "Epoch 57 has loss 4.4798\n",
            "Epoch 58 has loss 4.517\n",
            "Epoch 59 has loss 4.4818\n",
            "Epoch 60 has loss 4.3785\n",
            "Epoch 61 has loss 4.3513\n",
            "Epoch 62 has loss 4.3153\n",
            "Epoch 63 has loss 4.3444\n",
            "Epoch 64 has loss 4.4283\n",
            "Epoch 65 has loss 4.2989\n",
            "Epoch 66 has loss 4.2876\n",
            "Epoch 67 has loss 4.3443\n",
            "Epoch 68 has loss 4.472\n",
            "Epoch 69 has loss 4.275\n",
            "Epoch 70 has loss 4.3903\n",
            "Epoch 71 has loss 4.3325\n",
            "Epoch 72 has loss 4.2571\n",
            "Epoch 73 has loss 4.2757\n",
            "Epoch 74 has loss 4.2888\n",
            "Epoch 75 has loss 4.2563\n",
            "Epoch 76 has loss 4.3348\n",
            "Epoch 77 has loss 4.2657\n",
            "Epoch 78 has loss 4.1816\n",
            "Epoch 79 has loss 4.2078\n",
            "Epoch 80 has loss 4.3242\n",
            "Epoch 81 has loss 4.194\n",
            "Epoch 82 has loss 4.0389\n",
            "Epoch 83 has loss 4.0954\n",
            "Epoch 84 has loss 4.1287\n",
            "Epoch 85 has loss 4.0387\n",
            "Epoch 86 has loss 4.1184\n",
            "Epoch 87 has loss 4.0703\n",
            "Epoch 88 has loss 4.1714\n",
            "Epoch 89 has loss 4.1112\n",
            "Epoch 90 has loss 4.0669\n",
            "Epoch 91 has loss 4.035\n",
            "Epoch 92 has loss 3.9733\n",
            "Epoch 93 has loss 4.0299\n",
            "Epoch 94 has loss 3.985\n",
            "Epoch 95 has loss 3.9805\n",
            "Epoch 96 has loss 4.0964\n",
            "Epoch 97 has loss 3.9045\n",
            "Epoch 98 has loss 3.935\n",
            "Epoch 99 has loss 3.8224\n",
            "  4448 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "{'Hits@10': 0.03332109761853037, 'Hits@20': 0.040610087722583885, 'Hits@30': 0.040610087722583885, 'Hits@40': 0.040610087722583885, 'Hits@50': 0.040610087722583885, 'Accuracy': 0.8989}\n",
            "Prediction for 30 random positive validation edges:\n",
            "    (3271, 1151) -> 0.8912302851676941\n",
            "    (187, 998) -> 0.9574534893035889\n",
            "    (1295, 817) -> 0.9713167548179626\n",
            "    (691, 3093) -> 0.9713167548179626\n",
            "    (2576, 3264) -> 0.9713167548179626\n",
            "    (2324, 1036) -> 0.9713167548179626\n",
            "    (495, 3097) -> 0.9713167548179626\n",
            "    (333, 3461) -> 0.8809986114501953\n",
            "    (524, 507) -> 0.9713167548179626\n",
            "    (375, 1017) -> 0.8855013251304626\n",
            "    (1353, 287) -> 0.9402866959571838\n",
            "    (1111, 3789) -> 0.5780715346336365\n",
            "    (2768, 2614) -> 0.9713167548179626\n",
            "    (662, 2341) -> 0.9713167548179626\n",
            "    (35, 527) -> 0.2048850804567337\n",
            "    (718, 2488) -> 0.9713167548179626\n",
            "    (3733, 450) -> 0.9284216165542603\n",
            "    (284, 2203) -> 0.9717294573783875\n",
            "    (1608, 763) -> 0.8360358476638794\n",
            "    (280, 523) -> 0.9475659728050232\n",
            "    (4053, 1131) -> 0.9713167548179626\n",
            "    (809, 1745) -> 0.6879823803901672\n",
            "    (324, 1050) -> 0.8360866904258728\n",
            "    (2070, 463) -> 0.9670882821083069\n",
            "    (3059, 993) -> 0.8999872803688049\n",
            "    (2050, 247) -> 0.9713167548179626\n",
            "    (861, 911) -> 0.970974862575531\n",
            "    (1947, 916) -> 0.9119144678115845\n",
            "    (1271, 3633) -> 0.9713167548179626\n",
            "    (547, 2018) -> 0.9713167548179626\n",
            "  Correct: 29; Incorrect: 1; Accuracy: 0.9666666666666667\n",
            "Prediction for 30 random negative validation edges:\n",
            "    (34, 2394) -> 0.4349181056022644\n",
            "    (1874, 1985) -> 0.09536894410848618\n",
            "    (1118, 1979) -> 0.8868817090988159\n",
            "    (421, 970) -> 0.39088156819343567\n",
            "    (1807, 2026) -> 0.3056928813457489\n",
            "    (1018, 1833) -> 0.6990100741386414\n",
            "    (131, 673) -> 0.9036470651626587\n",
            "    (176, 685) -> 0.49891600012779236\n",
            "    (1011, 3929) -> 0.0001926467812154442\n",
            "    (1144, 1728) -> 0.4931708872318268\n",
            "    (1368, 4168) -> 0.017486046999692917\n",
            "    (1345, 1361) -> 0.6848403215408325\n",
            "    (2048, 2770) -> 0.2635769546031952\n",
            "    (2112, 2479) -> 0.021462008357048035\n",
            "    (114, 670) -> 0.9713167548179626\n",
            "    (1430, 1907) -> 0.24943366646766663\n",
            "    (588, 3052) -> 0.8450279235839844\n",
            "    (35, 372) -> 0.03684048727154732\n",
            "    (359, 1406) -> 0.018443182110786438\n",
            "    (1167, 1194) -> 0.15650615096092224\n",
            "    (834, 3290) -> 4.495108896662714e-06\n",
            "    (60, 1231) -> 0.2640760540962219\n",
            "    (102, 465) -> 0.3332868814468384\n",
            "    (1281, 1406) -> 0.012998693622648716\n",
            "    (725, 2587) -> 0.00025392728275619447\n",
            "    (256, 3453) -> 0.42774611711502075\n",
            "    (1069, 2461) -> 0.019444448873400688\n",
            "    (451, 1092) -> 0.2986446022987366\n",
            "    (933, 972) -> 0.8855569362640381\n",
            "    (221, 2595) -> 0.15561921894550323\n",
            "  Correct: 23; Incorrect: 7; Accuracy: 0.7666666666666667\n",
            "\n",
            "  Correct: 52; Incorrect: 8; Accuracy: 0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(1, hidden_dimension, hidden_dimension, 7, 4, 0.5).to(device) # It was just SAGE in the original code\n",
        "print(model)\n",
        "\n",
        "predictor = NeuralLinkPredictor(hidden_dimension, hidden_dimension, 1, 4, 0.5).to(device)\n",
        "print(predictor)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "            list(model.parameters())  +\n",
        "            list(predictor.parameters()), lr=0.01)\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(),\n",
        "      optimizer, 64 * 1024, 100)\n",
        "print(test(model, predictor, emb, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024))\n",
        "pick_random_predictions(model,30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  4448 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  5421 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔍 Resultados del modelo con NeuralLinkPredictor (sin embedding GPT):\n",
            "Hits@10: 0.0333\n",
            "Hits@20: 0.0406\n",
            "Hits@30: 0.0406\n",
            "Hits@40: 0.0406\n",
            "Hits@50: 0.0406\n",
            "Accuracy: 0.8989\n"
          ]
        }
      ],
      "source": [
        "results_neural = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔍 Resultados del modelo con NeuralLinkPredictor (sin embedding GPT):\")\n",
        "for k, v in results_neural.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7394\n",
            "Epoch 1 has loss 10.4278\n",
            "Epoch 2 has loss 9.699\n",
            "Epoch 3 has loss 9.0932\n",
            "Epoch 4 has loss 8.408\n",
            "Epoch 5 has loss 7.7797\n",
            "Epoch 6 has loss 7.5728\n",
            "Epoch 7 has loss 7.4241\n",
            "Epoch 8 has loss 7.1634\n",
            "Epoch 9 has loss 6.8597\n",
            "Epoch 10 has loss 6.5289\n",
            "Epoch 11 has loss 6.0437\n",
            "Epoch 12 has loss 5.9747\n",
            "Epoch 13 has loss 5.7626\n",
            "Epoch 14 has loss 5.5774\n",
            "Epoch 15 has loss 5.5006\n",
            "Epoch 16 has loss 5.4981\n",
            "Epoch 17 has loss 5.5569\n",
            "Epoch 18 has loss 5.5666\n",
            "Epoch 19 has loss 5.3689\n",
            "Epoch 20 has loss 5.2132\n",
            "Epoch 21 has loss 5.0729\n",
            "Epoch 22 has loss 5.0992\n",
            "Epoch 23 has loss 5.0397\n",
            "Epoch 24 has loss 4.9324\n",
            "Epoch 25 has loss 4.8769\n",
            "Epoch 26 has loss 4.9225\n",
            "Epoch 27 has loss 4.8642\n",
            "Epoch 28 has loss 4.9871\n",
            "Epoch 29 has loss 4.8609\n",
            "Epoch 30 has loss 4.7438\n",
            "Epoch 31 has loss 4.6543\n",
            "Epoch 32 has loss 4.6859\n",
            "Epoch 33 has loss 4.6472\n",
            "Epoch 34 has loss 4.6344\n",
            "Epoch 35 has loss 4.5274\n",
            "Epoch 36 has loss 4.4417\n",
            "Epoch 37 has loss 4.5753\n",
            "Epoch 38 has loss 4.525\n",
            "Epoch 39 has loss 4.4539\n",
            "Epoch 40 has loss 4.4566\n",
            "Epoch 41 has loss 4.3751\n",
            "Epoch 42 has loss 4.3194\n",
            "Epoch 43 has loss 4.2873\n",
            "Epoch 44 has loss 4.2331\n",
            "Epoch 45 has loss 4.1703\n",
            "Epoch 46 has loss 4.2299\n",
            "Epoch 47 has loss 4.1875\n",
            "Epoch 48 has loss 4.1029\n",
            "Epoch 49 has loss 4.2044\n",
            "  17696 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  22915 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  30149 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  33385 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  36473 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + BERT (sin usar el embedding) v1:\n",
            "Hits@10: 0.1326\n",
            "Hits@20: 0.1717\n",
            "Hits@30: 0.2259\n",
            "Hits@40: 0.2501\n",
            "Hits@50: 0.2732\n",
            "Accuracy: 0.9010\n"
          ]
        }
      ],
      "source": [
        "### CORRIDA 1 CORREGIDA\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + BERT (sin usar el embedding) v1:\")\n",
        "for k, v in results_sage.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8043\n",
            "Epoch 1 has loss 11.7925\n",
            "Epoch 2 has loss 11.7875\n",
            "Epoch 3 has loss 11.7841\n",
            "Epoch 4 has loss 12.806\n",
            "Epoch 5 has loss 11.5146\n",
            "Epoch 6 has loss 10.1617\n",
            "Epoch 7 has loss 9.0117\n",
            "Epoch 8 has loss 8.1855\n",
            "Epoch 9 has loss 7.8434\n",
            "Epoch 10 has loss 7.623\n",
            "Epoch 11 has loss 7.49\n",
            "Epoch 12 has loss 7.4683\n",
            "Epoch 13 has loss 7.5128\n",
            "Epoch 14 has loss 7.3905\n",
            "Epoch 15 has loss 7.4785\n",
            "Epoch 16 has loss 7.5009\n",
            "Epoch 17 has loss 7.3344\n",
            "Epoch 18 has loss 7.3592\n",
            "Epoch 19 has loss 7.3387\n",
            "Epoch 20 has loss 7.1337\n",
            "Epoch 21 has loss 6.9071\n",
            "Epoch 22 has loss 6.6371\n",
            "Epoch 23 has loss 6.4465\n",
            "Epoch 24 has loss 6.3106\n",
            "Epoch 25 has loss 6.1225\n",
            "Epoch 26 has loss 5.9795\n",
            "Epoch 27 has loss 5.9869\n",
            "Epoch 28 has loss 6.2419\n",
            "Epoch 29 has loss 5.8923\n",
            "Epoch 30 has loss 5.7596\n",
            "Epoch 31 has loss 5.7695\n",
            "Epoch 32 has loss 5.6007\n",
            "Epoch 33 has loss 5.5768\n",
            "Epoch 34 has loss 5.6943\n",
            "Epoch 35 has loss 5.5239\n",
            "Epoch 36 has loss 5.469\n",
            "Epoch 37 has loss 5.3538\n",
            "Epoch 38 has loss 5.3354\n",
            "Epoch 39 has loss 5.2304\n",
            "Epoch 40 has loss 5.2319\n",
            "Epoch 41 has loss 5.3016\n",
            "Epoch 42 has loss 5.2349\n",
            "Epoch 43 has loss 5.2835\n",
            "Epoch 44 has loss 5.1749\n",
            "Epoch 45 has loss 4.9943\n",
            "Epoch 46 has loss 5.0259\n",
            "Epoch 47 has loss 4.9648\n",
            "Epoch 48 has loss 4.913\n",
            "Epoch 49 has loss 4.9091\n",
            "Epoch 50 has loss 4.8447\n",
            "Epoch 51 has loss 4.8149\n",
            "Epoch 52 has loss 4.7413\n",
            "Epoch 53 has loss 4.8322\n",
            "Epoch 54 has loss 4.8153\n",
            "Epoch 55 has loss 4.8671\n",
            "Epoch 56 has loss 4.7181\n",
            "Epoch 57 has loss 4.6236\n",
            "Epoch 58 has loss 4.6507\n",
            "Epoch 59 has loss 4.5515\n",
            "Epoch 60 has loss 4.5223\n",
            "Epoch 61 has loss 4.518\n",
            "Epoch 62 has loss 4.4653\n",
            "Epoch 63 has loss 4.5049\n",
            "Epoch 64 has loss 4.5035\n",
            "Epoch 65 has loss 4.5217\n",
            "Epoch 66 has loss 4.5092\n",
            "Epoch 67 has loss 4.5185\n",
            "Epoch 68 has loss 4.46\n",
            "Epoch 69 has loss 4.4425\n",
            "Epoch 70 has loss 4.3898\n",
            "Epoch 71 has loss 4.4067\n",
            "Epoch 72 has loss 4.4581\n",
            "Epoch 73 has loss 4.4449\n",
            "Epoch 74 has loss 4.4955\n",
            "Epoch 75 has loss 4.471\n",
            "Epoch 76 has loss 4.3959\n",
            "Epoch 77 has loss 4.3486\n",
            "Epoch 78 has loss 4.3014\n",
            "Epoch 79 has loss 4.3579\n",
            "Epoch 80 has loss 4.3398\n",
            "Epoch 81 has loss 4.1808\n",
            "Epoch 82 has loss 4.2118\n",
            "Epoch 83 has loss 4.2054\n",
            "Epoch 84 has loss 4.2795\n",
            "Epoch 85 has loss 4.2393\n",
            "Epoch 86 has loss 4.2277\n",
            "Epoch 87 has loss 4.1569\n",
            "Epoch 88 has loss 4.2671\n",
            "Epoch 89 has loss 4.1706\n",
            "Epoch 90 has loss 4.1771\n",
            "Epoch 91 has loss 4.1304\n",
            "Epoch 92 has loss 4.1081\n",
            "Epoch 93 has loss 4.1051\n",
            "Epoch 94 has loss 4.2459\n",
            "Epoch 95 has loss 4.2843\n",
            "Epoch 96 has loss 4.3071\n",
            "Epoch 97 has loss 4.1769\n",
            "Epoch 98 has loss 4.1987\n",
            "Epoch 99 has loss 4.0943\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(385, hidden_dimension, hidden_dimension, 7, 4, 0.5).to(device)\n",
        "predictor = NeuralLinkPredictor(hidden_dimension, hidden_dimension, 1, 4, 0.5).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(),\n",
        "      optimizer, 64 * 1024, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.593\n",
            "Epoch 1 has loss 10.6014\n",
            "Epoch 2 has loss 9.4579\n",
            "Epoch 3 has loss 8.6678\n",
            "Epoch 4 has loss 7.9335\n",
            "Epoch 5 has loss 7.4656\n",
            "Epoch 6 has loss 7.2588\n",
            "Epoch 7 has loss 6.7534\n",
            "Epoch 8 has loss 6.3465\n",
            "Epoch 9 has loss 6.0354\n",
            "Epoch 10 has loss 5.9303\n",
            "Epoch 11 has loss 5.7877\n",
            "Epoch 12 has loss 5.8461\n",
            "Epoch 13 has loss 5.7995\n",
            "Epoch 14 has loss 5.6554\n",
            "Epoch 15 has loss 5.5236\n",
            "Epoch 16 has loss 5.4877\n",
            "Epoch 17 has loss 5.4468\n",
            "Epoch 18 has loss 5.6057\n",
            "Epoch 19 has loss 5.4381\n",
            "Epoch 20 has loss 5.3993\n",
            "Epoch 21 has loss 5.3887\n",
            "Epoch 22 has loss 5.3514\n",
            "Epoch 23 has loss 5.2786\n",
            "Epoch 24 has loss 5.3654\n",
            "Epoch 25 has loss 5.2225\n",
            "Epoch 26 has loss 5.1891\n",
            "Epoch 27 has loss 5.1437\n",
            "Epoch 28 has loss 5.2854\n",
            "Epoch 29 has loss 5.1593\n",
            "Epoch 30 has loss 5.0992\n",
            "Epoch 31 has loss 5.0126\n",
            "Epoch 32 has loss 4.993\n",
            "Epoch 33 has loss 4.9712\n",
            "Epoch 34 has loss 4.8394\n",
            "Epoch 35 has loss 5.06\n",
            "Epoch 36 has loss 5.04\n",
            "Epoch 37 has loss 4.7502\n",
            "Epoch 38 has loss 4.7728\n",
            "Epoch 39 has loss 4.7833\n",
            "Epoch 40 has loss 4.7055\n",
            "Epoch 41 has loss 4.7569\n",
            "Epoch 42 has loss 4.5938\n",
            "Epoch 43 has loss 4.4803\n",
            "Epoch 44 has loss 4.5287\n",
            "Epoch 45 has loss 4.4454\n",
            "Epoch 46 has loss 4.4265\n",
            "Epoch 47 has loss 4.4002\n",
            "Epoch 48 has loss 4.3448\n",
            "Epoch 49 has loss 4.3368\n",
            "  15712 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  23600 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26332 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  27879 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  29250 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGEConv + BERT (como input) v1:\n",
            "Hits@10: 0.1177\n",
            "Hits@20: 0.1768\n",
            "Hits@30: 0.1973\n",
            "Hits@40: 0.2088\n",
            "Hits@50: 0.2191\n",
            "Accuracy: 0.8920\n"
          ]
        }
      ],
      "source": [
        "\n",
        "aug_emb = torch.cat([emb, embedding_global.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_input = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGEConv + BERT (como input) v1:\")\n",
        "for k, v in results_sage_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  7024 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  7024 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  7024 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  27704 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  35513 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "💊 Resultados del modelo con NeuralLinkPredictor + embedding GPT:\n",
            "Hits@10: 0.0526\n",
            "Hits@20: 0.0526\n",
            "Hits@30: 0.0526\n",
            "Hits@40: 0.2075\n",
            "Hits@50: 0.2660\n",
            "Accuracy: 0.8739\n"
          ]
        }
      ],
      "source": [
        "results_neural_with_gpt = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n💊 Resultados del modelo con NeuralLinkPredictor + embedding GPT:\")\n",
        "for k, v in results_neural_with_gpt.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Comparación de resultados:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Modelo</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NeuralLink (sin GPT)</th>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NeuralLink + GPT</th>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Hits@10   Hits@20   Hits@30   Hits@40   Hits@50  \\\n",
              "Modelo                                                                   \n",
              "NeuralLink (sin GPT)  0.132565  0.171662  0.225854  0.250096  0.273229   \n",
              "NeuralLink + GPT      0.117703  0.176794  0.197260  0.208849  0.219119   \n",
              "\n",
              "                      Accuracy  \n",
              "Modelo                          \n",
              "NeuralLink (sin GPT)     0.901  \n",
              "NeuralLink + GPT         0.892  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results_comparativos = {\n",
        "    \"NeuralLink (sin GPT)\": results_sage,\n",
        "    \"NeuralLink + GPT\": results_sage_input,\n",
        "}\n",
        "\n",
        "# Crear DataFrame\n",
        "df_results = pd.DataFrame(results_comparativos).T  # transponemos para que los modelos estén como filas\n",
        "df_results.index.name = \"Modelo\"\n",
        "\n",
        "print(\"📊 Comparación de resultados:\")\n",
        "display(df_results)\n",
        "\n",
        "# Gráfico de barras para Hits@K\n",
        "hits_columns = [col for col in df_results.columns if \"Hits@\" in col]\n",
        "df_hits = df_results[hits_columns]\n",
        "\n",
        "ax = df_hits.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Hits@K por modelo\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de Accuracy\n",
        "df_accuracy = df_results[[\"Accuracy\"]]\n",
        "ax = df_accuracy.plot(kind='bar', figsize=(8, 5), color='orange')\n",
        "plt.title(\"Accuracy por modelo\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📌 Resumen de mejores modelos por métrica:\n",
            "\n",
            "🔹 Hits@10: NeuralLink + GPT (valor = 0.0526)\n",
            "🔹 Hits@20: NeuralLink + GPT (valor = 0.0526)\n",
            "🔹 Hits@30: NeuralLink + GPT (valor = 0.0526)\n",
            "🔹 Hits@40: NeuralLink + GPT (valor = 0.2075)\n",
            "🔹 Hits@50: NeuralLink + GPT (valor = 0.2660)\n",
            "🔹 Accuracy: NeuralLink (sin GPT) (valor = 0.8989)\n"
          ]
        }
      ],
      "source": [
        "# Resumen textual del mejor modelo por métrica\n",
        "mejores_modelos = df_results.idxmax()\n",
        "\n",
        "print(\"📌 Resumen de mejores modelos por métrica:\\n\")\n",
        "for metrica, modelo in mejores_modelos.items():\n",
        "    valor = df_results.loc[modelo, metrica]\n",
        "    print(f\"🔹 {metrica}: {modelo} (valor = {valor:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralLinkPredictorWithGPT(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, gpt_emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lins = nn.ModuleList()\n",
        "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "\n",
        "        # La última capa ahora recibe hidden + gpt_embedding\n",
        "        self.final = nn.Linear(hidden_channels + gpt_emb_dim, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "        self.final.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j, gpt_embedding):\n",
        "        x = x_i * x_j\n",
        "\n",
        "        for lin in self.lins:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Expandir gpt_embedding para cada par\n",
        "        if gpt_embedding.dim() == 1:\n",
        "            gpt_embedding = gpt_embedding.unsqueeze(0).expand(x.size(0), -1)\n",
        "\n",
        "        x = torch.cat([x, gpt_embedding], dim=-1)\n",
        "        x = self.final(x)\n",
        "\n",
        "        return torch.sigmoid(x).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase dummy para no usar el modelo base en el test\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def eval(self):\n",
        "        pass\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return None\n",
        "\n",
        "# Wrapper para incluir el embedding GPT al predictor\n",
        "class WrappedPredictor(torch.nn.Module):\n",
        "    def __init__(self, base_predictor, gpt_embedding):\n",
        "        super().__init__()\n",
        "        self.base_predictor = base_predictor\n",
        "        self.gpt_embedding = gpt_embedding\n",
        "\n",
        "    def eval(self):\n",
        "        self.base_predictor.eval()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        gpt_emb = self.gpt_embedding.unsqueeze(0).expand(x_i.size(0), -1)\n",
        "        return self.base_predictor(x_i, x_j, gpt_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  31070 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  37635 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  41700 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  44987 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  47407 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados del predictor con capa GPT final:\n",
            "Hits@10: 0.2328\n",
            "Hits@20: 0.2819\n",
            "Hits@30: 0.3124\n",
            "Hits@40: 0.3370\n",
            "Hits@50: 0.3551\n",
            "Accuracy: 0.9192\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "dummy_model = DummyModel()\n",
        "wrapped_predictor = WrappedPredictor(predictor_gpt_final, embedding_global)\n",
        "\n",
        "# Realizar evaluación\n",
        "wrapped_predictor.eval()\n",
        "with torch.no_grad():\n",
        "    results_final_gpt = test(\n",
        "        model=dummy_model,\n",
        "        predictor=wrapped_predictor,\n",
        "        x=z,\n",
        "        adj_t=adj_t,\n",
        "        split_edge=split_edge[\"valid\"],\n",
        "        evaluator=eval,\n",
        "        batch_size=64 * 1024\n",
        "    )\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"\\n🧪 Resultados del predictor con capa GPT final:\")\n",
        "for k, v in results_final_gpt.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6599\n",
            "Epoch 1 has loss 10.3177\n",
            "Epoch 2 has loss 10.2075\n",
            "Epoch 3 has loss 10.1274\n",
            "Epoch 4 has loss 10.0629\n",
            "Epoch 5 has loss 10.0376\n",
            "Epoch 6 has loss 10.0572\n",
            "Epoch 7 has loss 10.0806\n",
            "Epoch 8 has loss 9.9703\n",
            "Epoch 9 has loss 9.8706\n",
            "Epoch 10 has loss 9.8407\n",
            "Epoch 11 has loss 9.6549\n",
            "Epoch 12 has loss 9.6859\n",
            "Epoch 13 has loss 9.58\n",
            "Epoch 14 has loss 9.5679\n",
            "Epoch 15 has loss 9.5434\n",
            "Epoch 16 has loss 9.5432\n",
            "Epoch 17 has loss 9.5185\n",
            "Epoch 18 has loss 9.5142\n",
            "Epoch 19 has loss 9.5201\n",
            "Epoch 20 has loss 9.5018\n",
            "Epoch 21 has loss 9.5098\n",
            "Epoch 22 has loss 9.5246\n",
            "Epoch 23 has loss 9.5061\n",
            "Epoch 24 has loss 9.5009\n",
            "Epoch 25 has loss 9.4805\n",
            "Epoch 26 has loss 9.4745\n",
            "Epoch 27 has loss 9.486\n",
            "Epoch 28 has loss 9.4808\n",
            "Epoch 29 has loss 9.4731\n",
            "Epoch 30 has loss 9.4705\n",
            "Epoch 31 has loss 9.4685\n",
            "Epoch 32 has loss 9.4753\n",
            "Epoch 33 has loss 9.469\n",
            "Epoch 34 has loss 9.485\n",
            "Epoch 35 has loss 9.4795\n",
            "Epoch 36 has loss 9.462\n",
            "Epoch 37 has loss 9.4577\n",
            "Epoch 38 has loss 9.4531\n",
            "Epoch 39 has loss 9.4574\n",
            "Epoch 40 has loss 9.4547\n",
            "Epoch 41 has loss 9.4544\n",
            "Epoch 42 has loss 9.457\n",
            "Epoch 43 has loss 9.46\n",
            "Epoch 44 has loss 9.4518\n",
            "Epoch 45 has loss 9.4505\n",
            "Epoch 46 has loss 9.5421\n",
            "Epoch 47 has loss 9.4767\n",
            "Epoch 48 has loss 9.4616\n",
            "Epoch 49 has loss 9.4501\n",
            "📉 Epoch 0, Loss: 17.3103\n",
            "📉 Epoch 1, Loss: 14.9962\n",
            "📉 Epoch 2, Loss: 14.1659\n",
            "📉 Epoch 3, Loss: 13.6414\n",
            "📉 Epoch 4, Loss: 13.2993\n",
            "📉 Epoch 5, Loss: 13.0605\n",
            "📉 Epoch 6, Loss: 13.0937\n",
            "📉 Epoch 7, Loss: 12.9004\n",
            "📉 Epoch 8, Loss: 12.8434\n",
            "📉 Epoch 9, Loss: 12.7550\n",
            "📉 Epoch 10, Loss: 12.7539\n",
            "📉 Epoch 11, Loss: 12.6257\n",
            "📉 Epoch 12, Loss: 12.5180\n",
            "📉 Epoch 13, Loss: 12.8368\n",
            "📉 Epoch 14, Loss: 12.5602\n",
            "📉 Epoch 15, Loss: 12.5248\n",
            "📉 Epoch 16, Loss: 12.5474\n",
            "📉 Epoch 17, Loss: 12.4850\n",
            "📉 Epoch 18, Loss: 12.5829\n",
            "📉 Epoch 19, Loss: 12.4739\n",
            "📉 Epoch 20, Loss: 12.4084\n",
            "📉 Epoch 21, Loss: 12.3711\n",
            "📉 Epoch 22, Loss: 12.4439\n",
            "📉 Epoch 23, Loss: 12.3433\n",
            "📉 Epoch 24, Loss: 12.3578\n",
            "📉 Epoch 25, Loss: 12.3456\n",
            "📉 Epoch 26, Loss: 12.3266\n",
            "📉 Epoch 27, Loss: 12.2716\n",
            "📉 Epoch 28, Loss: 12.2934\n",
            "📉 Epoch 29, Loss: 12.2987\n",
            "📉 Epoch 30, Loss: 12.2102\n",
            "📉 Epoch 31, Loss: 12.1695\n",
            "📉 Epoch 32, Loss: 12.3134\n",
            "📉 Epoch 33, Loss: 12.1692\n",
            "📉 Epoch 34, Loss: 12.0969\n",
            "📉 Epoch 35, Loss: 12.2413\n",
            "📉 Epoch 36, Loss: 12.1609\n",
            "📉 Epoch 37, Loss: 12.1317\n",
            "📉 Epoch 38, Loss: 12.2243\n",
            "📉 Epoch 39, Loss: 12.4108\n",
            "📉 Epoch 40, Loss: 12.1397\n",
            "📉 Epoch 41, Loss: 12.1598\n",
            "📉 Epoch 42, Loss: 12.1510\n",
            "📉 Epoch 43, Loss: 12.1951\n",
            "📉 Epoch 44, Loss: 12.1024\n",
            "📉 Epoch 45, Loss: 12.0921\n",
            "📉 Epoch 46, Loss: 12.1414\n",
            "📉 Epoch 47, Loss: 12.1988\n",
            "📉 Epoch 48, Loss: 12.1389\n",
            "📉 Epoch 49, Loss: 12.1116\n",
            "  4372 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5964 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  7683 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  8356 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  9707 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔬 Resultados SAGEConv + BERT (en predictor final v1):\n",
            "Hits@10: 0.0328\n",
            "Hits@20: 0.0447\n",
            "Hits@30: 0.0576\n",
            "Hits@40: 0.0626\n",
            "Hits@50: 0.0727\n",
            "Accuracy: 0.8618\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = model(emb, adj_t)\n",
        "\n",
        "predictor = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z.size(1),\n",
        "    hidden_channels=z.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global.shape[0]\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor.train()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor(\n",
        "            z[edge[0]], z[edge[1]],\n",
        "            embedding_global.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor, embedding_global)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_final = test(dummy_model, wrapped_predictor, z, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔬 Resultados SAGEConv + BERT (en predictor final v1):\")\n",
        "for k, v in results_sage_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Skip-Connection\n",
        "\n",
        "A skip-connection, in theory, allows us to have more layers in our GNN. This would allow us to incorporate information from larger k-hop neighborhoods, while somewhat mitigating the risk of over-smoothing node embeddings.\n",
        "\n",
        "We can quickly build upon our base `SAGE` model to include skip-connections, as demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rZ674Bj6Qbfp"
      },
      "outputs": [],
      "source": [
        "class SkipConnSAGE(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_dimension, out_channels, num_layers,\n",
        "                dropout):\n",
        "    super(SkipConnSAGE, self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "\n",
        "    self.convs.append(SAGEConv(in_channels, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "    for _ in range(num_layers - 2):\n",
        "      self.convs.append(SAGEConv(hidden_dimension, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "    self.convs.append(SAGEConv(hidden_dimension, out_channels, normalize=True, aggr=\"add\"))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, adj_t):\n",
        "    prev_x = None\n",
        "    for i in range(len(self.convs) - 1):\n",
        "      prev_x = x\n",
        "      x = self.convs[i](x, adj_t)\n",
        "      # Skip Connection\n",
        "      if i > 0:\n",
        "        x = x + prev_x\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, adj_t)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raPR8Ui0heIF"
      },
      "source": [
        "With our `SkipConnSAGE` model defined, we can now initiate its training end evaluation, just as we did with `SAGE` before. This time, we will use 10 `SAGEConv` layers.\n",
        "\n",
        "*This cell will take around 10 minutes to run.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpC_0EdmVUlb",
        "outputId": "134e9809-92a5-4bdd-bc3c-608dee8d1b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7128\n",
            "Epoch 1 has loss 10.6736\n",
            "Epoch 2 has loss 10.1258\n",
            "Epoch 3 has loss 10.7489\n",
            "Epoch 4 has loss 9.8261\n",
            "Epoch 5 has loss 9.4874\n",
            "Epoch 6 has loss 10.1746\n",
            "Epoch 7 has loss 9.8716\n",
            "Epoch 8 has loss 9.631\n",
            "Epoch 9 has loss 9.814\n",
            "Epoch 10 has loss 9.4873\n",
            "Epoch 11 has loss 9.8333\n",
            "Epoch 12 has loss 9.5789\n",
            "Epoch 13 has loss 9.4183\n",
            "Epoch 14 has loss 9.348\n",
            "Epoch 15 has loss 9.5112\n",
            "Epoch 16 has loss 9.1874\n",
            "Epoch 17 has loss 9.4772\n",
            "Epoch 18 has loss 9.5587\n",
            "Epoch 19 has loss 9.0371\n",
            "Epoch 20 has loss 8.7618\n",
            "Epoch 21 has loss 8.9743\n",
            "Epoch 22 has loss 9.1689\n",
            "Epoch 23 has loss 8.7571\n",
            "Epoch 24 has loss 9.3734\n",
            "Epoch 25 has loss 8.9763\n",
            "Epoch 26 has loss 9.2501\n",
            "Epoch 27 has loss 8.5601\n",
            "Epoch 28 has loss 9.1568\n",
            "Epoch 29 has loss 8.8836\n",
            "Epoch 30 has loss 8.8834\n",
            "Epoch 31 has loss 8.9206\n",
            "Epoch 32 has loss 8.6544\n",
            "Epoch 33 has loss 8.4481\n",
            "Epoch 34 has loss 8.6398\n",
            "Epoch 35 has loss 8.4006\n",
            "Epoch 36 has loss 8.8481\n",
            "Epoch 37 has loss 8.6044\n",
            "Epoch 38 has loss 8.9117\n",
            "Epoch 39 has loss 9.0521\n",
            "Epoch 40 has loss 9.0247\n",
            "Epoch 41 has loss 8.678\n",
            "Epoch 42 has loss 8.7468\n",
            "Epoch 43 has loss 8.538\n",
            "Epoch 44 has loss 8.5879\n",
            "Epoch 45 has loss 8.9708\n",
            "Epoch 46 has loss 8.6176\n",
            "Epoch 47 has loss 8.5713\n",
            "Epoch 48 has loss 8.5671\n",
            "Epoch 49 has loss 8.7026\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo base\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  30 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  66 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  107 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  139 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  177 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔎 Resultados SkipConnSAGE (sin GPT):\n",
            "Hits@10: 0.0002\n",
            "Hits@20: 0.0005\n",
            "Hits@30: 0.0008\n",
            "Hits@40: 0.0010\n",
            "Hits@50: 0.0013\n",
            "Accuracy: 0.7420\n"
          ]
        }
      ],
      "source": [
        "results_skipconn = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔎 Resultados SkipConnSAGE (sin GPT):\")\n",
        "for k, v in results_skipconn.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.1727\n",
            "Epoch 1 has loss 10.9386\n",
            "Epoch 2 has loss 10.1991\n",
            "Epoch 3 has loss 10.2179\n",
            "Epoch 4 has loss 10.0631\n",
            "Epoch 5 has loss 10.1411\n",
            "Epoch 6 has loss 9.961\n",
            "Epoch 7 has loss 10.1545\n",
            "Epoch 8 has loss 10.063\n",
            "Epoch 9 has loss 9.8558\n",
            "Epoch 10 has loss 10.198\n",
            "Epoch 11 has loss 10.1198\n",
            "Epoch 12 has loss 9.9322\n",
            "Epoch 13 has loss 9.73\n",
            "Epoch 14 has loss 9.7905\n",
            "Epoch 15 has loss 10.0071\n",
            "Epoch 16 has loss 9.9602\n",
            "Epoch 17 has loss 9.6786\n",
            "Epoch 18 has loss 9.374\n",
            "Epoch 19 has loss 9.1066\n",
            "Epoch 20 has loss 9.2303\n",
            "Epoch 21 has loss 9.5614\n",
            "Epoch 22 has loss 9.7368\n",
            "Epoch 23 has loss 9.4793\n",
            "Epoch 24 has loss 9.0743\n",
            "Epoch 25 has loss 9.2116\n",
            "Epoch 26 has loss 8.868\n",
            "Epoch 27 has loss 8.8459\n",
            "Epoch 28 has loss 9.0735\n",
            "Epoch 29 has loss 8.683\n",
            "Epoch 30 has loss 8.8179\n",
            "Epoch 31 has loss 8.4689\n",
            "Epoch 32 has loss 9.0604\n",
            "Epoch 33 has loss 9.1882\n",
            "Epoch 34 has loss 8.9266\n",
            "Epoch 35 has loss 9.2236\n",
            "Epoch 36 has loss 8.6274\n",
            "Epoch 37 has loss 8.444\n",
            "Epoch 38 has loss 9.3171\n",
            "Epoch 39 has loss 9.4128\n",
            "Epoch 40 has loss 9.1928\n",
            "Epoch 41 has loss 8.581\n",
            "Epoch 42 has loss 8.797\n",
            "Epoch 43 has loss 8.8011\n",
            "Epoch 44 has loss 8.9725\n",
            "Epoch 45 has loss 8.6342\n",
            "Epoch 46 has loss 8.7105\n",
            "Epoch 47 has loss 8.6083\n",
            "Epoch 48 has loss 9.1528\n",
            "Epoch 49 has loss 8.9944\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_global.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb.size(1),  \n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  108 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  140 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  172 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  248 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  304 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧠 Resultados SkipConnSAGE (con embedding GPT como entrada):\n",
            "Hits@10: 0.0008\n",
            "Hits@20: 0.0010\n",
            "Hits@30: 0.0013\n",
            "Hits@40: 0.0019\n",
            "Hits@50: 0.0023\n",
            "Accuracy: 0.7520\n"
          ]
        }
      ],
      "source": [
        "results_skipconn_gpt_input = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧠 Resultados SkipConnSAGE (con embedding GPT como entrada):\")\n",
        "for k, v in results_skipconn_gpt_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1434\n",
            "Epoch 1 has loss 10.7378\n",
            "Epoch 2 has loss 10.4528\n",
            "Epoch 3 has loss 10.3543\n",
            "Epoch 4 has loss 10.3932\n",
            "Epoch 5 has loss 10.3018\n",
            "Epoch 6 has loss 10.2526\n",
            "Epoch 7 has loss 10.2198\n",
            "Epoch 8 has loss 10.178\n",
            "Epoch 9 has loss 10.2234\n",
            "Epoch 10 has loss 10.1769\n",
            "Epoch 11 has loss 10.1939\n",
            "Epoch 12 has loss 10.1629\n",
            "Epoch 13 has loss 10.1553\n",
            "Epoch 14 has loss 10.1147\n",
            "Epoch 15 has loss 10.1466\n",
            "Epoch 16 has loss 10.1124\n",
            "Epoch 17 has loss 10.1363\n",
            "Epoch 18 has loss 10.0871\n",
            "Epoch 19 has loss 10.1048\n",
            "Epoch 20 has loss 10.0731\n",
            "Epoch 21 has loss 10.091\n",
            "Epoch 22 has loss 10.0538\n",
            "Epoch 23 has loss 10.0767\n",
            "Epoch 24 has loss 10.0276\n",
            "Epoch 25 has loss 10.0347\n",
            "Epoch 26 has loss 10.0334\n",
            "Epoch 27 has loss 10.0108\n",
            "Epoch 28 has loss 9.9882\n",
            "Epoch 29 has loss 9.9472\n",
            "Epoch 30 has loss 9.9629\n",
            "Epoch 31 has loss 9.9801\n",
            "Epoch 32 has loss 9.9852\n",
            "Epoch 33 has loss 9.9471\n",
            "Epoch 34 has loss 9.9112\n",
            "Epoch 35 has loss 9.9446\n",
            "Epoch 36 has loss 9.9161\n",
            "Epoch 37 has loss 9.899\n",
            "Epoch 38 has loss 9.8634\n",
            "Epoch 39 has loss 9.8365\n",
            "Epoch 40 has loss 9.7644\n",
            "Epoch 41 has loss 9.7451\n",
            "Epoch 42 has loss 9.7097\n",
            "Epoch 43 has loss 9.8357\n",
            "Epoch 44 has loss 9.82\n",
            "Epoch 45 has loss 9.7564\n",
            "Epoch 46 has loss 9.739\n",
            "Epoch 47 has loss 9.7407\n",
            "Epoch 48 has loss 9.6668\n",
            "Epoch 49 has loss 9.6328\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,  \n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Entrenar solo el modelo base con aug_emb para tener consistencia en input\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "# Obtener embeddings\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip = model(emb, adj_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📉 Epoch 0, Loss: 11.2950\n",
            "📉 Epoch 1, Loss: 9.3468\n",
            "📉 Epoch 2, Loss: 8.8903\n",
            "📉 Epoch 3, Loss: 8.7354\n",
            "📉 Epoch 4, Loss: 8.5937\n",
            "📉 Epoch 5, Loss: 8.6547\n",
            "📉 Epoch 6, Loss: 8.5244\n",
            "📉 Epoch 7, Loss: 8.7121\n",
            "📉 Epoch 8, Loss: 8.6801\n",
            "📉 Epoch 9, Loss: 8.6235\n",
            "📉 Epoch 10, Loss: 8.6469\n",
            "📉 Epoch 11, Loss: 8.6087\n",
            "📉 Epoch 12, Loss: 8.7386\n",
            "📉 Epoch 13, Loss: 8.6684\n",
            "📉 Epoch 14, Loss: 8.6931\n",
            "📉 Epoch 15, Loss: 8.5563\n",
            "📉 Epoch 16, Loss: 8.6618\n",
            "📉 Epoch 17, Loss: 8.5670\n",
            "📉 Epoch 18, Loss: 8.6099\n",
            "📉 Epoch 19, Loss: 8.4373\n",
            "📉 Epoch 20, Loss: 8.5925\n",
            "📉 Epoch 21, Loss: 8.5894\n",
            "📉 Epoch 22, Loss: 8.5945\n",
            "📉 Epoch 23, Loss: 8.4784\n",
            "📉 Epoch 24, Loss: 8.5052\n",
            "📉 Epoch 25, Loss: 8.4735\n",
            "📉 Epoch 26, Loss: 8.5075\n",
            "📉 Epoch 27, Loss: 8.5167\n",
            "📉 Epoch 28, Loss: 8.4486\n",
            "📉 Epoch 29, Loss: 8.6843\n",
            "📉 Epoch 30, Loss: 8.6084\n",
            "📉 Epoch 31, Loss: 8.5728\n",
            "📉 Epoch 32, Loss: 8.6258\n",
            "📉 Epoch 33, Loss: 8.4891\n",
            "📉 Epoch 34, Loss: 8.6731\n",
            "📉 Epoch 35, Loss: 8.5401\n",
            "📉 Epoch 36, Loss: 8.6655\n",
            "📉 Epoch 37, Loss: 8.4896\n",
            "📉 Epoch 38, Loss: 8.5287\n",
            "📉 Epoch 39, Loss: 8.5311\n",
            "📉 Epoch 40, Loss: 8.6249\n",
            "📉 Epoch 41, Loss: 8.5085\n",
            "📉 Epoch 42, Loss: 8.6304\n",
            "📉 Epoch 43, Loss: 8.5319\n",
            "📉 Epoch 44, Loss: 8.6097\n",
            "📉 Epoch 45, Loss: 8.5590\n",
            "📉 Epoch 46, Loss: 8.6991\n",
            "📉 Epoch 47, Loss: 8.6465\n",
            "📉 Epoch 48, Loss: 8.5912\n",
            "📉 Epoch 49, Loss: 8.6374\n"
          ]
        }
      ],
      "source": [
        "gpt_emb_dim = embedding_global.shape[0]\n",
        "\n",
        "predictor_skip_gpt_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip.size(1),\n",
        "    hidden_channels=z_skip.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=gpt_emb_dim\n",
        ").to(device)\n",
        "predictor_skip_gpt_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_skip_gpt_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skip_gpt_final.train()\n",
        "\n",
        "# Entrenamiento del predictor con GPT concatenado al final (positivos + negativos)\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(split_edge[\"train\"][\"edge\"].size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        # Edges positivos y negativos del batch\n",
        "        pos_edge = split_edge[\"train\"][\"edge\"][perm].t().to(device)\n",
        "        neg_edge = split_edge[\"train\"][\"edge_neg\"][perm].t().to(device)\n",
        "\n",
        "        # Concatenamos edges y labels\n",
        "        edge = torch.cat([pos_edge, neg_edge], dim=1)\n",
        "        label = torch.cat([\n",
        "            torch.ones(pos_edge.size(1), device=device),\n",
        "            torch.zeros(neg_edge.size(1), device=device)\n",
        "        ])\n",
        "\n",
        "        # Entrenamiento\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skip_gpt_final(\n",
        "            z_skip[edge[0]], z_skip[edge[1]],\n",
        "            embedding_global.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  437 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  1056 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  1628 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  1992 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  2358 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "RESEEEEETTTTT UNOOOOOOOO\n",
            "\n",
            "🔬 Resultados SkipConnSAGE (con embedding GPT en la capa final):\n",
            "Hits@10: 0.0033\n",
            "Hits@20: 0.0079\n",
            "Hits@30: 0.0122\n",
            "Hits@40: 0.0149\n",
            "Hits@50: 0.0177\n",
            "Accuracy: 0.7774\n"
          ]
        }
      ],
      "source": [
        "wrapped_predictor = WrappedPredictor(predictor_skip_gpt_final, embedding_global)\n",
        "wrapped_predictor.eval()\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "# Ejecutamos la evaluación\n",
        "with torch.no_grad():\n",
        "    results_skipconn_gpt_final = test(\n",
        "        model=dummy_model,\n",
        "        predictor=wrapped_predictor,\n",
        "        x=z_skip,\n",
        "        adj_t=adj_t,\n",
        "        split_edge=split_edge[\"valid\"],\n",
        "        evaluator=eval,\n",
        "        batch_size=64 * 1024\n",
        "    )\n",
        "\n",
        "print(\"\\n🔬 Resultados SkipConnSAGE (con embedding GPT en la capa final):\")\n",
        "for k, v in results_skipconn_gpt_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Tabla comparativa de resultados:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Base GNN (DotProduct)</th>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT input</th>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT final</th>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConnSAGE</th>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT input</th>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT final</th>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Hits@10   Hits@20   Hits@30   Hits@40   Hits@50  \\\n",
              "Base GNN (DotProduct)  0.132565  0.171662  0.225854  0.250096  0.273229   \n",
              "Base GNN + GPT input   0.117703  0.176794  0.197260  0.208849  0.219119   \n",
              "Base GNN + GPT final   0.032752  0.044678  0.057555  0.062597  0.072718   \n",
              "SkipConnSAGE           0.000225  0.000494  0.000802  0.001041  0.001326   \n",
              "SkipConn + GPT input   0.000809  0.001049  0.001288  0.001858  0.002277   \n",
              "SkipConn + GPT final   0.003274  0.007911  0.012196  0.014923  0.017664   \n",
              "\n",
              "                       Accuracy  \n",
              "Base GNN (DotProduct)    0.9010  \n",
              "Base GNN + GPT input     0.8920  \n",
              "Base GNN + GPT final     0.8618  \n",
              "SkipConnSAGE             0.7420  \n",
              "SkipConn + GPT input     0.7520  \n",
              "SkipConn + GPT final     0.7774  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparative_results = {\n",
        "    \"Base GNN (DotProduct)\": results_sage,\n",
        "    \"Base GNN + GPT input\": results_sage_input,\n",
        "    \"Base GNN + GPT final\": results_sage_final,\n",
        "    \"SkipConnSAGE\": results_skipconn,\n",
        "    \"SkipConn + GPT input\": results_skipconn_gpt_input,\n",
        "    \"SkipConn + GPT final\": results_skipconn_gpt_final\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(comparative_results).T[\n",
        "    [\"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"]\n",
        "]\n",
        "\n",
        "print(\"\\n📊 Tabla comparativa de resultados:\")\n",
        "display(df_results)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_results[\"Accuracy\"].plot(kind=\"bar\", figsize=(10, 5), title=\"Accuracy por modelo\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_results[[f\"Hits@{k}\" for k in [10, 20, 30, 40, 50]]].plot(kind=\"line\", marker='o', figsize=(10, 6), title=\"Hits@K por modelo\")\n",
        "plt.ylabel(\"Proporción\")\n",
        "plt.xlabel(\"Modelo\")\n",
        "plt.xticks(ticks=range(len(df_results.index)), labels=df_results.index, rotation=45, ha='right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_results.loc[\n",
        "    [\"SkipConnSAGE\", \"SkipConn + GPT input\", \"SkipConn + GPT final\"],\n",
        "    [f\"Hits@{k}\" for k in [10, 20, 30, 40, 50]] + [\"Accuracy\"]\n",
        "].plot(kind=\"bar\", figsize=(10, 6), title=\"Comparación de variantes SkipConnSAGE\")\n",
        "plt.ylabel(\"Proporción\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Tabla comparativa de resultados:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>GNN model</th>\n",
              "      <th>Prompt type</th>\n",
              "      <th>Embedding source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Base GNN (DotProduct)</th>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT input</th>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT final</th>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConnSAGE</th>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT input</th>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT final</th>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Hits@10   Hits@20   Hits@30   Hits@40   Hits@50  \\\n",
              "Base GNN (DotProduct)  0.132565  0.171662  0.225854  0.250096  0.273229   \n",
              "Base GNN + GPT input   0.117703  0.176794  0.197260  0.208849  0.219119   \n",
              "Base GNN + GPT final   0.032752  0.044678  0.057555  0.062597  0.072718   \n",
              "SkipConnSAGE           0.000225  0.000494  0.000802  0.001041  0.001326   \n",
              "SkipConn + GPT input   0.000809  0.001049  0.001288  0.001858  0.002277   \n",
              "SkipConn + GPT final   0.003274  0.007911  0.012196  0.014923  0.017664   \n",
              "\n",
              "                       Accuracy     GNN model Prompt type Embedding source  \n",
              "Base GNN (DotProduct)    0.9010          SAGE     default             BERT  \n",
              "Base GNN + GPT input     0.8920          SAGE     default             BERT  \n",
              "Base GNN + GPT final     0.8618          SAGE     default             BERT  \n",
              "SkipConnSAGE             0.7420  SkipConnSAGE     default             BERT  \n",
              "SkipConn + GPT input     0.7520  SkipConnSAGE     default             BERT  \n",
              "SkipConn + GPT final     0.7774  SkipConnSAGE     default             BERT  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparative_results = {\n",
        "    \"Base GNN (DotProduct)\": results_sage,\n",
        "    \"Base GNN + GPT input\": results_sage_input,\n",
        "    \"Base GNN + GPT final\": results_sage_final,\n",
        "    \"SkipConnSAGE\": results_skipconn,\n",
        "    \"SkipConn + GPT input\": results_skipconn_gpt_input,\n",
        "    \"SkipConn + GPT final\": results_skipconn_gpt_final\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(comparative_results).T[\n",
        "    [\"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"]\n",
        "]\n",
        "\n",
        "df_results[\"GNN model\"] = [\n",
        "    \"SAGE\",  # Base GNN (DotProduct)\n",
        "    \"SAGE\",  # Base GNN + GPT input\n",
        "    \"SAGE\",  # NeuralLinkPredictor\n",
        "    \"SkipConnSAGE\",\n",
        "    \"SkipConnSAGE\",\n",
        "    \"SkipConnSAGE\"\n",
        "]\n",
        "\n",
        "df_results[\"Prompt type\"] = \"default\"\n",
        "df_results[\"Embedding source\"] = \"BERT\"\n",
        "\n",
        "print(\"\\n📊 Tabla comparativa de resultados:\")\n",
        "display(df_results)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PostProcessSAGE(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_dimension, out_channels, num_conv_layers,\n",
        "                num_linear_layers, dropout):\n",
        "    super(PostProcessSAGE, self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.lins = torch.nn.ModuleList()\n",
        "\n",
        "    self.convs.append(SAGEConv(in_channels, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "    for _ in range(num_conv_layers - 1):\n",
        "      self.convs.append(SAGEConv(hidden_dimension, hidden_dimension, normalize=True, aggr=\"add\"))\n",
        "\n",
        "    for _ in range(num_linear_layers - 1):\n",
        "      self.lins.append(torch.nn.Linear(hidden_dimension, hidden_dimension))\n",
        "    self.lins.append(torch.nn.Linear(hidden_dimension, out_channels))\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for conv in self.convs:\n",
        "      conv.reset_parameters()\n",
        "\n",
        "  def forward(self, x, adj_t):\n",
        "    for conv in self.convs[:-1]:\n",
        "      x = conv(x, adj_t)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.convs[-1](x, adj_t)\n",
        "\n",
        "    # Post-process\n",
        "    for lin in self.lins[:-1]:\n",
        "      x = lin(x)\n",
        "      x = F.relu(x)\n",
        "    x = self.lins[-1](x)\n",
        "    return x\n",
        "\n",
        "# copied from above (\"Base GraphSage Model\") in order to be able to run this cell without running the previous ones\n",
        "class DotProductLinkPredictor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DotProductLinkPredictor, self).__init__()\n",
        "\n",
        "  def forward(self, x_i, x_j):\n",
        "    # dot product of node embeddings, that is meant to be the edge embedding\n",
        "    out = (x_i*x_j).sum(-1)\n",
        "    return torch.sigmoid(out)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8061\n",
            "Epoch 1 has loss 11.152\n",
            "Epoch 2 has loss 10.2129\n",
            "Epoch 3 has loss 11.2978\n",
            "Epoch 4 has loss 10.2759\n",
            "Epoch 5 has loss 9.3099\n",
            "Epoch 6 has loss 8.5448\n",
            "Epoch 7 has loss 7.9527\n",
            "Epoch 8 has loss 7.8803\n",
            "Epoch 9 has loss 7.6593\n",
            "Epoch 10 has loss 7.4371\n",
            "Epoch 11 has loss 7.4729\n",
            "Epoch 12 has loss 7.2502\n",
            "Epoch 13 has loss 6.7462\n",
            "Epoch 14 has loss 6.9419\n",
            "Epoch 15 has loss 6.5565\n",
            "Epoch 16 has loss 6.3561\n",
            "Epoch 17 has loss 6.1552\n",
            "Epoch 18 has loss 6.1522\n",
            "Epoch 19 has loss 6.1141\n",
            "Epoch 20 has loss 6.0648\n",
            "Epoch 21 has loss 5.9676\n",
            "Epoch 22 has loss 5.9663\n",
            "Epoch 23 has loss 5.72\n",
            "Epoch 24 has loss 5.5955\n",
            "Epoch 25 has loss 5.6125\n",
            "Epoch 26 has loss 5.5918\n",
            "Epoch 27 has loss 5.6241\n",
            "Epoch 28 has loss 5.5723\n",
            "Epoch 29 has loss 5.5797\n",
            "Epoch 30 has loss 5.3562\n",
            "Epoch 31 has loss 5.2931\n",
            "Epoch 32 has loss 5.2844\n",
            "Epoch 33 has loss 5.3571\n",
            "Epoch 34 has loss 5.1666\n",
            "Epoch 35 has loss 5.0655\n",
            "Epoch 36 has loss 5.0545\n",
            "Epoch 37 has loss 5.0742\n",
            "Epoch 38 has loss 4.9783\n",
            "Epoch 39 has loss 4.953\n",
            "Epoch 40 has loss 4.9645\n",
            "Epoch 41 has loss 5.0666\n",
            "Epoch 42 has loss 4.9269\n",
            "Epoch 43 has loss 4.9885\n",
            "Epoch 44 has loss 4.8871\n",
            "Epoch 45 has loss 4.7968\n",
            "Epoch 46 has loss 4.9196\n",
            "Epoch 47 has loss 4.9887\n",
            "Epoch 48 has loss 4.8444\n",
            "Epoch 49 has loss 4.7063\n",
            "  695 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  28097 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  35469 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  39233 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  43043 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📊 Resultados de PostProcessSAGE (sin GPT):\n",
            "Hits@10: 0.0052\n",
            "Hits@20: 0.2105\n",
            "Hits@30: 0.2657\n",
            "Hits@40: 0.2939\n",
            "Hits@50: 0.3224\n",
            "Accuracy: 0.8781\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📊 Resultados de PostProcessSAGE (sin GPT):\")\n",
        "for k, v in results_post_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 14.5516\n",
            "Epoch 1 has loss 11.9049\n",
            "Epoch 2 has loss 9.7219\n",
            "Epoch 3 has loss 9.6376\n",
            "Epoch 4 has loss 9.6118\n",
            "Epoch 5 has loss 9.1526\n",
            "Epoch 6 has loss 8.6372\n",
            "Epoch 7 has loss 7.9888\n",
            "Epoch 8 has loss 7.9607\n",
            "Epoch 9 has loss 7.8096\n",
            "Epoch 10 has loss 7.8458\n",
            "Epoch 11 has loss 7.452\n",
            "Epoch 12 has loss 7.2762\n",
            "Epoch 13 has loss 7.3729\n",
            "Epoch 14 has loss 7.0534\n",
            "Epoch 15 has loss 6.7361\n",
            "Epoch 16 has loss 6.4547\n",
            "Epoch 17 has loss 6.1488\n",
            "Epoch 18 has loss 6.0313\n",
            "Epoch 19 has loss 5.8938\n",
            "Epoch 20 has loss 5.8509\n",
            "Epoch 21 has loss 5.8637\n",
            "Epoch 22 has loss 5.779\n",
            "Epoch 23 has loss 5.7662\n",
            "Epoch 24 has loss 5.7601\n",
            "Epoch 25 has loss 5.8305\n",
            "Epoch 26 has loss 5.7098\n",
            "Epoch 27 has loss 5.7045\n",
            "Epoch 28 has loss 5.6268\n",
            "Epoch 29 has loss 5.6017\n",
            "Epoch 30 has loss 5.5661\n",
            "Epoch 31 has loss 5.4976\n",
            "Epoch 32 has loss 5.3937\n",
            "Epoch 33 has loss 5.3867\n",
            "Epoch 34 has loss 5.318\n",
            "Epoch 35 has loss 5.2424\n",
            "Epoch 36 has loss 5.47\n",
            "Epoch 37 has loss 5.285\n",
            "Epoch 38 has loss 5.0786\n",
            "Epoch 39 has loss 5.0485\n",
            "Epoch 40 has loss 4.9805\n",
            "Epoch 41 has loss 4.981\n",
            "Epoch 42 has loss 4.9131\n",
            "Epoch 43 has loss 4.9291\n",
            "Epoch 44 has loss 4.9069\n",
            "Epoch 45 has loss 4.76\n",
            "Epoch 46 has loss 4.8337\n",
            "Epoch 47 has loss 4.8792\n",
            "Epoch 48 has loss 4.6815\n",
            "Epoch 49 has loss 4.6903\n",
            "  979 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  979 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  17744 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  20504 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  24201 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📊 Resultados CORREGIDOS de PostProcessSAGE + GPT como input (con NeuralLinkPredictor):\n",
            "Hits@10: 0.0073\n",
            "Hits@20: 0.0073\n",
            "Hits@30: 0.1329\n",
            "Hits@40: 0.1536\n",
            "Hits@50: 0.1813\n",
            "Accuracy: 0.8706\n"
          ]
        }
      ],
      "source": [
        "aug_emb_post = torch.cat([\n",
        "    emb, embedding_global.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb_post.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_post, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_gpt_input = test(model, predictor, aug_emb_post, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📊 Resultados CORREGIDOS de PostProcessSAGE + GPT como input (con NeuralLinkPredictor):\")\n",
        "for k, v in results_post_gpt_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8618\n",
            "Epoch 1 has loss 11.6664\n",
            "Epoch 2 has loss 13.0989\n",
            "Epoch 3 has loss 11.0273\n",
            "Epoch 4 has loss 10.5743\n",
            "Epoch 5 has loss 10.4257\n",
            "Epoch 6 has loss 9.9373\n",
            "Epoch 7 has loss 9.5618\n",
            "Epoch 8 has loss 9.4622\n",
            "Epoch 9 has loss 9.3814\n",
            "Epoch 10 has loss 9.4645\n",
            "Epoch 11 has loss 9.3639\n",
            "Epoch 12 has loss 9.3355\n",
            "Epoch 13 has loss 9.2904\n",
            "Epoch 14 has loss 9.2585\n",
            "Epoch 15 has loss 9.2305\n",
            "Epoch 16 has loss 9.2607\n",
            "Epoch 17 has loss 9.1923\n",
            "Epoch 18 has loss 9.1627\n",
            "Epoch 19 has loss 9.0994\n",
            "Epoch 20 has loss 9.4528\n",
            "Epoch 21 has loss 9.1208\n",
            "Epoch 22 has loss 9.2758\n",
            "Epoch 23 has loss 9.1502\n",
            "Epoch 24 has loss 9.4587\n",
            "Epoch 25 has loss 9.2872\n",
            "Epoch 26 has loss 9.0918\n",
            "Epoch 27 has loss 9.9835\n",
            "Epoch 28 has loss 9.6576\n",
            "Epoch 29 has loss 9.2894\n",
            "Epoch 30 has loss 9.2311\n",
            "Epoch 31 has loss 9.2097\n",
            "Epoch 32 has loss 9.1468\n",
            "Epoch 33 has loss 9.1611\n",
            "Epoch 34 has loss 9.0708\n",
            "Epoch 35 has loss 9.0967\n",
            "Epoch 36 has loss 9.6069\n",
            "Epoch 37 has loss 9.2204\n",
            "Epoch 38 has loss 9.0751\n",
            "Epoch 39 has loss 9.0948\n",
            "Epoch 40 has loss 9.0775\n",
            "Epoch 41 has loss 9.0124\n",
            "Epoch 42 has loss 9.003\n",
            "Epoch 43 has loss 8.9641\n",
            "Epoch 44 has loss 9.221\n",
            "Epoch 45 has loss 9.3648\n",
            "Epoch 46 has loss 9.261\n",
            "Epoch 47 has loss 9.1826\n",
            "Epoch 48 has loss 9.1139\n",
            "Epoch 49 has loss 9.0635\n",
            "📉 Epoch 0, Loss: 9.5563\n",
            "📉 Epoch 1, Loss: 8.5789\n",
            "📉 Epoch 2, Loss: 8.3975\n",
            "📉 Epoch 3, Loss: 8.2142\n",
            "📉 Epoch 4, Loss: 8.0529\n",
            "📉 Epoch 5, Loss: 7.9307\n",
            "📉 Epoch 6, Loss: 7.8483\n",
            "📉 Epoch 7, Loss: 7.7142\n",
            "📉 Epoch 8, Loss: 7.6158\n",
            "📉 Epoch 9, Loss: 7.5706\n",
            "📉 Epoch 10, Loss: 7.5261\n",
            "📉 Epoch 11, Loss: 7.5115\n",
            "📉 Epoch 12, Loss: 7.4869\n",
            "📉 Epoch 13, Loss: 7.4677\n",
            "📉 Epoch 14, Loss: 7.4497\n",
            "📉 Epoch 15, Loss: 7.4477\n",
            "📉 Epoch 16, Loss: 7.4356\n",
            "📉 Epoch 17, Loss: 7.4266\n",
            "📉 Epoch 18, Loss: 7.4242\n",
            "📉 Epoch 19, Loss: 7.4120\n",
            "📉 Epoch 20, Loss: 7.4132\n",
            "📉 Epoch 21, Loss: 7.4084\n",
            "📉 Epoch 22, Loss: 7.4013\n",
            "📉 Epoch 23, Loss: 7.3910\n",
            "📉 Epoch 24, Loss: 7.3862\n",
            "📉 Epoch 25, Loss: 7.3897\n",
            "📉 Epoch 26, Loss: 7.3781\n",
            "📉 Epoch 27, Loss: 7.3778\n",
            "📉 Epoch 28, Loss: 7.3721\n",
            "📉 Epoch 29, Loss: 7.3776\n",
            "📉 Epoch 30, Loss: 7.3739\n",
            "📉 Epoch 31, Loss: 7.3679\n",
            "📉 Epoch 32, Loss: 7.3623\n",
            "📉 Epoch 33, Loss: 7.3618\n",
            "📉 Epoch 34, Loss: 7.3603\n",
            "📉 Epoch 35, Loss: 7.3563\n",
            "📉 Epoch 36, Loss: 7.3551\n",
            "📉 Epoch 37, Loss: 7.3576\n",
            "📉 Epoch 38, Loss: 7.3571\n",
            "📉 Epoch 39, Loss: 7.3524\n",
            "📉 Epoch 40, Loss: 7.3491\n",
            "📉 Epoch 41, Loss: 7.3484\n",
            "📉 Epoch 42, Loss: 7.3433\n",
            "📉 Epoch 43, Loss: 7.3498\n",
            "📉 Epoch 44, Loss: 7.3445\n",
            "📉 Epoch 45, Loss: 7.3405\n",
            "📉 Epoch 46, Loss: 7.3402\n",
            "📉 Epoch 47, Loss: 7.3408\n",
            "📉 Epoch 48, Loss: 7.3461\n",
            "📉 Epoch 49, Loss: 7.3351\n",
            "  5124 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  6466 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  7134 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  8171 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  9125 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📊 Resultados de PostProcessSAGE + capa GPT final:\n",
            "Hits@10: 0.0384\n",
            "Hits@20: 0.0484\n",
            "Hits@30: 0.0534\n",
            "Hits@40: 0.0612\n",
            "Hits@50: 0.0684\n",
            "Accuracy: 0.8070\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post = model(emb, adj_t)\n",
        "\n",
        "predictor_post_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post.size(1),\n",
        "    hidden_channels=z_post.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global.size(0)\n",
        ").to(device)\n",
        "\n",
        "predictor_post_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "predictor_post_final.train()\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(split_edge[\"train\"][\"edge\"].size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        pos_edge = split_edge[\"train\"][\"edge\"][perm].t().to(device)\n",
        "        neg_edge = split_edge[\"train\"][\"edge_neg\"][perm].t().to(device)\n",
        "        edge = torch.cat([pos_edge, neg_edge], dim=1)\n",
        "        label = torch.cat([\n",
        "            torch.ones(pos_edge.size(1)),\n",
        "            torch.zeros(neg_edge.size(1))\n",
        "        ]).to(device)\n",
        "\n",
        "        pred = predictor_post_final(\n",
        "            z_post[edge[0]], z_post[edge[1]],\n",
        "            embedding_global.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(pred, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def forward(self, x, adj_t): return x\n",
        "\n",
        "class WrappedPredictor(torch.nn.Module):\n",
        "    def __init__(self, predictor, gpt_embedding):\n",
        "        super().__init__()\n",
        "        self.predictor = predictor\n",
        "        self.gpt_embedding = gpt_embedding\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        return self.predictor(\n",
        "            x_i, x_j,\n",
        "            self.gpt_embedding.unsqueeze(0).expand(x_i.size(0), -1)\n",
        "        )\n",
        "\n",
        "wrapped_predictor_post = WrappedPredictor(predictor_post_final, embedding_global).to(device)\n",
        "wrapped_predictor_post.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    results_post_gpt_final = test(\n",
        "        model=DummyModel(),\n",
        "        predictor=wrapped_predictor_post,\n",
        "        x=z_post,\n",
        "        adj_t=adj_t,\n",
        "        split_edge=split_edge[\"valid\"],\n",
        "        evaluator=eval,\n",
        "        batch_size=64 * 1024\n",
        "    )\n",
        "\n",
        "print(\"\\n📊 Resultados de PostProcessSAGE + capa GPT final:\")\n",
        "for k, v in results_post_gpt_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Tabla comparativa de resultados:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>GNN model</th>\n",
              "      <th>Prompt type</th>\n",
              "      <th>Embedding source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Base GNN (DotProduct)</th>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT input</th>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Base GNN + GPT final</th>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "      <td>SAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConnSAGE</th>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT input</th>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SkipConn + GPT final</th>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PostProcessSAGE</th>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PostProcessSAGE + GPT input</th>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PostProcessSAGE + GPT final</th>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>default</td>\n",
              "      <td>BERT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Hits@10   Hits@20   Hits@30   Hits@40   Hits@50  \\\n",
              "Base GNN (DotProduct)        0.132565  0.171662  0.225854  0.250096  0.273229   \n",
              "Base GNN + GPT input         0.117703  0.176794  0.197260  0.208849  0.219119   \n",
              "Base GNN + GPT final         0.032752  0.044678  0.057555  0.062597  0.072718   \n",
              "SkipConnSAGE                 0.000225  0.000494  0.000802  0.001041  0.001326   \n",
              "SkipConn + GPT input         0.000809  0.001049  0.001288  0.001858  0.002277   \n",
              "SkipConn + GPT final         0.003274  0.007911  0.012196  0.014923  0.017664   \n",
              "PostProcessSAGE              0.005206  0.210482  0.265707  0.293904  0.322446   \n",
              "PostProcessSAGE + GPT input  0.007334  0.007334  0.132925  0.153601  0.181296   \n",
              "PostProcessSAGE + GPT final  0.038385  0.048438  0.053443  0.061211  0.068358   \n",
              "\n",
              "                             Accuracy        GNN model Prompt type  \\\n",
              "Base GNN (DotProduct)          0.9010             SAGE     default   \n",
              "Base GNN + GPT input           0.8920             SAGE     default   \n",
              "Base GNN + GPT final           0.8618             SAGE     default   \n",
              "SkipConnSAGE                   0.7420     SkipConnSAGE     default   \n",
              "SkipConn + GPT input           0.7520     SkipConnSAGE     default   \n",
              "SkipConn + GPT final           0.7774     SkipConnSAGE     default   \n",
              "PostProcessSAGE                0.8781  PostProcessSAGE     default   \n",
              "PostProcessSAGE + GPT input    0.8706  PostProcessSAGE     default   \n",
              "PostProcessSAGE + GPT final    0.8070  PostProcessSAGE     default   \n",
              "\n",
              "                            Embedding source  \n",
              "Base GNN (DotProduct)                   BERT  \n",
              "Base GNN + GPT input                    BERT  \n",
              "Base GNN + GPT final                    BERT  \n",
              "SkipConnSAGE                            BERT  \n",
              "SkipConn + GPT input                    BERT  \n",
              "SkipConn + GPT final                    BERT  \n",
              "PostProcessSAGE                         BERT  \n",
              "PostProcessSAGE + GPT input             BERT  \n",
              "PostProcessSAGE + GPT final             BERT  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparative_results = {\n",
        "    \"Base GNN (DotProduct)\": results_sage,\n",
        "    \"Base GNN + GPT input\": results_sage_input,\n",
        "    \"Base GNN + GPT final\": results_sage_final,\n",
        "    \"SkipConnSAGE\": results_skipconn,\n",
        "    \"SkipConn + GPT input\": results_skipconn_gpt_input,\n",
        "    \"SkipConn + GPT final\": results_skipconn_gpt_final,\n",
        "    \"PostProcessSAGE\": results_post_base,\n",
        "    \"PostProcessSAGE + GPT input\": results_post_gpt_input,\n",
        "    \"PostProcessSAGE + GPT final\": results_post_gpt_final,\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(comparative_results).T[\n",
        "    [\"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"]\n",
        "]\n",
        "\n",
        "df_results[\"GNN model\"] = [\n",
        "    \"SAGE\",  # NeuralLinkPredictor\n",
        "    \"SAGE\",  # Neural + GPT input\n",
        "    \"SAGE\",  # Neural + GPT final\n",
        "    \"SkipConnSAGE\",\n",
        "    \"SkipConnSAGE\",\n",
        "    \"SkipConnSAGE\",\n",
        "    \"PostProcessSAGE\",\n",
        "    \"PostProcessSAGE\",\n",
        "    \"PostProcessSAGE\",\n",
        "]\n",
        "\n",
        "df_results[\"Prompt type\"] = \"default\"\n",
        "df_results[\"Embedding source\"] = \"BERT\"\n",
        "\n",
        "print(\"\\n📊 Tabla comparativa de resultados:\")\n",
        "display(df_results)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Model Embedding Source Prompt Type GPT Injection     Predictor  \\\n",
              "0         SAGEConv          classic           -             -        Neural   \n",
              "1         SAGEConv    classic + GPT     default         input        Neural   \n",
              "2         SAGEConv          classic     default         final  Neural + GPT   \n",
              "3     SkipConnSAGE          classic           -             -        Neural   \n",
              "4     SkipConnSAGE    classic + GPT     default         input        Neural   \n",
              "5     SkipConnSAGE          classic     default         final  Neural + GPT   \n",
              "6  PostProcessSAGE          classic           -             -        Neural   \n",
              "7  PostProcessSAGE    classic + GPT     default         input        Neural   \n",
              "8  PostProcessSAGE          classic     default         final  Neural + GPT   \n",
              "\n",
              "   Epochs   Hits@10   Hits@20   Hits@30   Hits@40   Hits@50  Accuracy  \n",
              "0      50  0.132565  0.171662  0.225854  0.250096  0.273229    0.9010  \n",
              "1      50  0.117703  0.176794  0.197260  0.208849  0.219119    0.8920  \n",
              "2      50  0.032752  0.044678  0.057555  0.062597  0.072718    0.8618  \n",
              "3     100  0.000225  0.000494  0.000802  0.001041  0.001326    0.7420  \n",
              "4     100  0.000809  0.001049  0.001288  0.001858  0.002277    0.7520  \n",
              "5     100  0.003274  0.007911  0.012196  0.014923  0.017664    0.7774  \n",
              "6      50  0.005206  0.210482  0.265707  0.293904  0.322446    0.8781  \n",
              "7      50  0.007334  0.007334  0.132925  0.153601  0.181296    0.8706  \n",
              "8      50  0.038385  0.048438  0.053443  0.061211  0.068358    0.8070  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_results = [\n",
        "    # SAGEConv\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_input},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 50, **results_sage_final},\n",
        "\n",
        "    # SkipConnSAGE\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 100, **results_skipconn},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 100, **results_skipconn_gpt_input},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 100, **results_skipconn_gpt_final},\n",
        "\n",
        "    # PostProcessSAGE\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_base},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_gpt_input},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 50, **results_post_gpt_final},\n",
        "]\n",
        "\n",
        "# Convertimos en DataFrame ordenado\n",
        "df_summary = pd.DataFrame(model_results)\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import IPython.display as display\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "comparative_results = {\n",
        "    \"Base GNN (DotProduct)\": results_sage,\n",
        "    \"Base GNN + GPT input\": results_sage_input,\n",
        "    \"Base GNN + GPT final\": results_sage_final,\n",
        "    \"SkipConnSAGE\": results_skipconn,\n",
        "    \"SkipConn + GPT input\": results_skipconn_gpt_input,\n",
        "    \"SkipConn + GPT final\": results_skipconn_gpt_final,\n",
        "    \"PostProcessSAGE\": results_post_base,\n",
        "    \"PostProcess + GPT input\": results_post_gpt_input,\n",
        "    \"PostProcess + GPT final\": results_post_gpt_final,\n",
        "}\n",
        "\n",
        "# Guardamos el archivo\n",
        "with open(\"results_backup.pkl\", \"wb\") as f:\n",
        "    pickle.dump(comparative_results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"results_backup.pkl\", \"rb\") as f:\n",
        "    comparative_results = pickle.load(f)\n",
        "\n",
        "results_base = comparative_results[\"Base GNN (DotProduct)\"]\n",
        "results_with_gpt = comparative_results[\"Base GNN + GPT input\"]\n",
        "results_final_gpt = comparative_results[\"Base GNN + GPT final\"]\n",
        "results_skipconn = comparative_results[\"SkipConnSAGE\"]\n",
        "results_skipconn_gpt_input = comparative_results[\"SkipConn + GPT input\"]\n",
        "results_skipconn_gpt_final = comparative_results[\"SkipConn + GPT final\"]\n",
        "results_post_base = comparative_results[\"PostProcessSAGE\"]\n",
        "results_post_gpt_input = comparative_results[\"PostProcess + GPT input\"]\n",
        "results_post_gpt_final = comparative_results[\"PostProcess + GPT final\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_results[\"Accuracy\"].plot(kind=\"bar\", figsize=(10, 5), title=\"Accuracy por modelo\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9kAAAJOCAYAAACjoMSlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4VNXWwOHflGQySUjvIaRRQwtdeov0zkVUkIAKXpCLigWwIMULCrbPCwrWoKCoIEgvAQIoSO81kAakEUivk5n5/ggZCQmQSgKu9z7zXHLOPvusmTMxs2bvs5fCaDQaEUIIIYQQQgghRIUpqzsAIYQQQgghhBDiUSFJthBCCCGEEEIIUUkkyRZCCCGEEEIIISqJJNlCCCGEEEIIIUQlkSRbCCGEEEIIIYSoJJJkCyGEEEIIIYQQlUSSbCGEEEIIIYQQopJIki2EEEIIIYQQQlQSSbKFEEIIIYQQQohKIkm2EEIIUUl8fHwYO3ZsdYchKlFYWBgKhYKwsLAyHxsSEoJCoSAqKqrS4xJCCFFzSZIthBBClKAwQTp8+HCJ+7t160aTJk3u2cfZs2eZNWtWpSdZOTk56HS6+7a723NITU2lbdu2WFhYsGXLlkqNTQghhPinkyRbCCGEqCQXLlzgq6++Mv189uxZZs+eXSlJ9qFDh3jmmWdwdXVFq9Wi0Wjw8vJiypQpXLp0qdT9pKWl0atXL06ePMmaNWvo06dPhWMTQgghxN8kyRZCCCEqiUajwczMrFL7zM/PZ/LkybRr147o6GjeeOMN1q9fz6pVq5g0aRJ79+6ladOmLF68+L59paen07t3b44fP87q1avp27dvpcZaXllZWdUdghBCCFFpJMkWQgghKsnt92SHhIQwYsQIALp3745CoShyb+/hw4fp3bs3Tk5OaLVafH19efbZZ4v1OW7cOH788Uc2bdrEnj17ePXVVxkwYADDhg1jxowZHDt2jCVLlvDaa6+xZMmSu8aWkZFBnz59OHr0KKtXr6Z///73fT6F08337NnDCy+8gKOjIzY2NowZM4bk5ORi7T///HMaN26MRqPBw8ODF198kZSUlCJtCqfZHzlyhC5dumBpacmbb7551xjGjh2LtbU1MTExDBgwAGtrazw9PU1fKpw6dYoePXpgZWWFt7c3P/74Y7E+IiIiGDFiBA4ODlhaWvLYY4+xcePGYu2uXr3KkCFDsLKywsXFhVdeeYXc3NwS4zpw4AB9+vTB1tYWS0tLunbtyp9//nmvl7NMr5MQQoiHl7q6AxBCCCFqstTUVJKSkoptv9890V26dGHKlCl89tlnvPnmmzRq1AiARo0akZiYSK9evXB2dmb69OnY2dkRFRXFb7/9VqSPH374gTVr1nDgwAEaN24MgNFoJDMzE2trawCSkpJ45plncHJyYsSIEfTt2xdvb+8i/WRmZtK3b18OHTrEqlWrGDBgQJleg8mTJ2NnZ8esWbO4cOECX3zxBdHR0aZFwQBmzZrF7NmzCQoKYuLEiaZ2hw4d4s8//ywywn/jxg369u3Lk08+yejRo3F1db3n+fV6PX379qVLly4sWLCAFStWMHnyZKysrHjrrbcYNWoUw4YNY8mSJYwZM4b27dvj6+sLQEJCAh06dCArK4spU6bg6OjIsmXLGDRoEKtWrWLo0KEAZGdn07NnT2JiYpgyZQoeHh788MMP7Ny5s1g8O3fupG/fvrRq1Yp3330XpVLJd999R48ePdi7dy9t27a963Mpy+skhBDiIWUUQgghRDHfffedEbjno3HjxkWO8fb2NgYHB5t+/vXXX42AcdeuXUXarVmzxggYDx06dNfzGwwGo6+vr/HTTz81bfv999+NHh4eRsBYp04d49atW42AMTIy0mg0Go1Dhw41vvnmm8Weg7e3t9HMzMy4du3acr0GrVq1Mubl5Zm2L1iwwAgYf//9d6PRaDQmJiYazc3Njb169TLq9XpTu0WLFhkB47fffmva1rVrVyNgXLJkSaliCA4ONgLGefPmmbYlJycbtVqtUaFQGFeuXGnafv78eSNgfPfdd03bXn75ZSNg3Lt3r2lbenq60dfX1+jj42OK99NPPzUCxl9++cXULjMz01i3bt0i19BgMBjr1atn7N27t9FgMJjaZmVlGX19fY2PP/54sdev8PqU5XUSQgjx8JLp4kIIIcQ9LF68mO3btxd7NGvWrNx92tnZAbBhw4a7jogfOXKExMREnnvuOQCuXbvGU089Rdu2bVm9ejWvvPJKsenlQ4YMKbHUVEJCAhYWFnh5eZUr3gkTJhQZYZ04cSJqtZpNmzYBEBoaSl5eHi+//DJK5d8fLcaPH4+NjU2xqdkajYZx48aVKYbnn3/e9G87OzsaNGiAlZUVTzzxhGl7gwYNsLOzIyIiwrRt06ZNtG3blk6dOpm2WVtbM2HCBKKiojh79qypnbu7O//6179M7SwtLZkwYUKROI4fP054eDhPP/00N27cICkpiaSkJDIzM+nZsyd79uzBYDCU+BzK+joJIYR4OMl0cSGEEOIe2rZtS+vWrYttt7e3L3EaeWl07dqV4cOHM3v2bD755BO6devGkCFDePrpp9FoNEBBkt26dWvTtPAVK1bg6enJqlWrUKlUQEGyeXuy6urqyvXr14udb+nSpUydOpU+ffqwd+9eGjRoUKZ469WrV+Rna2tr3N3dTaumR0dHAxTr19zcHD8/P9P+Qp6enpibm5f6/BYWFjg7OxfZZmtrS+3atU3T1W/ffvv94tHR0bRr165Yn4XT96Ojo2nSpAnR0dHUrVu3WH93Pqfw8HAAgoOD7xpvamoq9vb2xbaX9XUSQgjxcJIkWwghhHjAFAoFq1at4q+//mL9+vVs3bqVZ599lo8++oi//voLa2trbty4gYeHh+mYqKgoWrRoYUqwgWL3/l65cgVHR8di5wsICGDTpk307NmTxx9/nD///LPco9qVQavVlqn97c+5NNuNRmOZYyqtwlHqhQsXEhgYWGKbwi9GhBBC/DPJdHEhhBCiitw5Knqnxx57jP/+978cPnyYFStWcObMGVauXAmAjY0NqampprZubm5cvny5yPG3T4s2Go188803BAUFlXiutm3bsnbtWhITE3n88cdLHPG+m8LR20IZGRnExcXh4+MDYFpo7cKFC0Xa5eXlERkZWWwhtgfJ29u7WFwA58+fN+0v/P/Lly8XS9DvPNbf3x8ouD5BQUElPu62eFlNfp2EEEJUHkmyhRBCiCpiZWUFUKw8U3JycrFkrnBUtLBkVKNGjTh06JBp5HTw4MEcO3aMmTNnEhERwd69e3n99dcBOHbsGMOHD+fq1au89NJLd42nZ8+e/PTTT1y6dIk+ffqQlpZWqufx5ZdfFrl3/IsvviA/P99UZzsoKAhzc3M+++yzIs/rm2++ITU1tVTlwqpKv379OHjwIPv37zdty8zM5Msvv8THx4eAgABTu9jYWFatWmVql5WVxZdfflmkv1atWuHv78+HH35IRkZGsfPd68uLmvw6CSGEqDwyXVwIIYSoIoGBgahUKj744ANSU1PRaDT06NGDH3/8kc8//5yhQ4fi7+9Peno6X331FTY2NvTr1w+ATp06kZeXx7p16xgyZAjNmzfnvffe4+2332bu3Lmo1Wo++ugjXnrpJYYNG0avXr3Ys2cPTk5O94xp6NChfPXVVzz77LMMGjSILVu2YGFhcc9j8vLy6NmzJ0888QQXLlzg888/p1OnTgwaNAgAZ2dnZsyYwezZs+nTpw+DBg0ytWvTpg2jR4+unBe0HKZPn85PP/1E3759mTJlCg4ODixbtozIyEhWr15tWoBs/PjxLFq0iDFjxnDkyBHc3d354YcfsLS0LNKfUqnk66+/pm/fvjRu3Jhx48bh6enJtWvX2LVrFzY2Nqxfv77EWGry6ySEEKLySJIthBBCVBE3NzeWLFnC/Pnzee6559Dr9ezatYuuXbty8OBBVq5cSUJCAra2trRt25YVK1aY6jtrNBpefvllXn31Vbp27Yq9vT0zZswgODiYy5cvU79+fVxdXWnVqhX169cvtjDYvYwbN46bN2/y2muvMWLECNasWYNaffePBIsWLWLFihXMnDkTnU7HU089xWeffVZkOvysWbNwdnZm0aJFvPLKKzg4ODBhwgTmzZtXrbWfXV1d2bdvH9OmTeN///sfOTk5NGvWjPXr1xcZOba0tGTHjh385z//4X//+x+WlpaMGjWKvn370qdPnyJ9duvWjf379zN37lwWLVpERkYGbm5utGvXjhdeeOGe8dTU10kIIUTlURircnUQIYQQQpRbTk4OHTt2RKVS8fvvv+Pu7l5iu1WrVjF06NC7LgRWXiEhIYwbN45Dhw6VuMK6EEIIIYqTe7KFEEKIGsrCwoJNmzahUCho0KAB06ZNY8+ePURHR3P+/Hm+//572rdvT3BwMEePHq3ucIUQQgiBTBcXQgghajRXV1f27t3LokWLWLRoEQsWLDDts7CwYOjQoXz//ffFalkLIYQQonpIki2EEELUcObm5kydOpWpU6cSFRXFtWvXsLCwoFGjRsUW5hJCCCFE9ZJ7soUQQgghhBBCiEoi92QLIYQQQgghhBCVRJJsIYQQQgghhBCiksg92dXEYDAQGxtLrVq1itQZFUIIIYQQQghRsxiNRtLT0/Hw8ECpvPdYtSTZ1SQ2NhYvL6/qDkMIIYQQQgghRClduXKF2rVr37ONJNnVpFatWkDBRbKxsanmaIrT6XRs27aNXr16YWZmVt3h/OPJ9ahZ5HrULHI9aha5HjWLXI+aRa5HzSLXo2ap6dcjLS0NLy8vUx53L5JkV5PCKeI2NjY1Nsm2tLTExsamRr7J/2nketQscj1qFrkeNYtcj5pFrkfNItejZpHrUbM8LNejNLf6ysJnQgghhBBCCCFEJZEkWwghhBBCCCGEqCSSZAshhBBCCCGEEJVE7skWQgghhBBCiCqm1+vR6XTVHUaNpdPpUKvV5OTkoNfrH/j5zczMUKlUldKXJNlCCCGEEEIIUUWMRiPx8fGkpKRUdyg1mtFoxM3NjStXrpRqcbGqYGdnh5ubW4XPL0m2EEIIIYQQQlSRwgTbxcUFS0vLaksgazqDwUBGRgbW1tYolQ/2rmaj0UhWVhaJiYkAuLu7V6g/SbKFEEIIIYQQogro9XpTgu3o6Fjd4dRoBoOBvLw8LCwsHniSDaDVagFITEzExcWlQlPHZeEzIYQQQgghhKgChfdgW1paVnMkojQKr1NF752XJFsIIYQQQgghqpBMEX84VNZ1kiRbCCGEEEKIKmYwGIkNTyErVk1seAoGg7G6QxJCVBG5J1sIIYQQQogqdPlYInt/DiczJRfQsuHEKazsNHQeWQ//Fi7VHZ4Q5RYSEsLLL78sK6ffQUayhRBCCCGEqCKXjyWyZenpWwn23zJTctmy9DSXjyVWU2TiYaI3GNl/+Qa/H7/G/ss30D+AmRBjx45lyJAhxbaHhYWhUChISUlh5MiRXLx40bRv1qxZBAYGlut8GRkZLFq0iC5duuDm5oanpyc9evRg6dKl5OfnF2v/5Zdf0q1bN2xsbEzx3OnmzZuMGjUKGxsb7OzseO6558jIyChXfGUhI9lCCCGEEEJUAYPByN6fw+/Z5o9fwvFt7oxSKffsipJtOR3H7PVniUvNMW1zt7Xg3YEB9GlSsVJTFaXVak2rclfEkSNHGDp0KJ6enkyYMIHGjRtjZmbGyZMnWbJkCUuWLGHr1q24uPw98yMrK4s+ffrQp08fZsyYUWK/o0aNIi4uju3bt6PT6Rg3bhwTJkzgxx9/rHDM9yIj2UIIIYQQQlSBuPCUYiPYd8pIziUuPOXBBCQeOltOxzFx+dEiCTZAfGoOE5cfZcvpuGqKrEBISAh2dnamf8+ePZsTJ06gUChQKBSEhIRgNBqZNWsWderUQaPR4OHhwZQpU0x9REdH069fP9566y02b95McHAwbdu2pUWLFgQHB7Nv3z4GDhxI3759i6z6/fLLLzN9+nQee+yxEmM7d+4cW7Zs4euvv6Zdu3Z06tSJ//3vf6xcuZLY2NgqfV1kJFsIIYQQQogqkJl27wS7rO3Ew89oNJKt05eqrd5g5N11ZyhpYrgRUACz1p2lY10nVKWYCaE1U1XpKucjR47k9OnTbNmyhdDQUABsbW1ZvXo1n3zyCStXrqRx48bEx8dz4sQJ03HTp09n3LhxjB8/nnPnzjFq1CgOHTpEixYt6NSpE9euXWPJkiWEhYWxfPlyxo0bV6p49u/fj52dHa1btzZtCwoKQqlUcuDAAYYOHVq5L8BtJMkWQgghhBCiCljZaCq1nXj4Zev0BMzcWil9GYH4tByaztpWqvZn5/TG0rxs6d+GDRuwtrYusk2vL/lLAq1Wi7W1NWq1Gjc3N9P2mJgY3NzcCAoKwszMjDp16tC2bVug4D7sjRs3EhkZCcCkSZOwtbVly5YtnDt3jn//+98MHz4cgODgYLZu3VrqJDs+Pr7I9HIAtVqNg4MD8fHxpXsBykmSbCGEEEIIIaqAez07rOw095wybm2vwb2e3YMLSogy6N69O1988UWRbQcOHGD06NGl7mPEiBF8+umn+Pn50adPH/r168fAgQNRq9VcvHgRHx8fHB0dSU9PZ8+ePVy5coXatWvTsmVLwsLCTFPE3d3dSU5OrtTnV1UkyRZCCCGEEKIKKJUKOo+sx5alp+/apt0gP1n07B9Ea6bi7JzepWp7MPImY787dN92IePa0NbXoVTnLisrKyvq1q1bZNvVq1fL1IeXlxcXLlwgNDSU7du3M2nSJBYuXMju3bvJz883LZxWmExbWVmZjrW2tjYl1kePHi0Wy724ubmRmFh09f78/Hxu3rxZZKS9KsjCZ0IIIYQQQlQR/xYu1G1dvBa24lZiHXUqCaOx6ssxiZpBoVBgaa4u1aNzPWfcbS2421cwCgpWGe9cz7lU/VXl/diFzM3NS5xOrtVqGThwIJ999hlhYWHs37+fU6dO4efnx8WLF9HpdNjZ2dGwYUPmzZuHTqfj/PnzrFy5EoPBwMaNG1m8eDGTJ08udSzt27cnJSWFI0eOmLbt3LkTg8FAu3btKuX53o0k2UIIIYQQQlSh5LgsAJoH1caheTYDpjRl+OstUSoVXD56nYsHE6o5QlETqZQK3h0YAFAs0S78+d2BAaVa9OxB8fHxITIykuPHj5OUlERubi4hISF88803nD59moiICJYvX45Wq8Xb2xsnJyeaNWvG8uXLAVi8eDErV65Eq9USFBTEoEGDWL58OTNnzuSXX36hUaNGpnPFx8dz/PhxLl26BMCpU6c4fvw4N2/eBKBRo0b06dOH8ePHc/DgQf78808mT57Mk08+iYeHR5W+DpJkCyGEEEIIUUVSErO4cS0DhVJB86DaWHrk41HPDldfW9oM8AFgz8qLpN/MuXdH4h+pTxN3vhjdEjdbiyLb3Wwt+GJ0y2qvk32n4cOH06dPH7p3746zszM//fQTdnZ2fPXVV3Ts2JFmzZoRGhrK+vXrcXR0BGD+/Pm89tprHD16lJYtWxIVFUVMTAxRUVF89NFH3Lx5kyNHjtC5c+ci51qyZAktWrRg/PjxAHTp0oUWLVqwbt06U5sVK1bQsGFDevbsSb9+/ejUqRNffvlllb8Ock+2EEIIIYQQVSTi+HUAPOvbYWFlVmRfy97eRJ26QUJkGjuWnWXwSy1M08iFKNSniTuPB7hxMPImiek5uNSyoK2vQ5WPYIeEhJS4vVu3bqZbHMaOHcvYsWNN+zQaDatWrSp2zJAhQ+56ng4dOvDxxx8TFBTEhAkTeP7556lXrx56vZ6TJ08yf/58evTowSuvvFLkuFmzZjFr1qx7PgcHBwd+/PHHe7apCjKSLYQQQgghRBWJOFaQZPu3cC62T6lSEjQuALW5kmsXUjix88qDDk88JFRKBe39HRkc6El7f8caNUW8MgQHBxMWFsaFCxdo0aIF5ubmaDQaRo8eTadOnXjxxRerO8QykZFsIYQQQgghqkBGcg4JkWmgAN/A4kk2gJ2LJZ1G1CNsxQX+WhuBV4ADjh7WJbYV4lHWrFkzli1bhqWlJdevX0ej0eDk5FTdYZWLjGQLIYQQQghRBSKOJwHg5muLla3mru0COnng3dQRfb6B0O/Oos83PKgQhahx1Go1np6eD22CDZJkCyGEEEIIUSUijhXU6PVvWfIodiGFQkH30Q2xsDIj6UoGBzdEPojwhBBVRJJsIYQQQgghKll2eh6x4SkA+N1lqvjtrGw1dBvdAIBjW6OJu5RShdEJIaqSJNlCCCGEEEJUssiTSRiN4ORljY2TtlTH+LdwoeFjbhiNEBpylryc/CqOUghRFSTJFkIIIYQQopL9vaq4S5mO6zSyPrUcLEhLyuHPX8OrIjQhRBWTJFsIIYQQQohKlJudz5VzNwHwK6F0171otGp6jm0ECjj7ZxyRJ65XRYhCiCokSbYQQgghhBCVKPpUEga9EXs3Sxzcrcp8vGd9e1oE1QFg1/LzZKXlVXaIQogqJEm2EEIIIYQQlahwqnhpFjy7m3aD/HD0tCI7XUfYivMYjcbKCk+IShMSEoKdnV11h1Hj/COS7MWLF+Pj44OFhQXt2rXj4MGDd23722+/0bp1a+zs7LCysiIwMJAffvihSBuj0cjMmTNxd3dHq9USFBREeLjcMyOEEEII8U+ny9MTfeYGAP4ty3Y/9u1UZkqCxjVGqVYQeSKJc/viKitE8TAy6CFyL5xaVfD/Bn2Vn3Ls2LEMGTKk2PawsDAUCgUpKSmMHDmSixcvmvbNmjWLwMDAcp0vIyODRYsW0aVLF9zc3PD09KRHjx4sXbqU/PyiiwDevHmT//znPzRo0ACtVkudOnWYMmUKqampRdrFxMTQv39/LC0tcXFx4fXXXy/WV1VQV/kZqtnPP//M1KlTWbJkCe3atePTTz+ld+/eXLhwAReX4v/hc3Bw4K233qJhw4aYm5uzYcMGxo0bh4uLC7179wZgwYIFfPbZZyxbtgxfX1/eeecdevfuzdmzZ7GwsHjQT1EIIYQQQtQQV87cJD/PQC0HC5y8rCvUl1Nta9oN8mP/b5f545dwPOvbY+tcupXKxSPk7DrYMg3SYv/eZuMBfT6AgEHVFxeg1WrRaiv+njxy5AhDhw7F09OTCRMm0LhxY8zMzDh58iRLlixhyZIlbN261ZS/xcbGEhsby4cffkhAQADR0dH8+9//JjY2llWrVgGg1+vp378/bm5u7Nu3j7i4OMaMGYOZmRnz5s2rcMz38siPZH/88ceMHz+ecePGERAQwJIlS7C0tOTbb78tsX23bt0YOnQojRo1wt/fn5deeolmzZrxxx9/AAWj2J9++ilvv/02gwcPplmzZnz//ffExsaydu3aB/jMhBBCCCFETXP5eCJQsOCZQqGocH+BQXVwr2uLLlfPjmVnMRhk2vg/ytl18MuYogk2QFpcwfaz66onrltuny4eEhLC7NmzOXHiBAqFAoVCQUhICEajkVmzZlGnTh00Gg0eHh5MmTLF1Ed0dDT9+vXjrbfeYvPmzQQHB9O2bVtatGhBcHAw+/btY+DAgfTt2xedTgdAkyZNWL16NQMHDsTf358ePXrw3//+l/Xr15tGqrdt28bZs2dZvnw5gYGB9O3bl7lz57J48WLy8qp2nYNHOsnOy8vjyJEjBAUFmbYplUqCgoLYv3//fY83Go3s2LGDCxcu0KVLFwAiIyOJj48v0qetrS3t2rUrVZ9CCCGEEOLRpM83EHXy1lTxMq4qfjdKpYKgsQGYWaiIu5TK8e0xldKvqCZGI+Rllu6Rkwab3wBK+mLl1rYt0wralaa/Kr6vf+TIkbz66qs0btyYuLg44uLiGDlyJKtXr+aTTz5h6dKlhIeHs3btWpo2bWo6bvr06YwbN47x48dz7do1Bg4caJpFPHfuXCZOnMicOXOwsrJi+fLldz1/amoqNjY2qNUFk7X3799P06ZNcXV1NbXp3bs3aWlpnDlzpupeCB7x6eJJSUno9foiLyyAq6sr58+fv+txqampeHp6kpubi0ql4vPPP+fxxx8HID4+3tTHnX0W7itJbm4uubm5pp/T0tIA0Ol0pm9kapLCmGpibP9Ecj1qFrkeNYtcj5pFrkfNItfjwbpy9iZ52flobcxw9LIs9rqX93pobdV0GO7P7hUXObAuAvf6NjjVrthUdPFgfj90Oh1GoxGDwYDBYIC8TJTv166k3o0FI9zve5WqtWH6VTAv/Wr3RqORDRs2YG1d9L2m1xfcD256Trf+rdFosLKyQq1WF7ktNzo6Gjc3N3r06IGZmRm1a9emdevWGAwGMjIy2LhxI5cvX8ZoNDJp0iRsbW3ZtGkT586dY9KkSQwbNgyDwcAzzzzDli1bCA4OLhZrUlISc+fOZfz48aaY4uLicHFxMf0M4Oxc8OVXbGwszZs3L/4aGQwYjUZ0Oh0qlarIvrK8Tx7pJLu8atWqxfHjx8nIyGDHjh1MnToVPz8/unXrVu4+58+fz+zZs4tt37ZtG5aWlhWItmpt3769ukMQt5HrUbPI9ahZ5HrULHI9aha5Hg9G8mkNYI7SNpPNWzbftV15rofRCBauFuQkmLFu8WFcO2ShUN3/OHF/Vfn7oVarcXNzIyMjo2CKsi4Luyo7272lpaeDWekXTNPpdHTu3JmPPvqoyPbDhw/zwgsvkJ6eTk5ODkaj0TSAmJubi16vN/0MBaPHn3zyCX5+fgQFBfH444/Tp08f1Go1J06cwMvLCzMzM+Lj49mzZw9nz57F3d2dunXrEhoaik6nIy0tDVtbW5KSkor0DQWDl8OGDaNevXq88sorRQYz74wlKyvL9P939gMFM6Gzs7PZs2dPsQXSCo8tjUc6yXZyckKlUpGQkFBke0JCAm5ubnc9TqlUUrduXQACAwM5d+4c8+fPp1u3bqbjEhIScHd3L9LnvVbSmzFjBlOnTjX9nJaWhpeXF7169cLGxqY8T69K6XQ6tm/fzuOPP46ZmVl1h/OPJ9ejZpHrUbPI9ahZ5HrULHI9HhyDwcjyvQcAHV0HtaR2Q/si+416PRkHD3J81y4Cu3fHum1bFKqyZcnZXfJYNf8o2engmN+Qxwb6VeIz+Od5EL8fOTk5XLlyBWtr64IFko21CkaUSyN6H8qfnrhvM8NTv4B3h/u2szGzhDKsE2BmZoaNjU2xHCclJQUoGJi0sLBAoVCY8hmNRoNKpSqS3wQEBHDhwgVCQ0MJDQ3l9ddf5/PPP2fXrl1oNBqsra2xsbExjZC7urqajre3tyc5ORkbGxsuXLhAgwYNivSdnp7Ok08+iZ2dHevWrSuyCLWXlxfHjx8v0v7GjYLbOfz8/ErMwXJyctBqtXTp0qXYgtYlJeV380gn2ebm5rRq1YodO3aYlp83GAzs2LGDyZMnl7ofg8Fgmurt6+uLm5sbO3bsML3h0tLSOHDgABMnTrxrHxqNBo1GU2y7mZlZjf6jV9Pj+6eR61GzyPWoWeR61CxyPWoWuR5V79rFZHIydGgs1dQJcEKl+nvpo7Rt20iYN5/8+HjcgYSfVnLDzQ3XN2dg06tXqc9h5mBGj2casfHzk5zcdQ3f5i7UbmB//wPFPVXl74der0ehUKBUKlEqb70nVLVKd3C9oIJVxNPiKPm+bAXYeKCsFwTKyp/WULh4mSnuWwp/vv05Ff6/RqNBr9cXO8bKyorBgwczePBgJk+eTMOGDTlz5gx169bl4sWL6PV67O3tadiwIfPnz2fevHlcvnyZn3/+mccff5zNmzfz+eefs3PnTlPfaWlp9OnTB41Gw7p164rNDu7QoQPz5s0jKSnJNH19x44d2NjY0KRJk2IxFj4PhUJR4nuiLO+RR3rhM4CpU6fy1VdfsWzZMs6dO8fEiRPJzMxk3LhxAIwZM4YZM2aY2s+fP5/t27cTERHBuXPn+Oijj/jhhx8YPXo0UPBme/nll3nvvfdYt24dp06dYsyYMXh4eJRYR04IIYQQQjz6Io5dB8C3WfEE+9pLL5N/x9o9+QkJXHvpZdK2bSvTeXyaORHQ2QOMsCPkLLnZVV/zV1QTpaqgTBcAd45A3/q5z/tVkmCXl4+PD5GRkRw/fpykpCRyc3MJCQnhm2++4fTp00RERLB8+XK0Wi3e3t44OTnRrFkz04JmixcvZuXKlWi1WoKCghg0aBDLly9n5syZ/PLLLzRq1AgoSLB79epFZmYm33zzDWlpacTHxxMfH28aEe/VqxcBAQE888wznDhxgq1bt/L222/z4osvljj4WZke6ZFsKFjl7vr168ycOZP4+HgCAwPZsmWLaeGymJiYIt9iZGZmMmnSJK5evYpWq6Vhw4YsX76ckSNHmtq88cYbZGZmMmHCBFJSUujUqRNbtmyRGtlCCCGEEP9ARqORiOMFSbbfbauKG/V6EubNL3lVZ6MRFAoS5s2nVs+eZZo63nF4Xa6eTybtejZ7f75I0NiACj8HUUMFDIInvr9Lnez3q71O9p2GDx/Ob7/9Rvfu3UlJSeG7777Dzs6O999/n6lTp6LX62natCnr16/H0dERKBjkHDhwIE2bNqVly5ZERUWRkJCAi4sLOTk5fPDBB6YyYYWOHj3KgQMHAEy3+RaKjIzEx8cHlUrFhg0bmDhxIu3bt8fKyorg4GDmzJlT5a+Dwmis4rXcRYkKb94vXGq+ptHpdGzatIl+/frJ9LIaQK5HzSLXo2aR61GzyPWoWeR6PBgJkWms+uAwao2K5z7shNqsIGHOPHCQmBJWQr5TnWXLsGrXtkznjLucypoPj2A0Qu/xTajbyuX+B4kiHsTvR05ODpGRkfj6+lZsQM6gh+h9kJEA1q4F92DXoBHsilq2bBkvvfQSEyZM4Pnnn6devXro9XoOHjzI/Pnz6dGjB6+88kqVx3Gv61WW/O2Rny4uhBBCCCFEVYo4ngiATxNHU4INkH/9eqmOL22727n729KyjzcAYT+eJzM19z5HiIeaUgW+naHpvwr+/xFKsAGCg4MJCwvjwoULtGjRAnNzczQaDaNHj6ZTp068+OKL1R1imTzy08WFEEIIIYSoKkajkctHi08VB1A7O5d0SDGlbXenNv19iTlzk+sx6ez8/jwDJjdDUYbVo4WoSZo1a8ayZcuwtLTk+vXraDQanJycqjuscpGRbCGEEEIIIcrpZmwmqdezUamVeDdxLLLPsnUr1G5u9yybpHZzw7J1q3KdW6VWEjQ2AJVaScyZG5zZG3v/g4So4dRqNZ6eng9tgg2SZAshhBBCCFFul2+tKu4V4IC5RdFJogqVCtc3Z5S88NktLq++WuZ62bdz8LCi/VB/AP5cFU5KQla5+xJCVA5JsoUQQgghhCinwtJdfoElT/m26dULy44di++4Vd0m68jhCsfQrHttaje0Jz/PwPbvzmLQGyrcpxCi/CTJFkIIIYQQohxSErO4cS0DhVKBb/OSp7Ya8/LIPX0aAMfXXiXuqSfx+PYbvL76sqCPlT+THhZWoTgUSgU9xjTCXKsmMSqNI1uiK9SfEKJiJMkWQgghhBCiHApHsT3r22FhVXIJqIw//0SfmorK2Qm70aNJDwzEsk0brDt2xCF4DABxb79D/s2bFYqlloMFXZ+qD8ChjVEkRKVVqD8hRPlJki2EEEIIIUQ5RBwvSLL9W9x9dfC0DRsBsOnbt9i9185Tp2Je1x99UhJx78zEeI97t0ujXhtX6rZ2wWgwEvrdWXR5+gr1J4QoH0myhRBCCCGEKKOM5BwSItNAAb53uR/bkJlJ+s6dANgOGFBsv1KjwXPhQjAzI2PHDlJ/+61CMSkUCro+1QArW3NSErLY/9vlCvUnhCgfSbKFEEIIIYQoo8JRbHc/W6xsNSW2Sd+5C2N2NmZ16mDRtGmJbSwaNcJ5yn8ASPjvPPKuXKlQXBZWZvQIbgTAqbCrxJy9UaH+hLiXkJAQ7OzsqjuMGkeSbCGEEEIIIcrItKr4PaeKbwDAdkB/FPeole347LNoW7fCkJVF7BvTMObnVyi2OgGONO1WG4Cdy86Rk6mrUH+i+ukNeg7FH2JTxCYOxR9Cb6j6WwHGjh3LkCFDim0PCwtDoVCQkpLCyJEjuXjxomnfrFmzCAwMLNf5MjIyWLRoEV26dMHNzQ1PT0969OjB0qVLyS/hd+KFF17A398frVaLs7MzgwcP5vz580XaxMTE0L9/fywtLXFxceH1118vsa/KJkm2EEIIIYQQZZCdnkdseApw99Jd+cnJZPz5JwA2/fvfsz+FSoXH+x+gtLIi+9gxbnz9dYVjbD/MHztXSzJT89j944UK3+8tqk9odCi9V/fm2a3PMm3vNJ7d+iy9V/cmNDq0ukNDq9Xi4uJS4X6OHDlCkyZN2LhxI8899xzr1q1jw4YNBAcHExISQps2bUhMTCxyTKtWrfjuu+84d+4cW7duxWg00qtXL/T6gi8g9Ho9/fv3Jy8vj3379rFs2TJCQkKYOXNmheO9H0myhRBCCCGEKIPIE0kYjeBcpxY2TtoS26Rv3Qr5+WgCGqHx979vn+a1PXF9520Ari9aTPap0xWK0cxcxePPBqBUKrh0JJHwQwkV6k9Uj9DoUKaGTSUhq+j1S8xKZGrY1GpPtG+fLh4SEsLs2bM5ceIECoUChUJBSEgIRqORWbNmUadOHTQaDR4eHkyZMsXUR3R0NP369eOtt95i8+bNBAcH07ZtW1q0aEFwcDD79u1j4MCB9O3bF53u71kZEyZMoEuXLvj4+NCyZUvee+89rly5QlRUFADbtm3j7NmzLF++nMDAQPr27cvcuXNZvHgxeXl5Vfq6SJIthBBCCCFEGVwunCp+l1FsgNTCqeL9iy94dje2gwdTq1cvyM8ndto0DNnZFYrTxduG1v19ANiz8iLpN3Mq1J+oOKPRSJYuq1SP9Nx05h+cj5HisxCMt/73/sH3Sc9NL1V/VT2bYeTIkbz66qs0btyYuLg44uLiGDlyJKtXr+aTTz5h6dKlhIeHs3btWpretkbB9OnTGTduHOPHj+fatWsMHDgQFxcXevfuzdy5c5k4cSJz5szBysqK5cuXl3juzMxMvvvuO3x9ffHy8gJg//79NG3aFFdXV1O73r17k5aWxpkzZ6r0tVBXae9CCCGEEEI8QnKz87l6vqCm9d3ux9bFxpJ9+AgoFNj071fqvhUKBW6zZ5F97Bh5EREkfvgRbrdGt8urVR9vok7dIDEqjZ3fn2PQlEAUyrvfHy6qVnZ+Nu1+bFdp/SVkJdBhZYdStT3w9AEszSzL1P+GDRuwtrYusq1wOvadtFot1tbWqNVq3NzcTNtjYmJwc3MjKCgIMzMz6tSpQ9u2bYGC+7A3btxIZGQkAJMmTcLW1pYtW7Zw7tw5/v3vfzN8+HAAgoOD2bp1K+PGjTP1/fnnn/PGG2+QmZlJgwYN2L59O+bm5gDEx8cXSbAB08/x8fFleh3KSkayhRBCCCGEKKWok0kY9Ebs3SxxcLcqsU3apk0AWLZujdltyUZpqO3tcZ83D4DkFSvI2PtHheJVqpQ8Pi4AtbmSq+eTObnraoX6E/8s3bt35/jx40UeX5dxzYARI0aQnZ2Nn58f48ePZ82aNabFxy5evIiPjw+Ojo5kZmayZ88ePv/8c1q2bMmoUaN48sknTf24u7uTnJxcpO9Ro0Zx7Ngxdu/eTf369XniiSfIyan+GRsyki2KMRiMxIankBWrJjY8Ba+GTijlG08hhBBCCFPprnutKp66sSDJtimhNnZpWHfuhP2oUSSvWEHcm2/iu+531Pb25eoLwM7Vko7D67L7p4vsX3sZr0YOOHiU/AWBqFpatZYDTx8oVdsjCUeYtGPSfdt93vNzWrm2KtW5y8rKyoq6desW2Xb1atm+qPHy8uLChQuEhoayfft2Jk2axMKFC9m9ezf5+flotQVxFd5vbWX193vT2tralFgfPXq0WCy2trbY2tpSr149HnvsMezt7VmzZg1PPfUUbm5uHDx4sEj7hISCe9vdyvjlV1nJSLYo4vKxRL5/cx8bPjvFzRNaNnx2iu/f3MflY4n3P1gIIYQQ4hGmy9MTc7qg7rR/i5JXVM69dIncc+fAzIxavR4v97lcXnsVcz8/8q9fJ/7dWRW+n7ZxF0/qNHZArzMQGnIWfb6hQv2J8lEoFFiaWZbq0cGjA66WrigoebBLgQI3Szc6eHQoVX/3KiNXWczNzUucTq7Vahk4cCCfffYZYWFh7N+/n1OnTuHn58fFixfR6XTY2dnRsGFD5s2bh06n4/z586xcuRKDwcDGjRtZvHgxkydPvuu5jUYjRqOR3NxcANq3b8+pU6eKrEq+fft2bGxsCAgIqPwnfxtJsoXJ5WOJbFl6msyU3CLbM1Ny2bL0tCTaQgghhPhHu3LmJvk6A7UcLXDysi6xTerGjQBYd+pUodFnpVaLx4IFoFaTvm0bqb//Xu6+oCC56zGmERorNddj0jm0MbJC/Ymqp1KqmN52OkCxRLvw52ltp6FSqh54bHfj4+NDZGQkx48fJykpidzcXEJCQvjmm284ffo0ERERLF++HK1Wi7e3N05OTjRr1sy0oNnixYtZuXIlWq2WoKAgBg0axPLly5k5cya//PILjRo1AiAiIoL58+dz5MgRYmJi2LdvHyNGjECr1dKvX8E6CL169SIgIIBnnnmGEydOsHXrVt5++21efPFFNBpNlb4OkmQLoGCK+N6fw+/Z5o9fwjEYpMaiEEIIIf6ZCgcc/Fo4lzgqaDQaSdtQkGTfrzZ2aWibNMZ58osAJMx9j7yr1yrUn5Wthu6jGgJwdEs0cZdTKxyjqFpB3kF83O1jXCyLzpxwtXTl424fE+QdVE2RlWz48OH06dOH7t274+zszE8//YSdnR1fffUVHTt2pFmzZoSGhrJ+/XocHR0BmD9/Pq+99hpHjx6lZcuWREVFERMTQ1RUFB999BE3b97kyJEjdO7c2XQeCwsL9u7dS79+/ahbty4jR46kVq1a7Nu3z1S3W6VSsWHDBlQqFe3bt2f06NGMGTOGOXPmVPnrIPdkCwDiwlOKjWDfKSM5l7jwFDwblP9bWSGEEEKIh5E+30DUySQA/O9Suivn5El0V66g0Gqp1aN7pZzX8fnnydi9h+xjx4idPg3vZctQqMo/cunf0oUG7dy4cCCe0JCzjHyrDeYWkhLUZEHeQXT36s7RxKNcz7qOs6UzLV1aVvkIdkhISInbu3XrZrp9YezYsYwdO9a0T6PRsGrVqmLHDBky5K7n6dChAx9//DFBQUFMmDCB559/nnr16qHX6zl58iTz58+nR48evPLKK6ZjPDw82HRrgcF78fb2LlW7yiYj2QKAzLR7J9hlbSeEEEII8Si5eiGZvBw9ljbmuPnZltgm9dYodq2ePVFalq1U0t0o1Go8FnyA0tKS7MNHuPHttxXus/OT9bF20JB2PZs/V1+qhChFVVMpVbRxa0M/v360cWtTo6aIV4bg4GDCwsK4cOECLVq0wNzcHI1Gw+jRo+nUqRMvvvhidYdYJpJkCwCsbEp3X4KsMi6EEEKIf6KIo7emigc6l1hn2pifT9rmzQDYDKj4VPHbmXt54frWmwBc/+x/5Jw9W6H+NFo1QcEBoICze2NNI/RCVKdmzZqxbNkykpOTiYqKIiEhgYiICKZNm2aqff2wkCRbAOBezw4ru/sn2qHfneWv3y+Tl5P/AKISQgghhKh+BoORiBMFiejdSndlHjiAPikJlZ0d1h07VnoMtsOGYR3UE3Q6rr3xBobcis0u9GxgT/OeXgDs/OEc2el5lRGmEBWmVqvx9PTEycmpukMpN0myBVAwQt15ZL17trFzs0Sfb+TI5miWv7Of07uvotdL+QchhBBCPNriLqWQk6FDY6nGo75diW0KFzyr1ac3CjOzYvv1Bj2HEw5zIu8EhxMOozcUL3N0LwqFAvc5c1A5OZF36TLXP/64zM/jTo8N9sPBw4rsdB27lp+vcJkwIUQBSbKFiX8LF/q80KTYiLa1vYY+LzTh6Xfb0feFpti6aMlO17H7p4usnHOQyBPX5T/KQgghhHhkXT52HQDf5k6oVMU/Phtyckjftg0A2wEDiu0PjQ6l9+reTNgxgV+zfmXCjgn0Xt2b0OjQMsWhdnDA/b25ANxc9j2Z+/aV9akU7c9MRdC4AJQqBZEnkji/P75C/QkhCkiSLYrwb+HCmHkdGDClKQ7NsxkwpSnP/LcD/i1cUCgU+LVw5ql329F5ZH0srM1ISchi0xenWPvxMRKi0qo7fCGEEEKISmU0GIm4lWT7tXApsU3G7j0YMjNRu7ujbdmyyL7Q6FCmhk0lISuhyPbErESmhk0tc6Jdq1s37J4cCUDsjDfRp6SU6fg7OXvVot0gPwD2/nKRtKTsCvUnhJAkW5RAqVTgUc8OS498POrZFVvsTKVS0qx7bUbPbU/L3t6ozJTEhqew6v3DbPvmjPzHWQghhBCPjMTodDJTcjHTqPBqVHIZ07QNGwCw7d8PhfLvj9d6g573D76PkeIz/gq3fXDwgzJPHXd94w3Mvb3JT0ggfs6cCs8oDHy8Du51bdHl6AkNOYvBIDMUhagISbJFuWm0atoP9WfU7Mdo8JgbKCD8UAIrZv3Fn6vCycnUVXeIQgghhBAVcvlYwari3k0dUZsVL5ukT08nY/duAGzumCp+NPFosRHs2xkxEp8Vz9HEo2WKSWlpicfCBaBSkbZps+l+8PJSKhX0DA7ATKMi7lIqx0NjKtSfEP90kmSLCqvlYEHQ2ACemNGG2g3tMeQbOR56heXv7Od4aAx6nSyOJoQQQoiHj9F421TxwJJXFU/fHooxLw/zuv5oGjQosu961vVSnae07W6nbdYMp4kTAYifMwddbGyZ+7idrbOWTk8ULIJ74PcIkq6mV6g/If7JJMkWlca5Ti0GvRTIgMnNcfCwIjcrnz9XXeLH2X8RfjhBFkcTQgghxEPlZmwmqdezUamVeDdxLLGNaar4gAEoFEVvsXO2LDkxv1Np293J6d8vYNG8GYb0dGJnvInRULGBjUYd3PFp5oRBbyT0u7MyUCLuKyQkBDs7u+oOo8aRJFtUKoVCgXcTR0a+1YbuoxtiaWNOWlIO274+w+oFR4i9lFLdIQohhBBClErhquJeAQ6YW6iL7c+/fp3Mv/4CwKZfv2L7W7q0xNXS9Z7ncLN0o6VLy3u2uRuFWo3nBx+g0GrJOnCAmyHLytWPqT+Fgu6jG6KtZcaNa5kcWBdRof5E5THq9WQeOEjqho1kHjiIUV+2+/jLY+zYsQwZMqTY9rCwMBQKBSkpKYwcOZKLFy+a9s2aNYvAwMBynS8jI4NFixbRpUsX3Nzc8PT0pEePHixdupT8/Py7Hmc0Gunbty8KhYK1a9cW2RcTE0P//v2xtLTExcWF119//Z59VRZJskWVUKqUBHTyYNScx2gzwBe1RkVCZBprPjzK5iWnSEnIqu4QhRBCCCHuKeLW/dj+LUoeaU7bvAUMBiyaN8O8Tp1i+1VKFdPbTr/nOUY0GIFKWfxe79Iy9/HBdXrBOa5/8gk5Fy6Uuy8ASxtzuo9uCMCx0BiuXUyuUH+i4tK2beNSzyBigoOJfe01YoKDudQziLRbZeOqk1arxcWl5FX3y+LIkSM0adKEjRs38txzz7Fu3To2bNhAcHAwISEhtGnThsTExBKP/fTTT4vNIgHQ6/X079+fvLw89u3bx7JlywgJCWHmzJkVjvd+JMkWVcrcQk3bAb6MnvMYAZ09UCgg4vh1fpp9gD0rL5KdnlfdIQohhBBCFJOSkMWNa5kolQp8mjmV2CZ1Y+Gq4sVrYxfqWacnztriSbqFygKAXy/+SmpuaoVitXtiBNbdumHU6Yh9/Q0MeRX7fOXb3JmAju5ghNCQs+RmV/3InyhZ2rZtXHvpZfLji9Ywz09I4NpLL1d7on37dPGQkBBmz57NiRMnUCgUKBQKQkJCMBqNzJo1izp16qDRaPDw8GDKlCmmPqKjo+nXrx9vvfUWmzdvJjg4mLZt29KiRQuCg4PZt28fAwcOpG/fvuh0RRdWPn78OB999BHffvttsdi2bdvG2bNnWb58OYGBgfTt25e5c+eyePFi8ir4O3I/kmSLB8LKVkP3UQ0Z+U5bvJs6YjAYORV2leXv7OfIlijy86p+yosQQgghRGlFHC+YKu7ZwA4LK7Ni+/NiYsg5cRKUSmz69rlrP0cTj3I9+zoWKgs+6/oZIyxH8GXPL9kxYgfeNt7EZ8bz9p9vV2jtGoVCgft7c1E5OJB78SLXP/2/cvdVqOOIetg4WZBxM5c/fr54/wNEqRiNRgxZWaV66NPTSXjvv1DSe8NoBIwk/Hce+vT0UvVX1esjjRw5kldffZXGjRsTFxdHXFwcI0eOZPXq1XzyyScsXbqU8PBw1q5dS9OmTU3HTZ8+nXHjxjF+/HiuXbvGwIEDcXFxoXfv3sydO5eJEycyZ84crKysWL58uem4rKwsnn76aRYvXoybm1uxePbv30/Tpk1xdf37lo3evXuTlpbGmTNnqvS1KH5ziRBVyNHDmgEvNufq+Zv8ufoSSVcy+GttBKd3X6PdYD8atHVDoSw+3UMIIYQQ4kEqvB/br0XJU2HTNhaUzbJ67DHUzndfuOy38N8A6Ovbl06enUg7kUZr19aYmZmxsMtCRm0aRdiVMH48/yOjGo0qd7xqJyfc35vL1UkvcvO777Du2hWrdm3L3Z+5hZqgsQGs+ego5/+Kx6e5E/53eS1E6Rmzs7nQslUldVYwon2xTemuc4OjR1BYWpbpFBs2bMDa2rrINv1d7gfXarVYW1ujVquLJL0xMTG4ubkRFBSEmZkZderUoW3bgpgzMjLYuHEjkZGRAEyaNAlbW1u2bNnCuXPn+Pe//83w4cMBCA4OZuvWrYwbNw6AV155hQ4dOjB48OAS44mPjy+SYAOmn+PvmBlQ2WQkW1SL2g0deGJGG4LGBWDtoCEjOZcdIef4Zf4hrpy/Wd3hCSGEEOIfLP1mDolRaaAA3+bFp4objUZS1xdMFb+zNvbtMvIy2B69HYBh9YYV29/IsRGvtn4VgI8Of8TZG2crFHetHj2wG/EvMBqJnT4dfVpahfpzr2tHi97eAIQtv0Bmam6F+hMPn+7du3P8+PEij6+//rpMfYwYMYLs7Gz8/PwYP348a9asMS0+dvHiRXx8fHB0dCQzM5M9e/bw+eef07JlS0aNGsWTTz5p6sfd3Z3k5II1AtatW8fOnTv59NNPK+25ViYZyRbVRqFU0KCdG/4tnDm56ypHNkeRdCWDdZ8ep05jRzoM88fR0/r+HQkhhBBCVKLIEwWj2O5+tljZaortzz1/nryICBTm5tR6POiu/WyO2kx2fja+tr40d25e4qrGTzd8moNxB9l5ZSev736dXwb+gpWZVbljd5k2ncy/DqC7coX4997Dc8GCcvcF0HaALzFnbpB0JYNdP5yn/4vNSlxkSpSOQqulwdEjpWqbdfgwVya8cN92Xl8uxbJ161Kdu6ysrKyoW7dukW1Xr14tUx9eXl5cuHCB0NBQtm/fzqRJk1i4cCG7d+8mPz8f7a24Cu+3trL6+/1vbW1tSqyPHj1qimXnzp1cvny5WPmw4cOH07lzZ8LCwnBzc+PgwYNF9ickJACUOL28MslItqh2anMVLXt7M/q99jTtXhulUkHMmRv8/N5Bdv1wTr41FUIIIcQDdflo4VTxkqeBp96qjW3drRuqWrXu2s+a8DUADKs77K6JqUKhYE7HObhbuROTHsPcv+ZW6N5ZlbUVHgs+AKWStHXrSdu8udx9AajUSoLGBqBSK4k+fYOzf8RWqL9/OoVCgdLSslQPq44dUbu5wd2+1FAoULu5YdWxY6n6exBfjpibm5c4nVyr1TJw4EA+++wzwsLC2L9/P6dOncLPz4+LFy+i0+mws7OjYcOGzJs3D51Ox/nz51m5ciUGg4GNGzeyePFiJk+eDBTcx33y5MkiI+wAn3zyCd999x0A7du359SpU0VWJd++fTs2NjYEBARU6esgSbaoMbTW5nQZWZ+n3m2HfwtnjEY4+2ccy9/Zz8H1EeTlyMqWQgghhKhaWWl5xF1KAcAvsHiSbTQYSNtUkLjaDOh/137Ck8M5lXQKtULNQP+B9zynrcaWBV0WoFKo2BixkbWX1pY7fgDLFi1wfGECAHGzZqO7NXpXXo6e1jw2xA+AP34NJyVRSrE+CAqVCtc3Z9z64Y4E+dbPrm/OQKEqfwm4yubj40NkZCTHjx8nKSmJ3NxcQkJC+Oabbzh9+jQREREsX74crVaLt7c3Tk5ONGvWzLSg2eLFi1m5ciVarZagoCAGDRrE8uXLmTlzJr/88guNGjUCCkaimzRpUuQBUKdOHXx9fQHo1asXAQEBPPPMM5w4cYKtW7fy9ttv8+KLL6LRFJ+hUpn+EUn24sWL8fHxwcLCgnbt2hWbNnC7r776is6dO2Nvb4+9vT1BQUHF2o8dO9a0LH3ho0+fu68q+bAx6vVkHTpErePHyTp06IEUu7+dnaslfV5oyrDXWuLqa0N+noFDG6NYMfMvzuy9hkFveKDxCCGEEOKfI+pkEkYjONephY1T8em12UePkh8Xh9LaGuuuXe/aT+GCZ129uuKodbzveQNdApncomCUbv7B+USkRJTzGRRwnjQJiyZNMKSmEjdjBkZDxT4/Ne/hhWcDO/LzDIR+d1Y+jz0gNr164fl/n6K+YwEvtasrnv/3KTa9elVTZCUbPnw4ffr0oXv37jg7O/PTTz9hZ2fHV199RceOHWnWrBmhoaGsX78eR8eC34v58+fz2muvcfToUVq2bElUVBQxMTFERUXx0UcfcfPmTY4cOULnzp3LFItKpWLDhg2oVCrat2/P6NGjGTNmDHPmzKmKp17EI39P9s8//8zUqVNZsmQJ7dq149NPP6V3795cuHChxMLpYWFhPPXUU3To0AELCws++OADevXqxZkzZ/D09DS169Onj2kqAlDl34Y8KGnbtpEwbz758fG4A7E/rSTRzQ3XN2c88F9i97p2DH+jFZePXmf/2sukXc8mbMUFTuy8Sodh/ng3cZR7goQQQghRqf5eVfzeU8Vr9eqF8i6f//L0eWyIKGhX0oJnd/Nsk2c5GHeQ/XH7eXX3q/zU/ycs1BZlCd9EYWaGx4IFRA4bRua+/SQvX47DmDHl6gsK1tLpGRzAyjkHSIhM4+jWaFr38y13f6L0bHr1olbPnmQdPkL+9euonZ2xbN2qykewQ0JCStzerVs30y0NY8eOZezYsaZ9Go2GVatWFTtmyJAhdz1Phw4d+PjjjwkKCmLChAk8//zz1KtXD71ez8mTJ5k/fz49evTglVdeuWe8Jd1m4e3tzaZNm+55XFV45EeyP/74Y8aPH8+4ceMICAhgyZIlWFpalliwHGDFihVMmjSJwMBAGjZsyNdff43BYGDHjh1F2mk0Gtzc3EwPe3v7B/F0qlRNLHavUCio28qFp99tR6cR9dBYqUmOy2Tj4pP8/ukxrsekP/CYhBBCCPFoys3ScfVWlRP/EpJsY14e6Zu3AGB7j6niu67sIiU3BRetCx08OpT6/EqFknmd5+Fo4cillEssOFSxRcs0fr64vP4aAIkffkTupUsV6q+WgwVdnmoAwKENUSRGV2z1clF6CpUKq3ZtsR3QH6t2bWvUFPHKEBwcTFhYGBcuXKBFixaYm5uj0WgYPXo0nTp14sUXX6zuEMvkkU6y8/LyOHLkCEFBf6/6qFQqCQoKYv/+/aXqIysrC51Oh4ODQ5HtYWFhuLi40KBBAyZOnMiNGzcqNfYHzajXkzBv/j2K3UPCvPkPfOp4IZVaSfOeXjwztz0tHq+DSq3k2oUUfpl3iO3fnSH9Zk61xCWEEEKIR0fUqRsY9Ebs3Syxdyu+wnfGvn3oU1NROTlh2a7dXfspXPBscN3BqJVlmzjqpHVifuf5KFDw68Vf2Rq1tWxP4g72Tz+NVefOGPPyuPb6Gxjz8irUX/22rvi3dMFgMBL63Vny86rns6F49DRr1oxly5aRnJxMVFQUCQkJREREMG3aNMzNzas7vDJ5pKeLJyUlodfrSyxCfv78+VL1MW3aNDw8PIok6n369GHYsGH4+vpy+fJl3nzzTfr27cv+/ftR3eVbpdzcXHJz/14lO+1W3UKdTmdarr46ZR06VGwEuwijkfz4eNIOHMCyTZsHF9gdlGbQZpA3DTu5cmh9FJcOX+figQQuH0mkSTdPWvTywlz76L2tC98jNeG9IuR61DRyPWoWuR41i1yPsrl0tGCBMJ/mjiW+Zinr1gNg3bsX+QYDlHCfc1xmHPti9wEwwGdAkX5Kez1aO7dmbMBYvjv7HbP2zaK+bX1qW9cu35MCnGfPInvYcHLPnSP+/z7D6eWXyt0XQMcn/Ii7lEJyfBZ/rg6nw7/8K9RfdXkQvx86nQ6j0YjBYMBQwfviH3WF071VKhXu7u4AD/w1MxgMGI1GdDpdsbyuLO8ThbEiNQJquNjYWDw9Pdm3bx/t27c3bX/jjTfYvXs3Bw4cuOfx77//PgsWLCAsLIxmzZrdtV1ERAT+/v6EhobSs2fPEtvMmjWL2bNnF9v+448/YmlpWcpnVHVqHT+O+08r79succAAUjp3egARlU5eqpLU8xpybxYk1kozA7Xq5mFdR4fikZ6nIYQQQojKZMiHuB3WGA0KXDpkYm5b9MO9Ii8P/zlzUep0xLw4iZw6dUrsZ2fOTnbm7MRX7ctz1s+VOx69Uc83Gd8Qo4/BU+XJeOvxqBXlH0iwPn0ajx+WY1QouPrCBLJ9K3Y/dc51FUmHCz7DOrXJwsJJRrRLolarcXNzw8vL66Ebjf0nysvL48qVK8THxxera5+VlcXTTz9NamoqNjY29+zn0Rvyu42TkxMqlcpUdLxQQkLCfQuQf/jhh7z//vuEhobeM8EG8PPzw8nJiUuXLt01yZ4xYwZTp041/ZyWloaXlxe9evW670V6ELKcnYktRZLtsmEDPmlp2D45EsuOHVEoqz+TNRqNxJy+yYHfI0lJyCb1nAXGJDvaDvLFt/mjsTiaTqdj+/btPP7445iZmVV3OP94cj1qFrkeNYtcj5pFrkfpRR5PItZwjloOGgY/2anY54f0TZtI0OlQ165N9xdeKPHzhcFo4PN1nwMwrs04+vn2K7K/rNejdWZrntr8FNfyrnHZ/TKvtLz3wk/31K8fCWnppP/+O77r1lNn9SqU1tbl7w/4Q3uJs3vjyA63o/+/WqKxfLjeYw/i9yMnJ4crV65gbW2NhUX5FrH7pzAajaSnp1OrVq1q+/yek5ODVqulS5cuxa5X4Uzk0nikk2xzc3NatWrFjh07TCvaFS5iVljIvCQLFizgv//9L1u3bqV169b3Pc/Vq1e5ceOGaVpDSTQaTYkrkJuZmdWIP3o27dqR6OZGfkJCyfdlAwqNBmNuLll79pC1Zw9m3nWwf+op7IYORWVr+4AjLqpuSzf8mrtw9s84Dq6PIO16DqHfnMPNz5aO/6qLm1/1xldZasr7RRSQ61GzyPWoWeR61CxyPe4v+tStBc9aupQ44phZuODZwAF3HZHcH7uf2MxYrM2s6ePXBzN1ya95aa9HHbs6zOk4h5d3vcwP53/gMc/H6FK7S2mfUjHu77xNzuHD6K5d48aChXjMn1fuvgA6/as+sRdTSUnIYt+qSHo917hC/VWXqvz90Ov1KBQKlEolyhowOFWTFU4NL3y9qoNSqUShUJT4nijLe+SRv9JTp07lq6++YtmyZZw7d46JEyeSmZnJuHHjABgzZgwzZswwtf/ggw945513+Pbbb/Hx8SE+Pp74+HgyMjIAyMjI4PXXX+evv/4iKiqKHTt2MHjwYOrWrUvv3r2r5TlWhvsWu1co8Fi4AP8tm3EIDkZZqxa66BgS3/+A8G7diXtnJjmlvM+9qihVSpp08WT03Pa07ueD2kxJfEQqqxccYcuXp0hJzKrW+IQQQghRM+l1BqJOJgHg16J4idf85GQy/vgDANsBA+7aT+GCZ/18+6FVF6+xXR496/Tk6YZPA/DWH2+RkJlwnyPuTmVtjccH74NCQeqaNRWuHGOmURE0NgCFUkH4oQTCD5U/NiEeJY98kj1y5Eg+/PBDZs6cSWBgIMePH2fLli2mxdBiYmKIi4sztf/iiy/Iy8vjX//6F+7u7qbHhx9+CBTciH/y5EkGDRpE/fr1ee6552jVqhV79+596Gtll6bYvbmPD64zplNvdxhus2ejqV8fY3Y2Kb/+SuSQoUSNHk3a5s0Yq3GBFXMLNe0G+TFqTnsadXAHBVw+ep2fZh9g7y8XycmQxV+EEEII8berF5LJy9FjaWOOm2/x2/jSt26D/Hw0jRqh8S95ka/U3FR2xBSUfC1LbezSeLX1qzRyaERKbgrT905Hbyj//c+WrVvj+PzzAMTPfBddYmKFYnP1taF1X28Adv90gYzk3PscIcSj75GeLl5o8uTJd50eHhYWVuTnqKioe/al1WrZurVipRRqssJi92kHDnBk+3ZaPf44Nu3aFavFp7S0xH7kE9g9MYLsI0e4uWIF6dtDyT58hGuHj6B2dsZu5EjsnhiBmUvxb4QfBGt7DT3GNKJ5Ty/2/XaJmDM3ObnzKuf3x9OqrzfNutdGbfZo1RgUQgghRNlFHCtINP0CnVEoi98LmrZhA3Dv2tgbIjaQZ8ijvn19AhwDKjU+c5U5C7su5In1T3A44TBfnvySiYETy92f838mk/HnH+SePUfcW2/j9eXSCt0D26qfD9Gnb5AYnc7O788y8D+BJb6OQvxTPPIj2aLsFCoVlm3akB4YiGWbNvcsdq9QKLBs3Zran3xC3R07cHrxRVTOTuRfv07SokVc6tGTa1NfJevIEaprIXtHT2sG/ieQQVMCcaxtTV52Pvt/u8yP7x7gwoF4jIZHdoF9IYQQQtyHQW8g4sStqeItnYvt18XFkXXkCAA2/foV219o7aW1QMEodlUs2uRt48077d8BYMnJJRyKP1TuvhTm5nguWIDC3JzMvXtJ/vHHCsWmUikJGheA2kzJlXPJnNp9tUL9iYdHSEgIdnZ21R1GjSNJtqg0Zq4uOP9nMvV27MDjow/RtmwJ+fmkbdpE9KjRRA4dRvKvv2LIzq6W+LwCHHjizTb0DG6ElZ2G9Js5hH53ll/fP8y1C8nVEpMQQgghqlfcpVRyMnRorNR41LMrtj9t02YwGrFs3Rqzuyxye/bGWc7fPI+Z0oz+vncf7a6oAX4DGFJ3CAajgel7pnMz52a5+9LUrYvLa68CkLjwQ3IjIioUm72bFR2G1wVg32+XuRmXWaH+RFEGg5FrF5K5eCieaxeSMTyAQaKxY8eaFo++XVhYGAqFgpSUFEaOHMnFixdN+2bNmkVgYGC5zpeRkcGiRYvo0qULbm5ueHp60qNHD5YuXVqsnBZAt27dUCgURR7//ve/i7SJiYmhf//+WFpa4uLiwuuvv15iX5XtHzFdXDxYCnNzbPv3x7Z/f3LOnSP5xx9JXb+B3PPniX9nJokLP8Ru2DDsn34K87vUmKwqSqWChu3d8W/lwokdVzi6NZrrMems/eQYPk0daT+sLg7uVg80JiGEEEJUn8vHrwPg28wJlar4+FPqxoKp4jb3WPDst/DfgIJFyuws7Co/yNvMaDuDk9dPEpEawdt/vM2inotQKso3bmY/ejQZYWFk7ttP7BvT8PnpRxQVWGW7SVdPok4mEXP2JqHfnWX4tFYlvqaibC4fS2Tvz+Fkpvx9v7uVnYbOI+vhX8JCfQ+SVqtFq634In9Hjhxh6NCheHp6MmHCBBo3boyZmRknT55kyZIlLFmyhK1bt+Jyx22o48ePZ86cOaafLS0tTf/W6/X0798fNzc39u3bR1xcHGPGjMHMzIx58yq2sv79yLteVCmLRo1wnzuXemG7cHnjDcy8vDCkpXEzJITLvfsQ88ILZOzZg/HWkv0Pipm5itZ9fRg9pz1NunqiUCqIOnWDlXMPErbiPFlpeQ80HiGEEEI8eEaDkYhjBUl2SclK7uXL5J49B2o1tXr3KrGPnPwcNkVsAmBovaFVF+wtlmaWLOy6EI1Kw95re/nh7A/l7kuhVOI+fz5KW1tyTp/m+uefVyg2hUJBjzGN0FiquR6TzuGNURXqTxQk2FuWni6SYANkpuSyZelpLh+r2MJ1FXX7dPGQkBBmz57NiRMnTCPLISEhGI1GZs2aRZ06ddBoNHh4eDBlyhRTH9HR0fTr14+33nqLzZs3ExwcTNu2bWnRogXBwcHs27ePgQMH0rdvX3R3LK5saWmJm5ub6WFj8/fChdu2bePs2bMsX76cwMBA+vbty9y5c1m8eDF5eVX7WV+SbPFAqOzscHx2HP5bt+C1dAlWXTqD0Ujm7j1cmfACl/v05cZ3IehTUx9oXJY25nR9qgFPzWyLb3MnjAYjZ/bGsvyd/RzaGIkut/yrdwohhBCiZkuITiMzJRczjYrajeyL7U/buBEA606dUNsX3w8QGhNKui4dDysPHnN/rErjLVTfvj5vtHkDgE+PfMqp66fK3ZeZqyvus94F4MbSL8k6eqxCsVnZaej6dAMAjmyOIj7iwX62q+mMRiO6XH2pHrnZ+ez9+eI9+9v7czi52fml6q+q10caOXIkr776Ko0bNyYuLo64uDhGjhzJ6tWr+eSTT1i6dCnh4eGsXbuWpk2bmo6bPn0648aNY/z48Vy7do2BAwfi4uJC7969mTt3LhMnTmTOnDlYWVmxfPnyIudcsWIFTk5ONGnShBkzZpCV9XfJ3v3799O0aVNTVSmA3r17k5aWxpkzZ6r0tZDp4uKBUiiVWHftinXXruRFR5P840+krFmDLiaGxA8+4Pr//R+2AwdiP+ppLBo2fGBx2btZ0W9iM2LDk/lz1SUSo9M5uD6SM3uu0XaQHw3bu6OUVTKFEEKIR0rhKLZ3U8diFUeMRiOpGwqSbJv+d7/PurA29pC6Q8o9bbs8RtQfwYG4A2yL3sbre17n14G/Usu8Vrn6sunbl/Rdu0hbt57YadPwXbMGlXX5b5+r19qVqJNJXDyYQOh3Zxn5dlvMNFLRBSA/z8CXL+2utP4yU3L5+pU9pWo74f+6lvk6bNiwAWtr6yLb9PqSB6G0Wi3W1tao1Wrc3NxM22NiYnBzcyMoKAgzMzPq1KlD27ZtgYL7sDdu3EhkZCQAkyZNwtbWli1btnDu3Dn+/e9/M3z4cACCg4PZunUr48aNA+Dpp5/G29sbDw8PTp48ybRp07hw4QK//VZw+0Z8fHyRBBsw/RwfH1+m16GsZCRbVBtzb++Cmtthu3CbMxtNgwYYc3L+rrk9ajRpmzZhrOLpHLfzqGfPv6a1ptdzjanlaEFmah67fjjPL/89SMyZGw8sDiGEEEJULaPx3lPFc06dQhcTg0KrpVaP7iX2cSXtCgfjD6JAwZC6Q6oy3GIUCgXvdngXT2tPrmVcY9a+WRUaqXR7+23U7u7orlwh8YP3KxxflyfrY22vIfV6Nn+uvlTh/kT16N69O8ePHy/y+Prrr8vUx4gRI8jOzsbPz4/x48ezZs0a0+JjFy9exMfHB0dHRzIzM9mzZw+ff/45LVu2ZNSoUTz55JOmftzd3UlO/nux4gkTJtC7d2+aNm3KqFGj+P7771mzZg2XL1+unCdfATKSLaqd0tIS+yeewG5EQc3t5B9/JG3bdrKPHOHakSOonJ2wf2Ikdk88gZlr1S/uoFAqqNfGFb9AZ07tvsrhTVHcuJbJ+v+dwKuRPR2G18Wpdvm+KRZCCCFEzXDjWiap17NRqZXUaexQbH/qrdrYtXr0QGlV8qjumksFo9jtPdrjbl3yyuNVycbchoVdFjJm8xi2RW/j14u/8kSDJ8rVl8rGBo/33ydm7FhSfl2Fdffu1OrRo9yxaSzN6BnciN8/Pc6ZPdfwaeqIT1Oncvf3qFCbK5nwf11L1TY2PIUNi07ct92Ayc1LXBm/pHOXlZWVFXXr1i2y7erVspVo8/Ly4sKFC4SGhrJ9+3YmTZrEwoUL2b17N/n5+aaF0wrvt7a67ffN2tralFgfPXq0WCy3a9euHQCXLl3C398fNzc3Dh48WKRNQkICQJGR9qogI9mixiisue358cdFam7rryeRtHgxl3r25NrUqQ+s5rbKTElgUB1Gz21P8yAvlGoFV84l8/N/D7Fj2VkyknOqPAYhhBBCVI2IWwtGeQU4YG5RdNzJqNcXlO4CbAaUPFVcb9Dz++XfgQez4NndNHVuykstXwJgwaEFXEy+9z2892LVri0Ot6bixr39DvlJSRWKrXZDB5r38AJg5w/nyc6QhWUVCgVmGlWpHl4BDljZae7Zn7W9Bq8Ah1L1VxX12+9kbm5e4nRyrVbLwIED+eyzzwgLC2P//v2cOnUKPz8/Ll68iE6nw87OjoYNGzJv3jx0Oh3nz59n5cqVGAwGNm7cyOLFi5k8efJdz338+HGgYMQboH379pw6dYrExL8Xh9u+fTs2NjYEBARU7hO/gyTZoka6vea258cfoW3V6lbN7c0FNbeHDCX5l18w3La4QVWxsDKj07/q8fS7j1G3tQsY4fz+eFbM/Iu/1l4mL7vqa+0JIYQQonJF3Crd5d/Sudi+rAMH0CclobK1xbpjxxKP/zP2TxKzErHT2NHDq/wjvpVhTOMxdPLsRK4+l9d2v0aWrvyfj5xffglNgwbob94k7q23Kzyw8dgQP+zdrchOyyNsxYUHMlDyqFAqFXQeWe+ebTo9Ua9GrRvk4+NDZGQkx48fJykpidzcXEJCQvjmm284ffo0ERERLF++HK1Wi7e3N05OTjRr1sy0oNnixYtZuXIlWq2WoKAgBg0axPLly5k5cya//PILjRo1AuDy5cvMnTuXI0eOEBUVxbp16xgzZgxdunShWbNmAPTq1YuAgACeeeYZTpw4wdatW3n77bd58cUX0Wju/eVFRUmSLWo0hbk5Nv364bNiOb5rfsNuxL9QWFiQe+EC8TPfJbxbdxLe/4C86Ogqj8XWWUvv55swfFor3Ovakq8zcGRLNMtn7udU2FX0+gdbhkwIIYQQ5ZOSkMWNa5kolYoSpzAXLnhWq08fFObmJfZRuODZAL8BmKtKbvOgKBVK/tvpv7hoXYhMjWT+wfnl78vcHI8FC1CYmZGxezcpP/9SodjU5ioeHxeAUqUg4th1Lhyo2gWnHjX+LVzo80KTYiPa1vYa+rzQpNrrZN9p+PDh9OnTh+7du+Ps7MxPP/2EnZ0dX331FR07dqRZs2aEhoayfv16HB0dAZg/fz6vvfYaR48epWXLlkRFRRETE0NUVBQfffQRN2/e5MiRI3Tu3Nl0HnNzc0JDQ+nVqxcNGzbk1VdfZfjw4axfv97URqVSsWHDBlQqFe3bt2f06NGMGTOmSF3tqiL3ZIuHRmHNbZfXXiPltzUk//gjuitXuBkSws1ly7Dq3AmHUaOw6twZhbLqvj9y87Vl6KstiTyRxP41l0lJyGLPyouc3HWV9kP98W3u9ECm4wghhBCifApHsT0b2GFhZVZknyE3l/Rt2wCwvctU8RvZNwi7EgZU71Tx2zlYOPB+l/d5ftvzrL20lnbu7RjgN6BcfVk0qI/zK6+QuGABCR98gNVj7TD38Sl3bM51atFmgC8Hfo9gz8qLeNS1w8ZJW+7+/mn8W7jg29yZuPAUMtNysbLR4F7PrspHsENCQkrc3q1bN9OMhLFjxzJ27FjTPo1Gw6pVq4odM2TIkLuep0OHDnz88ccEBQUxYcIEnn/+eerVq4der+fkyZPMnz+fHj168Morr5iO8fLyYvfu+6/S7u3tzaZNm+7brrLJSLZ46KhsbXEcN7Z4ze09e7nywr+53LsPN779Dn1KSpXFoFAo8At05smZbenyZH0srM1ISchi85JTrPnoKAmRaVV2biGEEEJUzOVbq4r7lTAKmLFnD4aMDNRubgW3q5VgQ8QG8o35NHFsQn37+lUaa1m0cWvDC81eAGDu/rlEp5V/pp/D2GAs27XDmJ3NtWnTMOZX7Pa4lr3q4OZniy5Hz45l5zAaZNp4WSiVCjwb2FO/jRueDexr1BTxyhAcHExYWBgXLlygRYsWmJubo9FoGD16NJ06deLFF1+s7hDLRJJsUYzeoOdwwmFO5J3gcMJh9IaSa+FVt8Ka23W+/BL/rVtwGDsWpY1NQemJBQsI79aduHfeIefcuSqLQaVS0rRbbZ6Z255WfbxRmSmJu5TKqg8Os+3r06QlZVfZuYUQQghRduk3c0iMSgMF+DYvPlU8zVQbu1+JM+OMRiO/hRfU4a0po9i3e6HZC7R2bU1Wfhav736dPH35FhtTKJV4vD8fZa1a5Jw4SdKSpRWKS6lSEjSuEWqNitjwFI7vuFKh/sSjp1mzZixbtozk5GSioqJISEggIiKCadOmYX6X2zZqKkmyRRGh0aH0Xt2bCTsm8GvWr0zYMYHeq3sTGh1a3aHdk7m3N67Tp5VQc3sVkUOHEfX0KFI3bqyymtvmWjWPDfFn1OzHaPiYGygg/HAiK2b9xR+rwsnJ1FXJeYUQQghRNoVTxd39bbGyLXqfqz4jg4xduwCwHVDyVOsT108QkRqBhcqCvr59qzbYclApVbzf+X3sNfacu3mOj498XO6+zNzdcZs5E4CkL74g+8T9y0ndi62zJZ1HFCzk9dfvl7lxLaNC/YlHk1qtxtPTEyenh7fkmyTZwiQ0OpSpYVNJyEoosj0xK5GpYVNrfKINf9fc9l27Bu8Vy7Hp1xfUarKPHiX21dcI79mT6/9bhC4h8f6dlUMtBwt6jg3giTfbULuhPYZ8IydCr7D8nf0cD41Br5PF0YQQQojqFFE4VTyw+Kri6dtDMeblYe7vj6ZhwxKPL6yN3cunF7XMa1VdoBXgauXKe53eA2DFuRXsjNlZ7r5sBw7Apl8/0OuJfWNahSu7NOrojk8zJwz5RrZ/e1Y+G4lHkiTZAiiYIv7+wfcxUvz+mMJtHxz8oMZOHb+TQqHAslWrgprbO3fgNHlysZrbV195hazDh6uklISzVy0GvRTIgP80x8HDitysfP5cdYkfZ/9F+OEEKV8hhBBCVIOstDziLqUA4NeieJKdtmEDULDgWUmLmGbpstgSuQWAIXWHVFmclaFL7S4EBwQD8M6f7xCXEVfuvtzenYna1ZW86GgSFiyoUFwKhYLuoxtiYW3GjWsZHNwQUaH+Hhby2e/hUFnXSZJsAcDRxKPFRrBvZ8RIfFY8RxOPPsCoKoeZiwvOk18sVnM7ffMWokc/U1Bz++fKr7mtUCjwbuzIyLfb0v2ZhljampOWlMO2r8+w6oMjxIanVOr5hBBCCHFvkSeuYzQWrHZt41h0dev8pCQy9+8HKBi5LcHWqK1k5WdRp1YdWru2rvJ4K+qlli/RxLEJaXlpTNs7jXxD+RYvU9na4vF+QVmwlJU/kx4WVqG4LG3M6T66YKbA0W0xj/RnIjOzgtXrsyr5c6aoGoXXqfC6lZeU8BIAXM+6Xqp20WnRtHFrU8XRVI3Cmts2/fqRc/48ySt+JHX9+oKa2+++S+KHH2I3bBj2Tz+Fubd3pZ1XqVQQ0NGDeq1dOR4aw9FtMSRGpbHmo6P4Nnei/VB/7N2sKu18QgghhChZ4f3YJY5ib94CBgMWzZrd9XPA7QuePQzlOs1UZizouoAn1j/BscRjfH78c6a0nFKuvqzat8cheAw3l31P3NvvoF33O2oHh3LH5hfoTMMO7pzfF0doyFmefLst5tpHLzVRqVTY2dmRmFhwq6KlpeVD8d6pDgaDgby8PHJyclBWYTnekhiNRrKyskhMTMTOzg6VSlWh/h69d7IoF2fL4n9sSjJn/xzWXV5Hl9pd6OzZmfr29R/K/1BYNGyI+9w5uLz2atGa28uWFdTc7tIZ+6efxrpLl0qruW2mUdGmvy8BnTw4tCGSs3/EEnkiiahTN2jS2YM2A3zR1nq4Vk4UQgghHha5WTqunk8GwP8+U8VLEpESwfHrx1EqlAzyH1R1gVYyr1pevNvhXV7f/Tpfn/qaNm5taO/Rvlx9OU+dSsaff5J36TJx78yk9qL/VehzYOcR9bh2IZn0Gzns/TWcnmMalbuvmszNzQ3AlGiLkhmNRrKzs9FqtdWWX9jZ2ZmuV0VIki0AaOnSEldLVxKzEku8LxtArVCTb8znWOIxjiUe4/+O/h+ulq50rt2Zzp6decz9MSzNLB9w5BVTWHPbIXgMmX/8wc0VK8jcs9f0MPPywv6pp7AbNhSVnV2lnNPKVkO3UQ1p1t2L/WsuEXXqBqd2X+P8gXha9vameU8vzMz//vbMYDASG55CVqya2PAUvBo6PXK1EYUQQoiqFnXqBga9EXt3q2IzyPKuXClYOVuppFafPiUev/bSWgA6e3bGxbJ4fe2arI9PHw7EHWDVxVXM2DuDVYNW4aQt+8rNSo0Gz4ULiXxiJBk7dpD622/YDR9e7rjMtWqCxgaw5uOjnN8Xh28zpxIXpHvYKRQK3N3dcXFxQaeTijN3o9Pp2LNnD126dKnwdO3yMDMzq/AIdiFJsgVQUO5hetvpTA2bigJFkURbQUFCt7DrQho7Nmbvtb3subqHA3EHSMhKYNXFVay6uAozpRlt3NrQ2bMzXWp3oY5Nnep6OmWmUCqx7tIF6y5dyIuOJvmnlaT89pup5vb1zz7DZkB/HJ5+GouAgEo5p4OHFf1fbM7VC8nsW32J6zHpHPg9gjN7rtFukB8N2rkRceI6e38OJzMlF9Cy4cQprOw0dB5ZD/8WD9cfeCGEEKI6Fa4qXuIo9saC2thWj7XDzKX431edQcfvl38HamZt7NKY1mYaxxOPcynlEm/ufZMljy9BqSj7bD2LRo1wnvIfrn/0MQn/nYdl27aYe3mVOy6Pena07FWHo1tj2LX8PG5+tljaPJoz+1QqVaUlcY8ilUpFfn4+FhYW1ZJkVyZZ+EyYBHkH8XG3j4t9O+tq6crH3T4myDsId2t3nmjwBIt6LuKPp/7gi6AveKrhU3hae6Iz6NgXu48PDn1A/zX9GbhmIAsOLWB/7H50+ofnWztTze3dYbjNnYOmYUOMOTmkrlpN5LDhBTW3N1Reze3aDewZMb01QeMCsHbQkJGcy45l5/jhnf1sWXr6VoL9t8yUXLYsPc3lYzLlSAghhCgNXa6emDM3gOKlu4xGI6nrC6aK2/QvuTb2nqt7uJlzE0cLR7rU7lK1wVYRC7UFH3b9EAuVBfvj9vPt6W/L3Zfjs8+ibd0KQ1YWsW9Mw5hfvgXVCrUd4IejpzU5GTp2/XBOVuIWDz1JskURQd5BbB2+lS97fskIyxF82fNLtgzfQpB3ULG2GpWGTp6deLPdm2wetpnfh/zOa61fo61bW9QKNVFpUfxw9gcmbJ9Ap5WdeHnXy6y+uJrErIcjOVRqtdiPGIHvmt/w/nFFwUqjhTW3X3uN8B49uf7Z/yql5rZCqaBBOzdGzX6M9kP9MbNQkX4j557H/PFLOAaD/BESQggh7ifmzA3ydQZsnCxw8rIusi/3wgXyLl9GYW5OrV6Pl3j8mvCC2tiD/Adhpnx4R9j87fx5s92bACw6tohjicfK1Y9CpcLj/Q9QWlmRfewYN77+ukJxqcyUPP5sAEq1gqhTNzj3Z/nLjQlRE0iSLYpRKVW0dm1Nc/PmtHZtjUp5/2ktCoUCP1s/ghsH803vb9j75F4+7vYxQ+oOwdHCkaz8LHbE7GDW/ln0/LUnT6x/gv8d+x/HE4/X+NrbCoUCy5Yt8fz4I1PNbbWzM/qkJJI+//zvmtuHDlX4m1e1mYqWvb0JGnv/hT8yknOJe4RLXgghhBCV5fKtqeJ+gc7FFlQqnCpu3bUrqlq1ih2bmJXI3mt7ARhSb0jVBvoADKk7hH6+/dAb9UzbM43U3NRy9WNe2xPXd94G4PqixWSfOl2huBw9rXlssD8Ae38NJ/W6lLwSDy9JskWVsDa35nHvx5nbcS47n9jJygErmRQ4iWZOzVCg4NzNc3x58kue2fwM3X/pzoy9M9gUsanc/6F/UAprbtfduQPPTz5G2/q2mtvPjCFy8JBKqbmdrzOUql1mWu79GwkhhBD/YHqdgehTSQD43bGeidFgIPVWkm0zoOSp4usur8NgNNDCpQV+tn5VG+wDoFAomNl+JnVq1SEuM46Zf84s9yCB7eDB1OrVC/LziZ02DUN2doViC+zphUc9O/Jz9YR+dxaDvnSfh4SoaSTJFlVOqVDS2LExE5tPZEX/Fex6Yhf/7fRfevv0ppZZLZJzk9kQsYFpe6fR5ecuBG8O5utTX3Ph5oUae0+OwswMm7598Vm+HN+1a7AbMQKFhQW5Fy8S/+67hHftRsL8+eRFRZWrfysbTaW2E0IIIf6prl5IJi9Hj6WtOW6+NkX2ZR87Rn5sHEpra6y7Fr/X2mg0mqaKD637cC54VhIrMysWdl2ImdKMnVd28tP5n8rVj0KhwG32LNTOzuRFRJD44UcVikuhVNBzbCPMLFTER6RxdFtMhfoTorpIki0eOEetI4P8B/Fh1w/Z8+Qevuv9HeOajKOuXV0MRgNHE4/yf0f/j3+t/xe9Vvdizv457IrZRZauZk4bKqy5XW93GC7Tp2FWpw6G9HRuLvuey336EjN+Aum7dmHUl35avHs9O6zs7p1AW9trcK9nV8HohRBCiEdb4UKhfoHOKO4ogZl6qzZ2rccfR2lhUezYwwmHiUmPwVJtSW+f3lUf7AMU4BjAq61fBeDDwx9y7sa5cvWjtrfHfd48AJJXrCBj7x8VisvGUUuXJ+sDcGh9JNdj0ivUnxDVQZJsUa3USjWt3VoztdVU1gxew9bhW3m73dt0rd0VC5UF8Znx/HrxV6bsmkLnlZ359/Z/s+LcCq6kXanu0ItR2driOHYs/ls24/XlUqy6dgGFgsy9e7k6cRKXe/fhxjffok9JuW9fSqWCziPr3bNNpyfqSb1sIYQQ4h4MegORJwqnit+xqrhOR/rmLQDYDOhf4vGFo9h9fftiaWZZhZFWj6cbPk13r+7oDDpe3/M6mbrMcvVj3bkT9qNGARD35pvkJydXKK4G7dzwb+GMwWBk+3dnyc+r2ev3CHEnSbJFjeJh7cHIhiNZ1HMRe5/cy+c9P+fJBk/iae1JniGPP2P/5P2D79NvTT9TibC/4v6qUSXCCmtu11m6FP+tW3AYNw6ljQ26q1dJXLiQ8K7diH37bXLOnr1nP/4tXOjzQpMSR7TVGiVufrZV9RSEEEKIR0LcpVRyMnRorNR43jH7K3PfPvQpKagcHbFq167Ysel56WyP3g5UUm1sgx5F9B943tyPIvoPqAELvyoUCuZ2nIublRvRadG899d75b5Vz+W1VzH38yP/+nXi351VoVv+FAoFXUc1QGtjTnJcJn+tjSh3X0JUB0myRY1lobagc+3OvPXYWwUlwgb/zqutXi1WImz8tvF0/rkzr+x6hd/Cf+N61vXqDt3EvE4dXKe9Qb3dYbi/N7eg5nZu7t81t596+p41t/1buDBmXgcGTGmKQ/Ns+r3YBMfaVuTnGtj9Y829Z10IIYSoCQpXFfdt7oxSVfRjb+qGWwue9e2LQq0uduzmyM3k6HPwt/WnmVOzigVydh182gT18iG0jv4C9fIh8GmTgu3VzFZjywedP0ClULEhYgO/X/69XP0otVo8FiwAtZr0bdtI/b18/RTSWpvT45mGAJzYeYUr529WqD8hHiRJssVDQaFQ4Gfnx9gmY/mm9zfseXIPH3X9yFQiLFOXSWhMKO/ue5cev/YwlQg7cf1EjSgRptRqsfvXv4rX3D527Laa25+hS0godqzCaMAuJRzXxMM4pF+i55iGKFUKIk8kcfFg8fZCCCGEAKPBSMTxgiTbP7DoVHFDVhbpO3YAYHuXqeK/hf8GFIxi31n2q0zOroNfxkBabNHtaXEF22tAot3StSWTAicBMO/APCJSyjdyrG3SGOfJLwKQMPc98q5eq1BcPk2daNzFE4Cdy86Rm1VzZi4KcS+SZIuHUi3zWvTy6fV3ibD+K5nUfBJNnZoWKRE2etNoU4mwzZGbq71EWLGa2/+5veb2F1zq0ZOrL/9dcztt2zYu9Qwi9tnncP9pJbHPPkfKmCE0bZAPwN6fL5KZKmW8hBBCiDslRKeRmZKLmYWK2o3si+xL37ULY1YWZrVrY9G8ebFjL9y8wJkbZ1Ar1Qz0H1j+IAx62DINKGnm2a1tW6bXiKnjzzV5jnbu7cjOz+a1Pa+Rk59Trn4cn38ebYsWGDIziZ0+rUwLv5ak4/C62DpryUjOZc/KixXqS4gHRZJs8dBTKpQ0dmrMxMCJ/Nj/R3Y9sYv3Or5XrETYG3veoOvPXU0lwi4mX6zW6dZmLi44v3hHzW29nvQtBTW3L/XoybUpL5EfH1/kuPyEBOyXvIKDnZHcrHzCVsi0cSGEEOJOEUcLRrF9mjiiNlMV2ZdWOFV8QP8SR6nXXCpY8Ky7V3ccLBzKH0T0vuIj2EUYIe1aQbtqplKqeL/z+zhYOBCeHM7CQwvL1Y9CrcZjwQcoLS3JPnyEG99+W6G4zDQqgsYFoFDAxYMJhB+WWXyi5pMkWzxyHLWODK47mA+7fsjuJ3fzbe9vTSXC9Ea9qUTY8HXD6bW6F3P3zyXsSli1lQgrUnP797XYPfEEWFiQHxdX8gFGI0qjgQZHl6JUKYg6mcTFA/EltxVCCCH+gYxGI5dvTRX3a+FSZF9+cjIZe/cCYDtgQLFj8/R5bIgoKO1V4drYGaVMCEvbroo5aZ2Y32k+AL9c/IWtUVvL1Y+5lxeub70JwPXP/nffxV7vx83PllZ9fQDY/eMFMpJlFp+o2STJFo80M6UZbdzamEqEbRm+hbfbvU2X2l1MJcJ+ufgL/9n5n4ISYaG3SoSlV0+JMIsGDXCfM5vaH39074ZGI9qYUwQ2NwNg7y/hZKbIHxwhhBAC4Ma1TNKuZ6MyU1KncdGR6PRt2yE/H03Dhmjq1i127M6YnaTmpuJq6UoHjw4VC8TatXLbPQAdPDvwXJPnAJi1bxZX06+Wqx/bYcOwDuoJOh3X3ngDQ27FPqe07u+Dc51a5Gbls/OHczKLT9RokmSLfxRPa09GNhzJ4p6LSy4Rdu1WibDf+jFo7SAWHlrIgbgDD7xEmCEru1TtGrql4uJd8Adn14rz8gdHCCGEACKOJQJQJ8ABc4uiK4enbSyYKn6/Bc8G1x2MSqkqsU2peXcAG497NFCAjWdBuxrkxRYv0ty5ORm6DN7Y80a5PgcpFArc58xB5eRE3qXLXP/44wrFpFIpefzZAFRmSq6cvcnp3RVbVE2IqiRJtvjHurNE2NrBa3m11au0cWuDWqEmMjWS789+z/PbnjeVCFsTvoak7KQqj03t7Hz/RoC5qzM9ghuhVCuIPnWDC3/JtHEhhBCisHSXX4uif0918fFkHToEUFDp4w6xGbH8FfcXAEPqDql4IEoV9Png3m36vF/QrgYxU5qxoMsCapnX4lTSKf537H/l6kft4ID7e3MBuLnsezL3Vezec3s3KzoM8wdg3+pLJMdnVqg/IaqKJNlCUPBtq7+dP2ObjOXb3t+aSoQN9h+Mg4WDqUTYzH0z6f5Ld0ZuGMmiY4s4ef0kBqOh0uOxbN0KtZsb3KNkiNrNDcvWrXD0sKbtAF+gYNq43KckhBDinywlIYubsZkolQp8mjoV2Ze2aXPBLVetW2HmUXyEee2ltRgx0tatLV61vConIAe/u+9rGQwBgyrnPJXMw9qDuR0LEuTvznzH3qt7y9VPrW7dsHtyJACxM95En5JSobiadq2NVyN78nUGQr87i15f+Z/DhKgoSbKFKEFhibD3Or3Hrid2mUqENXFsAsDZG2dZenIpozaNovsv3Xlz75tsidxCWl5apZxfoVLh+uaMWz+UnGi7zpiBQlXwzXeLx+vg4mNDXnY+u5bLtHEhhBD/XIW1sT0b2mNhZVZkX9qGggXNSlrwTG/Qs/bSWqCgNnal2XtrnZWAIeSPXsth74noW4+/FexOeMC3pJVFzzo9earhUwC89cdbJGYllqsf1zfewNzbm/yEBOLnzKnQ5xSFUkGPMQFoLNUkRqdzZFNUufsSoqr8I5LsxYsX4+Pjg4WFBe3atePgwYN3bfvVV1/RuXNn7O3tsbe3JygoqFh7o9HIzJkzcXd3R6vVEhQURHh4eFU/DVFNbi8R9tOAn0wlwnp598LazJqbOTdZH7Ge1/e8TpeVXQjeHMw3p74hPDm8Qn9EbHr1wvP/PkXtWvJiKIbsv1dDV6qU9BzTCJVaScyZG5zff5eVyYUQQohH3OWjBYmgX2DRqeK5EREFq1yr1dTq3bvYcQfiDhCXGUcts1oE1QmqnGCSwuFMQTkwuryO0bsT1xzaY+gxE6ycISUGTv1aOeeqIq+2fpWGDg1Jzk1mxt4Z6MtR01tpaYnHwgWgUpG2abOphFp5Wdtr6PpUAwAOb44mPjK1Qv0JUdke+ST7559/ZurUqbz77rscPXqU5s2b07t3bxITS/4mLiwsjKeeeopdu3axf/9+vLy86NWrF9eu/b24woIFC/jss89YsmQJBw4cwMrKit69e5OTk/OgnpaoRk5aJwbXHcxH3T5iz5N7CkqENR6Hv62/qUTYp0c/Zdi6YfRe3Zu5++ey+8pusvNLt5jZ7Wx69aLujlA8vv2GuKeexOPbb3CaMgWAhHnz0SX8XfLDwcOKtgMLpo3/8Us46Tfl/SiEEOKfJf1mDonR6aAA3+Z3TBW/ldhZdeyA2t6+2LGFtbH7+fXDQm1ROQHt/RgwQoN+4Nbk7+1mWmg/+e825UhcHxSNSsPCLgvRqrUcjD/Il6e+LFc/2mbNcJo4EYD4OXPQxd6rfvj91WvjSr02rhgNRkK/O4sut+a+huKf55FPsj/++GPGjx/PuHHjCAgIYMmSJVhaWvLtt9+W2H7FihVMmjSJwMBAGjZsyNdff43BYGDHjh1AwSj2p59+yttvv83gwYNp1qwZ33//PbGxsaxdu/YBPjNRE5hKhLWeytoha9kyfAtvtXuLzp6d0ag0xGXG8cvFX5i8czKdV3ZmYuhEfjz3Y5nKYShUKizbtCE9MBDLNm1wmjAeiyZNMKSlETdzZpHR8sDH6+Dqa0Nejp4wmTYuhBDiH6Zwqri7vy1WthrTdqPRSOrGu08VT8lJYUdMwWe9YfWGVU4wyVFw8ueCf3d+rfj+Ns+BhR3cCIezv1fOOauIj60P7zz2DgBLTizhUPyhcvXj9O8XsGjeDEN6OrEz3sRoqNj91F2erI+VnYbUxGz2/XapQn0JUZke6SQ7Ly+PI0eOEBT095QfpVJJUFAQ+/fvL1UfWVlZ6HQ6HBwKaixGRkYSHx9fpE9bW1vatWtX6j7Fo8vT2pMnGz7J50Gfs/fJvSzuuZiRDUbiYeVBrj6XP679wfyD8+n7W18GrR3Eh4c+5GDcQXSGu9+PpTfoOZxwmBN5JziccBiDUoHH/HkozMzI3L2H1DVrTW2VSgU9g29NGz97k3P7ZNq4EEKIf46IW6uK+7dwKbI95/RpdNExKCwsqNWjR7HjNkZuRGfQ0dChIQGOAZUTzB+fglEP/j2gdqvi+zW14LGCkV32fgQ1/Ivxgf4DGeQ/CIPRwPQ900nOSS5zHwq1Gs8PPkCh1ZJ14AA3Q5ZVKCYLKzN6BjcC4PTua0SfvlGh/oSoLOr7N3l4JSUlodfrcb3jnlZXV1fOnz9fqj6mTZuGh4eHKamOj4839XFnn4X7SpKbm0tu7t+rPqelFSyQpdPp0Olq3oIXhTHVxNgeFmrUtHdtT3vX9rzR8g0i0yLZe20vf8T+wfHrx4lMjSQyNZJlZ5dhbWZNO7d2dPLoREePjjhpC6a47biyg4VHFpoWGvl1x6+4WLrweqvXaTlpEjf+7/9ImDcPTZvWBauRA9aO5rQe4M2BtZH88Ws47nVrYe1QSdPeBCC/HzWNXI+aRa5HzfJPuh5ZaXnEXkoBwKuJXZHnnPz7OgCsundDb26O/rZ9RqOR1RdXAzDId1DlvFZpcaiPr0AB5Hd4BeMd18F0jpbPot73GYqE0+Sf3YCxfp+Kn7sKvdHyDU5eP0lUWhRv7n2T/+v6fyjuUQmlJApPT5xef53rc+aQ+MknaNq1RVO/frljcqtbiyZdPTi9O5Yd359lxJutii14dzf/pN+Ph0FNvx5liUthfITnk8bGxuLp6cm+ffto3769afsbb7zB7t27OXDgwD2Pf//991mwYAFhYWE0a9YMgH379tGxY0diY2Nxd3c3tX3iiSdQKBT8/PPPJfY1a9YsZs+eXWz7jz/+iKWlZXmenniIZRuyuZx/mQu6C1zMv0imsWidR0+VJ/YKe07nn75rH09rRtL7mz1or1whs359rj07zrQSudEI1/+yJC9FhcYxH6c22feqBiaEEEI89DKumJFy2gIzWz2uHf5eHBSDAb9581Gnp3MteAyZAUVHqq/lX+OLjC9Qo2aazTS0Sm2FY2lydQX+17eSZNWAP+u/dc+2Add+pl7iRm5a+rO3/sx7lu+sCeL18SxJX0I++fS16EtHi45l78RoxGPZMqzPnSfXzY2Y/0zGqC7/2J9RDwl/WpKfqULrpsMhMKemv4ziIZSVlcXTTz9NamoqNjY292z7SI9kOzk5oVKpSLhtcSiAhIQE3G6N+t3Nhx9+yPvvv09oaKgpwQZMxyUkJBRJshMSEggMDLxrfzNmzGDq1Kmmn9PS0kyLqt3vIlUHnU7H9u3befzxxzEzK923gaJ8DEYD526eY++1vfwZ+ydnbp7hmv4a17h212MUKNihCuPFzz4jduRTWF28SKfcXGyG/X0fWUrrLFZ/cIzcG2r87FrQqKP7XfsTZSO/HzWLXI+aRa5HzfJPuh6bPj9NCskEdvGnRa+/a1xn/fUXsenpKG1s6PLSSyjueB3mHZwHlyDIO4jhHYdXPJDM66gXvQCA3aD36OfX3bSrxOuR0Rrj4h04ZF2mf4A1Rt+uFY+hitUKr8X8Q/PZnredp7s+TWPHxmXuI7/dY1wZPgxNfDxtwsNxevXVCsV0vXk6az86QXa8GfVdmlCvjct9j/kn/X48DGr69SiciVwaj3SSbW5uTqtWrdixYwdDhgwBMC1iNnny5Lset2DBAv773/+ydetWWrduXWSfr68vbm5u7Nixw5RUp6WlceDAASbeWjGxJBqNBo1GU2y7mZlZjXwTFarp8T0qAt0CCXQL5D+t/kNSdhI/nP2Bb0+XvDgfgBEjCVkJXLDPxO+lKSQu/JCkhR9i07kzZh4eADjXtuWxwX78ueoSf62JxKepMzaOFf92XvxNfj9qFrkeNYtcj5rlUb8euVk6Yi+kAFCvlWuR55q5ZQsANn36YH7H7MHs/Gy2RBfsH95geOW8Roe/hPxs8GiJuv7jJY5MF7ke9p7QaiwcWIJ636dQv5LKh1Whpxo9xeHEw2yP3s70P6fz68BfqWVeq0x9mLm74f7ee1yd9CIpy77HpnsPrNq1LXdMHv4OtB3gw4F1kfz5yyW8GjpSq5S3yz3qvx8Pm5p6PcoS0yO98BnA1KlT+eqrr1i2bBnnzp1j4sSJZGZmMm7cOADGjBnDjBkzTO0/+OAD3nnnHb799lt8fHyIj48nPj6ejIwMABQKBS+//DLvvfce69at49SpU4wZMwYPDw9TIi9ERThpnWhg36BUba9nXcdh7Fi0zZtjyMgg7u13iqwo3qyHF+7+tuhy9ez6QVYbF0II8WiKOnUDg8GIvbsV9m5Wpu2GvDzSt20HwGZA/2LHhUaHkqHLwNPak7Zu5U/wTLJuwsGvCv7d5fXST/3uMAWUZhC1F2L+qngcVUyhUDCrwyw8rT25lnGN2ftnl+szRq0ePbAb8S8wGomdPh19GUYKS9Kyt7epysqOZWcxGuRzj6gej3ySPXLkSD788ENmzpxJYGAgx48fZ8uWLaaFy2JiYoiL+3sF5i+++IK8vDz+9a9/4e7ubnp8+OGHpjZvvPEG//nPf5gwYQJt2rQhIyODLVu2YGEhi0uJyuFs6VzqdgqVCvf581FoNGTu20fKr7+a9iuVCnqMaYTaTMnV88mc2VuxmpRCCCFETfT3quJF/35m7tmDIT0dtasrlnfMTgT4Lfw3AIbUHYJSUQkfiw9+CXkZ4NoEyrKIma0nBD5d8O89H967bQ1hY27Dgi4LUCvUbI3ayqrwVeXqx3X6dMy8vMiPiyP+vfcqFJNSpSRoXABqcyXXLqRwYueVCvUnRHk98kk2wOTJk4mOjiY3N5cDBw7Qrl07076wsDBCQkJMP0dFRWE0Gos9Zs2aZWqjUCiYM2cO8fHx5OTkEBoaSv0KrIooxJ1aurTE1dIVBXf/BtzN0o2WLi0B0Pj54vzyywAkvv8Bumt/389t52rJY0P8Adi3+hJpSdlVF7gQQgjxgOly9cScKSjd5HdHkv3/7N13dJRFF8Dh37slvfeEhDR6Db0GpLeIFBsCKsWC4idioYgCIk2KDSsCoqBYwEIRRWroNfROGum9l83ufn+8yYYICalkCfOck8PL7szsXZYkOzt37qRt2QqAzZAhSIqSb3sj0iM4HnccCYlhDYZVPZC8DDj8hXwd+DooKvg2u/sUkBRwbQdEn6p6PPdAK+dW/K/t/wBYfHQxV1OuVngMhaUlHh8sBoWC9D83k/7XX1WKyc7Fgm6PNgTg8O83SIrKrNJ4glAZRj3J3rlzJzNnzmTixImMHz++xJcg1GVKhZLpHacDlDrRfqXNKygVSsPfHZ4ei3nbtuiys4meNatk2ngvT9wbyGnju76/JNKnBEEQhDoj4nwSBRodNk5mOHlaGW7XZmaSuXs3ALZ3SBX/7dpvAHSt1xU3y7IL4pbLsVWQmwqODaHZIxXv7+AHLR+Tr4OXVT2ee+SZ5s/QvV538rR5vLn3TXIKKv5hvkWbNji+8DwAMXPmovlP0eKKah7ogXcLR7QFOnasuYC2QFel8QShoox2kj137lz69+/Pzp07SUxMJCUlpcSXINR1fb37svyh5bhYlKyOqZTkifX+6P0lbpeUStznv49kZkb2ocOk3nKcnHRL2njU5RTOB5deuVwQBEEQ7ifXC1PF/dq4lDizOePff9Hn5WHi54dp06Yl+hToCvjj2h8AjGgwgirLz4ZDK+TrwKlwy4fgFdK98CSai5sh/mLV47oHFJKC+d3n42zuzPW06yw6uqhS4zi/9BJmLVqgS0sjZsYM9LrKT4wlSaLX2CaYWapJupnJ0S2hlR5LECrDaCfZX375Jd9++y1Hjhzh999/57fffivxJQgPgr7effl75N983edrHrN4jK/7fM3agWtRSkr+Cv2LrTe2lmhv6uuLy2tTAIj7YAn5N28a7rNzsaDzcDlt/MCm6yJtXBAEQbjvaTU6ws8mArfvx043pIoPLjH5BjgQdYCEnATsTe3p5dWLKjv5HWQlgF394tXoynBpAk2HytfBy6se1z3iYObAosBFSEhsurqJbTe2VXgMSa3G44MPkMzMyDp4iJR166oUk6WtKQ+NkQvJnvo7nOhrqVUaTxAqwmgn2fn5+XTt2rW2wxCEWqdUKGnv2p7WJq3lP11a80Jr+fzN+YfnE5MZU6K9/dixmLdvhz47m5i3Z5X4JLjVQ554NLSjIE/Lru8uirRxQRAE4b4WeSmZ/FwtlrYmuPrYGG4vSEwk69AhAGyH3J4qXlTwLMg/CLWyikcFFeTBgY/l6+6vQVXH6/GG/Oe5XyHpetXGuoc6unc0vD+Ze2guEekRFR7D1M8Xl7feBCB+6TLyrl2rUkz+bVxo0tkNvR52fnuB/NyCKo0nCOVltJPsiRMn8sMPP9R2GIJglJ5r+RytnFuRoclg5v6ZaHVaw32SQoHH/Ply2viRI6Rs2HDLfRK9n24iV928ksq5fSJtXBAEQbh/3QgpTBUPcEZSFK9Wp2//G7RazFq2xMTHp0SfxJxE9t3cB1RTqnjID5ARDdbuEDC66uO5t4aG/UGvgwMfVX28e+iFVi/Q1qUt2QXZvLH3DfK1+RUew37UKCwDA9Hn5xP15lvo8ys+xq26P9EIawcz0hNz2f9LxQuzCUJlGO0kOzc3l+XLl9OzZ09eeeUVpk6dWuJLEB5kKoWKRd0XYa4y53jccb678F2J+028vXF5/XUA4pcsJT+y+AgLW2cLugxvAMDBTddIS8i+d4ELgiAIQjXRaXWEhsip4v+tKp6+ZQtw54Jnm69vpkBfQCunVjSwb1C1ILQFsP9D+brbq6Ayrdp4RQILV7NDfoS0m2W3NSIqhYrFPRZjZ2rHxeSLfHjiwwqPIUkS7vPfR2lnR97FiyR8uqJKMZmaq+jzbFOQ4OKBGEJPJ1RpPEEoD6OdZJ85c4aAgAAUCgXnzp3j1KlThq+QkJDaDk8Qap2XjZehAvknpz7hcvLlEvfbj34Kiw4d0OfkEDPz7RJp4y171qNeIzsK8nXs+k5UGxcEQRDuP9HX0sjN0mBmqcajoZ3h9vzISHJCQkCSsB40qEQfvV5vSBUf3nB41YM49yukhoOFE7R9purjFanfCXwCQaeBA59U37j3gJulG+93k8+7XndxHbsjdld4DLWLC27vzQUg6ZtvyD5+vEox1WtkT0Df+gDsXneJ7PSqrY4Lwt0Y7SR79+7dpX7t2rWrtsMTBKMwvMFwenv1pkBXwPTg6eRp8wz3SQoF7gvmI1lYkH3sGCnrf7jlPoleY5uiMlUSfTWVs3vvn0/JBUEQBAHgRmFVcd/WTiiUxW9p07fKRbcsOndC7VLyhI6QhBDC0sMwV5kz0Gdg1QLQaYuP2uryMphYVG28/yram31yLWTGV+/YNaynV0/GNhsLwDsH3yE2K7bCY9j074/t8OGg1xP91jS0mVU777rzUD8c61mSk6Fh97pLJY46FYTqZrST7FvdvHmTmzfFJEAQ/kuSJGZ3nY2jmSPXUq/x0YmPStxv4uWFy+vy9or4ZcvIDw833GfrbE7XwmrjhzZdJzVepI0LgiAI9we9Tl+8H/u/qeJbi1LFg27rV7SK3c+7H1YmVrfdXyEX/4TEK2BmCx0mVm2sO/HtCZ4doCC3+Hiw+8hrbV+juWNz0vLSeGvfWxToKl50zPXtmajr1UMTHU3c/AVVikepVtB3XHMUKomwM4lc2B9N9NVUsqNVRF9NRSey+oRqZLSTbJ1Ox3vvvYetrS3e3t54e3tjZ2fHvHnz0FXh3DxBqGsczBx4r9t7gJyWdTD6YIn77UeNwqJTJ/S5uUT/J228RY961GtsR4FGJ6qNC4IgCPeNuLB0slLzUJsp8Wxib7g99/IV8q5eQ1Krse7Xr0SfLE0Wf4f9DcCIhlUseKbXw76l8nWnSWBmU3b7ypCk4r3Zx1ZBdnL1P0YNUivVLOmxBEu1JafiT/F5yOcVHkNpZYXH4kUgSaT99hvp//xTpZicPK3oNNQPgD3rL7Plk7MknzZnyydn+W7mQa6fur8yBgTjZTST7NWrV3Pu3DnD399++21WrFjBokWLDHuxFyxYwKeffso777xTi5EKgvHp4dmDJxo/AcA7+98hLS/NcJ+kUOA+/30kCwtyTpwg5fvvb7lPovfYpqhNlcRcS+PMbpExIgiCIBi/olRxnxaOqNRKw+1FBc+sHuqJ0qbkxPfvsL/JKcjBx8aHti5tqxbAle0Qdw5MrKDTC1UbqyyNBoBrS8jPhCNf1dzj1BAvGy/mdJkDwDdnv+FQ9KEKj2HRvj2OE+VMgdh3Z6OJr9pE2MbR/I63Z6Xmsf2rc2KiLVQLo5lke3t7M2jQIMN+67Vr1/LNN98wadIkWrVqRatWrXjppZdYuXIl3377be0GKwhG6PX2r+Nj40N8TjxzD80tsdfIxNMT16JzJz/8iLzQUMN9Nk7mdB0pV1c9/Pt1UuNE2rggCIJgvPR6PdcNqeLFe671Oh3pW7cCYDOk9FTxYQ2GIUnSbfdXIIDiVewOE8HCofJj3Y0kQQ/5tBCOfAm56TX3WDVkoO9ARjYciR49M4JnkJiTWOExnF+ZjGmzpmhTU4l5e1al91PrdPq7HuO1/+erInVcqDKjmWT36dOHnTt3Mn26XC05OTmZJk2a3NauSZMmJCffX+kyglAlOi1S+H7qJR9CCt8vF1q5A3OVOYt6LEIlqdgRvoPNNzaXuN/uiSew6NIZfW6uXG1cWzxO80APPJvYG9LGxS8XQRAEwVglRWWSnpCDUq2gfvPiCW5OSAia6GgUlpZYPdSzRJ8bqTc4nXAapaTkkQaPVC2AG3sg6jiozOSCZzWt6VBwagS5qXB8Vc0/Xg2Y1nEaDewakJSbxNv730anr9jWT8nEhHoffIBkYkJWcDApP/xw9053EHM1lazUvDLbZKbkEXM1tVLjC0IRo5lkAzRq1Ih9+/YB0Lp1a1asuL3Iw4oVK2jduvW9Dk0QaseFP+GjFqjWDaN9+Beo1g2Dj1rIt99Bc8fmvBTwEgALjiwgKjPKcJ8kSXi8/z4KCwtyTp0i+bvvS9zXa2wT1GZKYq6ncWZX5G1jC4IgCIIxuF6YKl6/mQMmZirD7UWp4tb9+qEwMyvRp2gVO9AzECdzp6oFULSK3e5ZsHIps2m1UCihu1zElEOfQf79l3FmrjJnac+lmCnNOBh9kDXn1lR4DNMGDXB5Q17Vj1+ylLwbNyo8RlZ62RPsirYThNIY1SQbwKzwh+IHH3zA6tWradasGRMmTGDChAk0a9aMb7/9liVLltRylIJwD1z4E35+GtKjS96eHiPfXspEe3yL8bRxaUOWJouZwTPR3rLyra5XD5dp0wBI+Ogj8m7ckjbuaE63orTxP26QEptVzU9IEARBEKquaD+2/y1VxfUaDel/bQfAZsiQEu01Wo0hu2tEgyoWPAs/COH7QaGGrv+r2lgV0fJRsPOGrAQ4+d29e9xq5G/nz4xOMwD49NSnhMSHVHgM+zFjsOzaRS7m+tY09BpNhfpb2phWaztBKI3RTbKL9OzZkytXrjB8+HBSU1NJTU1lxIgRXL58mcDAwNoOTxBqlk4L26cBd0rbLrxt+/Q7po4rFUoWdF+ApdqSk/EnWXO+5KfFdo8/hmXXrujz8oiZMaNE2niz7h54NbVHK9LGBUEQBCOUGpdNcnQWCoWEd8viFemsQ4fQpqSgdHDAskvnEn323txLcm4yTuZOBHpW8T1k0Sp2m9FgW69qY1WEUg3dp8jXBz+BgvtzpXV4g+EM8h2EVq/lrX1vlSjUWh6SQoH7woUobG3JPXeOhM8rVrHcvaEdlnZlT6At7Uxxb2hXoXEF4b+MdpIN4OHhwfz589m4cSMbN27k/fffx8PDo7bDEoTqU5AvH8mREgax5yDiMFzdAXsW376CXYIe0qPkT9TvwNPak+kd5foGn536jAtJFwz3SZKE+/vzUFhaknP6NMm3FBKU08abojZTEnsjndM7Rdq4IAiCYDyKKj/Xa2KPmaXacHtaYaq4zaBBSCpViT5FqeJD/YeiUpS8r0KiTsD1nSApoduUyo9TWQGjwdpd/v1/+sd7//jVQJIk3u38Ll7WXsRkxTD74OwKFzFTu7riPmc2AElffU32yVPl7qtQSAQ+0bDMNlb2plShLJ4gAFCFnzTV78yZM7Ro0QKFQsGZM2fKbNuqVat7FJUg/EdBHuRlQl66fKRGXob89/yM4uu8jFvuyyjZztAvE7RV/CQ6M67Uux7xf4R9N/exI3wH04On81PQT5ir5GMr1B4euM6YTsysd0j4+BOsHnoIU39/AKwdzOj+aEN2r7vEkT9u4NPSEXs3y6rFKQiCIAjV4E6p4rqcHDL+3QmATVDJVPG4rDgORB8A5FXUKgleLv/Z6nFw8K3aWJWhMpVT1P+eAfs/hIAxoDSqt/LlYmVixZIeSxjz1xh2Ruxkw+UNjGoyqkJj2AwaRMbu3aT/uZnoadPw/e03lFble6/i38aFgS+0IPinqyWKoJlbq8nN0hAXms7B364bttAJQmUY1XdmQEAAsbGxuLi4EBAQgCRJd/x0S5IktNo7V1gWqkGJatY24NdDLrpxv9LrCyfGGSUnwrdOgg0T4TtNngtvK5o86yq2/6dcVOZgagWm1vKZm3qdfP7m3Vi5lnpX0afFIfEhhKaF8uGJD5nZaabhftuRI0n/+x+ygoOJnjETnx/WGz79b9rNnesn44m4kMzOtRcZ8WY7FArxua4gCIJQezKSc4kPzwAJfFsXT7Izd+9Gn52Nul49zAMCSvT54/of6PQ62rq0xcfWp/IPHnceLm0BpOIiZLWh3TMQvFTOgDu3EVo/UXuxVEFzp+ZMbTeVD459wJJjS2jj0oYmDrefKlQWt1mzyD52HE1kJPGLF+E+b165+/q3ccG3tTORlxI5uPcoXXt2xKuJE1ePxfHvmguE7IjAxtGMlg95VvSpCQJgZJPs0NBQnJ2dDddCLbjwJ2yfhio9mvYA4V+AjQcMXAzNht67OPR60OSUvhp826rxnSbPt0yYdQXVH6PaonhSbGoFpjaF19by300Kb7t18nzHdta3fxKt08pVxNNjuPO+bEl+Xby7lhminZkd87rN48V/X+THSz/Sw7MH3et1l0eQJNznvceNh4eSe+YMSWvW4PTcc4b7eo1two9zjxAXmk7IvxG07e9dDf9ogiAIglA5RavY7v62WNiYGG5P21J4NnZQUInzr3V6Hb9d/Q2AEQ2rWPAseJn8Z7NHwLlR1caqChNL+diwne/JMbV8DBRGvfuzVGOajuFozFH23NzDm3vf5Kegn7BQW5S7v9LGBo9Fi4h49llSf/kVq169sO7du9z9FQoJj4Z2WFwtwKOhHQqFRONObmQk5XDkz1CCf7qCtYMZPq2qWI1eeCAZ1STb29v7jtfCPVJUzfq/k7qiataPf1f2RFuvB012GRPfW1aDy7NqrK+BbAW1ZfHk1jDxtf7PdeHEt0Q765KTZxOrmk3RUijlDzZ+fhqQuH2irYeBi8qVYdCtXjdGNx3N+ovreefAO2waugl7M3sA1G5uuE6fTszbb5P4yadYP/QQpg3lvUpW9mZ0e6whu7+/xNE/Q/Fp6YSDu0gbFwRBEGrHjZCiVPHiY7O0aWlkBgcDYPufVPHjsce5mXkTS7Ul/bz7Vf6BE6/COXlfNz3eqPw41aXDRNj/MSRehkub5Yn/fUiSJOZ1m8ejmx8lLD2M9w+/z4LABRUaw7JTRxzGjSN59WpiZr2D+Z+tUDlVbVLcbpAP6Ym5XDwYw9/fnGP4621x8bap0pjCg8eoJtm3WrhwIa6urowfP77E7atXryYhIYFphccQCdWkPNWsf38RLm0FTdYd9h0Xri7rddUfm8mtK8PW/1kNvnWybPOfdv+ZPJtY3l9p782Gyh9sbJ92hyJoijJTxf9rStspHI4+zPW068w9NJcPH/rQ8Gm/7YjhpP/zN1l798lp4xt+LE4b7+rO9ZMJRJxPYufai4x8sy0K5f35ibkgCIJw/8pOzyf6WioAfrfsx07/5x/QaDBt3NjwIXGRTdfkifEg30EVWiG9zf4PAT00GgRuLSs/TnUxs4VOL8C+D+Rq502HgnR/bumyM7NjcY/FjP97PJtvbKaTeyceaVCxDw2cp7xK1oED5F2+TMzbs/D88osSGQ0VJUkSPUc3JiM5l5uXUtj62Rkend4eawezu3cWhEJGO8n+6quv+OGHH267vXnz5jz55JNikl3dwg/epZo1kJ8FZzaUYzDpDivDd5r43iF12rBqXHittrxv06CqRbOh0GQIBTf2ERL8NwHdB6A6vQ7O/Qq/jocXg8HC4a7DmKnMWBi4kKe2PcXOiJ38fu13hjeUC8BIkoT7e4Vp4+fOkfTNKpxefMFwX68xTfjxvSPEh6VzakcE7Qb61OQzFgRBEITbhJ5OAD24eFuXmOykG1LFS65ip+en82/4v0AVz8ZOCYfThe99jGEVu0jnSXDoM4g9I59K0qh/bUdUae1c2/FS65dYEbKC+Ufm09K5JX62fuXurzAxweODDwh79FEy9+4l9aefsX+yanvVlUoFA19oyaYlJ0iOzmLLitOMeLMdpuZGO3USjIzR/k+JjY3F3d39ttudnZ2JiYmphYjquDKqVJfQYiTU73KHFOtb/q62eLAnxtVNoUTv3Z2o8+m09ukO9dtD9ElIvgF/TIYn15frE+ymjk2ZHDCZj05+xKKji2jv1h4vay9APg7DdeYMYqbPIOGzz7Dq1QuzxvKeMyt7U7o/1pBd313k6JZQfFo54ehhVaNPWRAEQRBuVbQf+9ZVbE1cHNlHjwJgO3hwifbbbmwjT5tHA7sGtHBqUfkHPvCxvH3Nrxd4tq/8ONXNwgE6jIeDn8K+JdCw3327mg0wseVEjsUe40jsEd7c+ybrB6/HTFX+lWOzxo1wfu014j/4gLjFi7Hs3AkTH58qxWRqriJocmt+XXyc5Ogstn91lqDJrVGqxHtc4e6M9n+Jl5cXBw4cuO32AwcOiLOya0J5U4/bjYOOz0HrJ6HJEPDrCfXaglNDsHaTV6DFBLtmmVrDY9+C0gQub4UjX5W767PNn6WdazuyC7KZGTyTglsKwtk+8ghWvXqBRkPMjBnoNcVV1Jt0ccO7pSO6Aj271l5Ep62BbQGCIAiCcAd52RpuXkoBSu7HTt/2F+j1mLdrh7pevRJ9is7GHtFwROVTh9Nj4NT38nWPNys3Rk3qMhmUpnDzKIQF13Y0VaJUKFkYuBAHMweupFxh6fGlFR7D4dlnsOjUCX1ODlHTpqEvqHrRW2sHM4Jebo3KVMnNSyns+eFyhc/1Fh5MRjsbeu6555gyZQpr1qwhPDyc8PBwVq9ezWuvvcZzhRWQhWrk3VWuVk1pv4gksKl312rWwj3i3hoGFBYH+WcWRJ0sVzelQsmC7guwUlsRkhDCqrOrDPdJkoTb3DkobG3JvXCBpG++KXFfr9FNMLVQER+ewcl/Iqr16QiCIAhCacLOJKLT6XHwsMTOtXhvdfqWLcDtBc8uJV/iYvJFVAoVQX5BlX/gg5+CNl/O4PPpVvlxaoq1G7R9Wr7eV/FJqbFxtnBmQXf5vc1Pl39iR/iOCvWXFAo8Fi1EYW1N7ukzJH5Z/kWIMuOqb82Aic2RJLh0MIYTf4VVy7hC3Wa0k+w333yTCRMm8NJLL+Hn54efnx+vvPIK//vf/5gxY0Zth1f3FFWzBm6faBf+vZzVrIV7pMNEaPqwfG73r+MgN61c3TysPAznZX9x+gvOJRafx612ccFt1tsAJHz+BbmXLxvus7QzpfvjclGZY1tCSYrKrK5nIgiCIAilul6UKh5QnCqedyOU3PPnQanEesCAEu2LVrF7e/U2nKZRYVmJcHy1fG1Me7H/q9uroFBB6F6IPFbb0VRZt3rdGN9CLno8+8BsojKjKtRf7e6O27vvApD4xRfknD5dLXH5tHQi8Al5G92RP0O5fCS2WsYV6i6jnGRrtVqCg4OZPn06CQkJHD58mNOnT5OcnMy7hd84Qg0oqmZt85+98DYedz++S7j3JAmGrgDb+pASBptflY9RK4cgvyAG+AxAq9cyI3gG2Zpsw302QUFY9ekDGg3R00umjTfu5IZPKyd0Wj07115EK9LGBUEQhBqkydMScSEZAP+2t1QV3yoXPLPs1hWVQ3EB0DxtHltvyPdV6WzsQ59BQQ54tAH/PpUfp6bZeclb+ACC7//VbIDJbSbTyrkVGZoM3tr3Fhqd5u6dbmH7cBA2gweDVkv0W9PQZWffvVM5tHzIk4B+9QHY9d1Foi6nVMu4Qt1klJNspVJJ//79SU1NxcrKig4dOtCiRQtMTU1rO7S6r9lQmHKOgjG/c9x7EgVjfocpZ8UE21iZ28Gjq+VPsc//Bie+LVc3SZJ4p/M7uFi4EJYexrLjy0rc5z5nNkpbW/IuXiTxq69L3PfQ6MaYWqhIiMjg1N/h1fyEBEEQBKFY+LkktBodNk5mONaTi27q9fpbUsVLpoPvDN9Jen46bpZudHbvXLkHzUmBoyvl6x5vGn9Bse5TQVLAle0Qc6a2o6kytULNBz0+wNrEmjMJZ/j01KcVHsNt9ruoXF3JDw8n7oMPqi22rsP98W/rjE6r56+vzpIck1VtYwt1i1FOsgFatGjBjRs3ajuMB1NRNWuHLui9u4sUcWPn1QH6zJavt0+H2HNlty9ka2rL/O7zAfj5ys/su7nPcJ/K2RnXd94BIPHLL8m9eNFwn6WtqSFl6tjWMBJvirRxQRAEoWbcCCmqKu5iKGCWe+48+eHhSGZmWPUuucpcdDb2I/6PoKzs+5cjX0N+Brg0l8/GNnaO/tC8cNU+eFnZbe8T9azq8V7X9wBYc24N+6P2V6i/0tYWj0ULAUjd8BMZe/ZUS1ySQqLvs81w87MhL7uALStOk52eXy1jC3WL0U6y33//fd544w22bNlCTEwM6enpJb4EQbhFl8nQsD8U5Mr7s/PL98lqZ/fOjG02FoB3DrxDUk6S4T6bIYOx7tcPCgrktPH84l8ijTq64tu6KG38gkgbFwRBEKqdVqMj7GwiAP63HN1VtIpt3bsXSitLw+1RmVEciTkCwLAGwyr3oHkZcOQL+brH6/fPiSmBr8t/XvgDEi6X3fY+0de7L080ls+7fnv/28Rnx1eov2WXLjg8IxeGi5n1DgXJydUSl8pEyeBJrbBxNicjKZetn51Gk6+tlrGFusNof3IMHjyY06dPM3ToUDw9PbG3t8fe3h47Ozvs7StZxEIQ6iqFAoZ9CdbukHgFtpX/qJFX275KA7sGJOcmM+fQHMPRFJIk4Tb7XZR2duRdvlyiSqckSfR8qjGmlioSIzM5uV2kjQuCIAjVK/JSMppcLZa2Jrj62ACg12pJ37YNkGuI3Or3a78D0Mm9E57WnpV70OOr5XRxxwbQbFhlQ7/3XJtBkyBAD/s/rO1oqs2bHd6ksX1jknOTmRE8A62uYpNZ56lTMW3YAG1iIjHvvFttx2+ZW5vw8OTWmFrKp67sWHUenU4c7SUUM9pJ9u7duw1fu3btMnwV/V0QhP+wdISRq+R9WSHrIeTHcnUzVZqyKHARaoWaPZF72Hh1o+E+lZMTbrMLq3R+9RU5588XP5ytKT0K08aPbw0j8WZG9T0XQRAE4YF345aq4pJCThXPPnacgoQEFLa2WHXvbmir1WkNk+wRDSpZ8EyTIx/bBfI+5/ttu1zRavaZnyE5tHZjqSamSlOW9FyCucqco7FHWXl2ZYX6K0xN8fjgA1Crydy5k7RNm6otNjtXCwZPaoVCJRF6OpGDv16rtrGF+5/RTrJ79uxZ5pcgCHfg0w0eko/nYuvrkHClXN0aOzTm1bavAvDBsQ8ITy9embYZNEg+HkWrJWb6DHS3pI037OCKX4AzOl1htfECkTYuCIIgVJ1OqyP0tJwq7tfWxXB7+lY5Vdymf38kExPD7YdjDhObFYuNiQ19vCtZDfzkd5CVIJ/a0erxygdfW+q1lSuh67Vw4OPajqba+Nr6MqvzLEA+evR47PEK9Tdr2hTn/70CQNz8BeRHRlZbbB4N7Oj7TDMATu+K5PSu6htbuL8Z7SQbIDU1lWXLljFx4kQmTpzIhx9+SFpa+c4CFoQHVuBU8O0Jmix5f7Ymp1zdxjYbS0e3juQU5DAjeEaJIzPcZr+L0sGBvKtXSfz8c8PtRWnjZpZqEiMzOfFXWHU/G0EQBOEBFH0tjdwsDWaWajwa2AKgy88n/e9/gNtTxYvOxh7iNwRTZSVOoynIK56Ydp8CSnWlY69VRWd6h6yH9OjajaUaDfUfylD/oej0OqYFTyMlt2LHZzmOH495+3bosrOJfmsa+oKCaoutYQdXOg/zA2D/L1cNxfqEB5vRTrKPHz+Ov78/H374IcnJySQnJ7N8+XL8/f05efJkbYcnCMZLoYQRK8HSGeLOwd8zy9dNUjC/+3ys1dacTTzLyjPFKVkqBwfcCs+oT1r5DTlniyuYW9iY0ONJOW38xF/hJESKtHFBEASham6clItc+bZ2QqGU365mBQejS09H5eqKRft2hrYpuSnsipS3Elb6bOzTP0J6lFzbJGB01YKvTd5dwbsbaPOLU9/riLc7vY2PjQ/x2fG8c+CdCu2vlpRKPBYtRmFpSc6pUyR98021xtZ2gDfNunuAHnasOk9cmCjS/KAz2kn2a6+9xtChQwkLC2PTpk1s2rSJ0NBQgoKCmDJlSm2HJwjGzdoVRnwNSHIRl3Pl24PkZunGO13ko7u+PvM1pxNOG+6zGTgAm8GDQKslesb0EmnjDdq74N+mMG38W5E2LgiCIFSeXqe/5eiu4qriaYVVxW0GD0ZSFu+X3nJjCwW6Apo6NKWJQ5OKP6C2oLhYWNf/gdqs8sEbg6K92cfXQGbdWVW1UFuwtOdSTBQm7L25l+8vfF+h/iae9XB9R047T1jxGdkhp8k+dgzrkBCyjx1Dr618hXBJkugxqhH1mzlQoNGx9fMzpCeWL5NQqJuMdpJ9/Phxpk2bhkqlMtymUql46623OH68YnsxBOGB5N9bTh0H2PwqJJfv3PlBvoMY7DsYrV7LjOAZZGuyDfe5vvMOSkdH8q9dJ/HTFYbb5V8ujTGzUpMUlcnxbWHV+UwEQRCEB0hcWDpZafmozZR4NXEAQJuZReau3QDYBA0xtNXr9YZU8UqvYp/bCClhYOEI7Z6pUuxGwb83eLSFghw4/Pnd299HGjs05s0O8gkqH578kPOJ5+/SoyTbRx6R68wUFBA+ejTR4yfg/uMGosdP4FqfvqT/80+lY1MqFQx4vgWOnlbkpOezZcVpcrM0d+8o1ElGO8m2sbEhIiLittsjIyOxtrauhYgE4T700Ezw6gx56fDLOHnPWTm83flt3CzdiMyI5INjHxhuV9nb4zZnNgBJq1aRc+aM4b4SaePbw0mIEGnjgiAIQsVdL6wq7tPSCaVafquaufNf9Hl5mPj4YNasmaHtucRzXEu9hqnSlMF+gyv+YDodBC+Vr7u8DCaWZbe/H0hS8d7soyvlI8nqkCcaP0E/734U6Ap4c9+bZOZnlruvJElYPVRYQPk/K9cFcXFEvTqlShNtEzMVQS+3wtLOlJTYbLZ/dVZk9z2gjHaS/cQTTzBhwgR++uknIiMjiYyMZMOGDUycOJFRo0bVdniCcH9QquDRVWBuDzEh8O+ccnWzMbFhQfcFSEhsvLqR3RG7i+/r1w+bIUNApyN6+gx0ecUT94btXfFv64Jep2fn2gtoNeIXiyAIglB+er2eG6fk/dh+Abemim8F5IJnkiQZbt90TV7F7uvdFxsTm4o/4MU/IfEKmNlCh+eqELmRaTQIXJpBfoY80a5DJEliTtc5eFh6EJkRyXuH3iv3/my9VkvCx5+Ucqc8RtyChVVKHbeyNyNocivUpkqirqSy+/tL1XY+t3D/MNpJ9tKlSxkxYgRPP/00Pj4++Pj48Oyzz/Loo4+yePHiCo312Wef4ePjg5mZGZ06deLo0aOltj1//jwjR47Ex8cHSZL46KOPbmszZ84cJEkq8dWkSSX2AAnCvWDrCcO+kK8Pfw6XtpWrWwe3Djzb/FkAZh+cTWJOouE+11lvo3RyIv/GDRI/LVlYpeeoRphbq0mKyuLYtrpxTqcgCIJwbyRFZZKemItSrcC7hSMABUlJZB08CIDNkOLV6mxNNn+F/gVU8mxsvR72Fa5id3oRzCoxSTdWCkXx3uzDn0Ne+Vd77wc2JjZ80PMDlJKSv8L+MmwZuJvs4ycoiI0tvYFeT0FsLNnHT1QpPidPawY83wJJIXH5SCzHtoj3Qw8ao51km5iY8PHHH5OSkkJISAghISEkJyfz4YcfYmpa/qMZfvrpJ6ZOncrs2bM5efIkrVu3ZsCAAcTHx9+xfXZ2Nn5+fixatAg3N7dSx23evDkxMTGGr/3791f4OQrCPdN4EHSZLF//PglSy3eO4+Q2k2ls35iUvBTePfCu4ZNYlb097nPnAJC0eg05ISGGPubWJvR4sjEAJ/+OID5cVNgUBEEQyqcoVbx+MwfUpnJxs/Tt20GrxaxFC0x9fQ1td4TvIEuThaeVJ+3d2lf8wa78DXFnwcRKnmTXNc2Hg4O/nC5+fHVtR1PtWju35pU28vnXi44u4lrKtbv2KUgoXyG48rYri3dzR3qOkrfRHdsaxqVDMVUeU7h/GO0ku4iFhQV2dnbY2dlhYWFR4f7Lly/nueeeY9y4cTRr1owvv/wSCwsLVq++8w+bDh06sGTJEp588skyJ/MqlQo3NzfDl5OTU4VjE4R7qs9suRBKbipsnADauxfjMFGasDBwISYKE4Kjgvn58s+G+6z79MFm6MNy2viMmehycw33NWjnQoP2RWnjF0XauCAIglAuNwon2f63VBVPN6SKDynRtmj1cnjD4SikCr6l1eth3xL5usMEsHCoZMRGTKEsLoB6aAVo6l6163EtxtHNoxu52lze2PsGOQVlP0eVs3OZ91e03d00D6xH2wHeAOz+/hKRl5KrZVzB+BntJLugoIB33nkHW1tbQ7q4ra0ts2bNQqMpX6W+/Px8Tpw4Qd++fQ23KRQK+vbty6FDh6oU39WrV/Hw8MDPz4/Ro0ffsUibIBgVlQk8uhpMbSDyCOxeUK5uDe0bMqXdFACWHl9KaFpxypPbzJmonJ3JDw29bY9TjyfltPHk6CyObhVpUoIgCELZUmKzSI7OQqGQ8GklL17k34wi59QpkCRsBhWnioelhXEy/iQKScEj/o9U/MFC90LUcVCZFWd61UWtngBbL8iMg1PrajuaaqeQFMzvPh8ncyeup11n8dGyt5RatG+Hys1NLg5XCqWjQ4lz2Kuq8yN+NGjvgk6nZ/tX50iKrlup+8Kdqe7epHa88sorbNq0iQ8++IAuXboAcOjQIebMmUNSUhJffPHFXcdITExEq9Xi6upa4nZXV1cuXbpU6dg6derEt99+S+PGjYmJiWHu3LkEBgZy7ty5Uiuf5+XlkXdLgaj0dDmFVqPRlPtDg3upKCZjjO1BVG2vh7Un0pAPUW2aAPuXU+DZGb1/77t2e7zB4+yN3MuR2CNM3zedNf3XoFaowdIS59nvEjP5FZK//Rbz3r0wDwgAQGUq0f3xBuxYdZFTf4dTv4U9Lt5142QA8f1hXMTrYVzE62Fc7qfX49qJOAA8GtuhUMsxp27eDIB5xw7gYG94HhuvbASgq3tXHEwcKvz8lHs/QAFoA8aiM7WHe/TvUxuvh6LzKyj/fgv9/o8oaPUUKE3u2WPfCzYqG+Z1mcdLu15i49WNtHduzwCfAaW2d5r2FrFTX5cn2ncoSKbNyCB9/wEsunapthh7PNWQzORcYm+ks+XT0wx7PQAL27r1OlQHY/95VZG4JL2RlruztbVlw4YNDBo0qMTt27ZtY9SoUaSlpd11jOjoaOrVq8fBgwcNE3WAt956i71793LkyJEy+/v4+DBlyhSmTJlSZrvU1FS8vb1Zvnw5EyZMuGObOXPmMHfu3Ntu/+GHHyqVBi8IVdEq8lt8E3eRp7Jmd5P55Knt7tonXZfOpxmfkqPP4SHTh+hrXpwh4vrzz9ieOEm+kxPhr/4PvUnxL46kEDNyYtSorLS4ds1GUtbEMxIEQRDud3EHLdCkKbFrnotVffnNrPeHH2EaG0vsyJGkd+wAgFavZUn6EjL1mYyyGEVzk+YVehyHzMsEXp2PTlKyo9lSck0cq/25GBOFLp9+51/HrCCNU/UnEOHYs7ZDqhH/5vzLnrw9mGLKS9Yv4ags/XW1OncO5z83o75lPqGxtUFrYYlZTAw6pZLYp0aR2aJFtcWnzYeEQ5YUZCtQ22hx7pSNwmiXO4U7yc7O5qmnniItLQ0bm7ILJRrtS2tqaoqPj89tt/v6+mJiUr5PfpycnFAqlcTFxZW4PS4ursyiZhVlZ2dHo0aNuHat9IILM2bMYOrUqYa/p6en4+XlRf/+/e/6ItUGjUbDjh076NevH2q1urbDeeBV++tR0Bv9mgGYxp+nf8bPaJ/aKO/dugvHCEem7Z/Gvvx9PNPjGVo7twZA2707EcNHYBIfT/srV3F+601Dn9yeGn6Zf4KcDHDSNaXTw76lDX/fEN8fxkW8HsZFvB7G5X55PTKTc/nhr2MgQdBTPbGwMSHvyhUiY2NBrabLa6+htJXfL+29uZfMfZnYm9oz5eEpqJUVe17KH9fKF62foveQsdX9VMpUW6+HwikKds4mIGMXLUYvKNfv/PtNf11/Xtj5AqcSTrFdvZ01/dZgUtqq/eDB6F9/ncyjRwnZvZuAXr2w6tgRdDpip00ja8e/ePzwIy7z3sPm4YerLca0bjn8sTyE3HRQxXjT/7lmKBSlp64/aIz951VRJnJ5GO0ke/LkycybN481a9YYCpDl5eUxf/58Jk8u394ZExMT2rVrx86dOxk2bBgAOp2OnTt3lnuM8sjMzOT69euMHVv6D2pTU9M7FlJTq9VG+Z+oiLHH96CpttdDrYbH18JXPVGE70dx6GN4aNpduw32H8yBmAP8ef1PZh2axcahG7FUW6J2dMRj3ntEvvAiaevWYTegPxbt5Uqvajs1D41uwl9fnuXMzps0aOeKm69t1Z+DERDfH8ZFvB7GRbwexsXYX4+Ic/KxSh4N7LB1tAQg5e9/ALDq2QMzp+JVyT9C/wBgqP9QLMwqmA0YdRJu7ARJiaLH6yhq6d/knr8eHSfCwY+QUkJRX9kCLR+9d499j6hR80HPD3h086NcTL7IijMrmNaxjPc2ajXWXbqQkZKCdZcuhtfD68MPiXnnXdJ++434mW8j5ebi8NRT1RKjk4eaIS+15vflp4g4l8yRTaEEPtmoxNnvgvH+vKpITEZb+OzUqVNs2bIFT09P+vbtS9++ffH09GTz5s2cPn2aESNGGL7KMnXqVFauXMnatWu5ePEikyZNIisri3HjxgHw9NNPM2PGDEP7/Px8w5Fh+fn5REVFERISUmKV+o033mDv3r2EhYVx8OBBhg8fjlKpZNSoUTXzjyEINcGpIQR9KF/vXQRh5TuGbnrH6XhYehCVGcWio4sMt1v17IntiBGg1xP99tvosrMN9/kFONOooyt6Pexae5ECjbZan4ogCIJwf7t+Sj5a1S9Aruqs1+tJ37IFANugIEO7hOwEgm8GA3JV8QoLXib/2fIxcLj/M6vKzdQKOr8sXwcvA13dPPXDzdKN97u9D8C6i+vYE7mnwmNIKhXu89/HvnDxLO69eSR+vbL6YvSzpe+4ZgCc3RvF6Z3lO1ZVuL8Y7STbzs6OkSNHEhQUhJeXF15eXgQFBTFixAhsbW1LfJXliSeeYOnSpbz77rsEBAQQEhLC9u3bDcXQIiIiiIkpPrcuOjqaNm3a0KZNG2JiYli6dClt2rRh4sSJhjY3b95k1KhRNG7cmMcffxxHR0cOHz6MczWV+xeEe6b1ExAwBvQ62DgRshLv2sXaxJoFgQuQkPj92u/8G/6v4T7X6dNQubmhCY8g/sOPSvQLfKIRFjYmpMRmc/RPUW1cEARBkGWn5xNzXd4b61d4dFfOqRA00dEoLCyweughQ9s/r/+JVq+llXMr/O38K/ZAcRfg0hZAKj7a6kHS8Tn5hJH4C3B5W21HU2Me8nqIMU3HADDrwCxis2IrPIakUOA6cwaOk+Tz0xOWLyd+2XKqq5RVg3YudB3RAIADG68ZPmQS6g6jTRdfs2ZNtY01efLkUtPD9+zZU+LvPj4+d/0G2rBhQ3WFJgi1b/AHcPMYJF6G316Ep34GRdmfv7Vzbcf4FuNZdW4Vcw7NoZVzK1wsXFDa2OA+bx6Rzz1HyvffY92vL5YdOwJgZqnmodGN2fbFWUL+jcCvjTNufnUjbVwQBEGovNDTCaAHF29rrB3MAAyr2Nb9+qIwk2/T6/X8fu13AEY0KDuT8Y6KVrGbDQXnxlWO+75jbidPtIOXQfBSaDKkzKOs7mevtXuNk/EnuZB0gWn7prFqwCpUFawyJkkSLq++itLKivglS0lauRJdViaus2Yh3eV9UnkE9PMiPTGHc/ui2LH6ApZTTevMdjrBiFeyiyQkJLB//372799PQkJCbYcjCHWPiSU89q18Vui1HXDo03J1ezngZZo6NCUtL413D7xr+HDKKrA7do/Je71i3p5VIm3ct7UzjTu5odfDzrUXKcgXaeOCIAgPuuun5Pd3RavYeo2G9O3bAbC5JVX8VPwpwtLDMFeZM9B3YMUeJPEanN8kXwe+UfWg71edXwK1BUSfgus7azuaGmOiNGFpj6VYqi05GX+SL09/WemxHCdMwG3OHJAkUn74kZgZM9AXFFQ5RkmSCHyiId4tHNFqdGz7/AxpCTlVHlcwDkY7yc7KymL8+PG4u7vTo0cPevTogYeHBxMmTCD7ljftgiBUA9dmMGixfL3zPYg8etcuaqWaRYGLMFWaciD6AD9e+tFwn8u0aajc3dFERhK/bHmJft0fb4iFrQmpcdkc+fNGtT4NQRAE4f6Sm6Uh6lIKAP5tXADIOnwYbXIySgcHLDt3NrTddFWeJA/wGYCl2rJiD7T/Q3lrVKOB4N6qeoK/H1k6QTu5LhH7ltVuLDXMy8aL2V1mA/D1ma85ElP20b1lsX/yCTyWLAGlkrQ//uTmlCno8vOrHKNCqaD/xOY4eVmRk6Fhy4rT5GYZ5xnRQsUY7SR76tSp7N27l82bN5Oamkpqaip//PEHe/fu5fXXX6/t8ASh7mn7DLQYCboC+HU85KTctYufnR9T28n72pafWM711OsAKK2scH9/HgAp69eTdbj4F5uZpZpeo5sAELIz0rAPTxAEQXjwhJ9NRKfT4+BhiZ2rXCm8KFXcZuBApMJqvpn5mfwTLlcbH9GwgqniqRFwpnCr34O8il2k6yugNIGIgxB2oLajqVGDfAcxouEI9OiZHjydpJykSo9lGzQEz08/RTIxIfPfndx8cVKJbL3KMjFTEfRya6zsTUmNy+avL8+i1dTNwnQPEqOdZG/cuJFVq1YxaNAgbGxssLGxYfDgwaxcuZJff/21tsMThLpHkiDoI7D3hbRI+GMylKPAx6gmo+hWrxt52jxmBM9Ao5U/gbXq1g27xx8HIObtt9FlZRn6+LRyoklnN9DDzrUX0Ii0cUEQhAfSf1PFdTk5ZOyQC2remiq+PWw7OQU5+Nj4EOAcULEHOfCx/AGy30Pg1aE6wr6/2bhDG7kwGMFLazeWe2B6x+n42/qTmJPI2/vfRqev/ATWuncvvL7+CsnCgqyDB4mYMBFtBc5OLo2lnSlBk1tjYqYk+moqO7+7iF5XPUXWhNphtJPs7OxsQwXwW7m4uIh0cUGoKWY28v5spYlcgfXo13ftIkkS87rOw87UjovJF/ks5DPDfS5vvYXawwNNVBRxS0v+Iu/+eEMsbU1Ii8/hyB8ibVwQBOFBk59bQMSFZAD8CyfZmXv2oMvORl2vHuZtAgxtf7v6GyCvYlfoTOH0GDj5vXzd481qibtO6PYqSEq4vguiTtR2NDXKXGXOkp5LDNvbvj3/bZXGs+zcGe/Vq1DY2JBz6hThzzxLQVLlV8iLONazYuALLVEoJK4ei+PIZvHe6H5mtJPsLl26MHv2bHJzcw235eTkMHfuXLp06VKLkQlCHecRAP3lMyb5ZxZEh9y1i7OFs2Hf0+pzqzkRJ//CVlpZ4j5fHiv1xw1kHTpk6GNqoeahMXLa+OldkURfS622pyAIgiAYv4jzyWg1OmyczXGsZwVA2tatANgMGWKYTF9LucaZxDOoJBUP+z9csQc5tAK0eeDVGby7VWv89zV7H2j1hHxdx/dmAzS0b8j0jtMB+PTkp5xOOF2l8cwDAvD+/juUjo7kXbxI+JixaGIrflTYf3k1daDnaLny/Ym/wrlwILrKYwq1w2gn2R999BEHDhzA09OTPn360KdPH7y8vDh48CAff/xxbYcnCHVbx+ehSRBo8+HXcZB791Sovt59GdZgGHr0zAyeSUZ+BgCWXbpgN+pJAKLffhttZqahj09LJ5p0dQc97Fp7UaSNC4IgPEBuhMip4v4BzkiShDYtjay9+wCwCRpiaLfpmlzwrIdnD5zMncr/AFmJcHy1fN3jzTp7XFWlBU4FJLi8FeLO13Y0NW5kw5EM9BlIgb6At/a+RXJOMsfjjnM6/zTH446j1VXsPYhZ48Z4r/selbs7+aGhhD81mvzw8CrH2aybB+0H+wCwZ/1lIi5UfZVcuPeMdpLdsmVLrl69ysKFCwkICCAgIIBFixZx9epVmjdvXtvh1WlanZ4jocmcSJQ4EpqMVuwJefBIEjyyAmzrQ/IN2DKlXPuzp3ecjqeVJ9FZ0Sw8stBwu+sbb6CuV4+C6BjiP1hSok/3RxtgaWdKWkIOh3+/Xt3PRBAEQTBCWo2OsLOJQPF+7IwdO9BrNJg2aoRZo0YAaLQatlyXC6FVuODZ4c9Bkw3uAdCgT7XFXmc4NYTmw+Tr4Lq/mi1JErO7zDa8T+m/sT/P73yeX7J/4fmdzzNg4wD+Df+3QmOa+vris34dJt7eaKKjCRszhtwrV6oca8eHfWnYwRW9Ts/2r8+RFJV5906CUTHKSbZGo8Hf35/w8HCee+45li1bxrJly5g4cSLm5ua1HV6dtv1cDN0X72LM6uN8d1XJmNXH6b54F9vPxdR2aMK9Zm4Pj66S92yd2wgnv7trF0u1JQsDF6KQFGy+sZntYfI5pwpLS9znzwcg9eefydxfXM3U1EJNr7Fy2viZ3TeJvnr3quaCIAjC/S3yUjKaXC2Wtia4+tgAkLalMFX8loJnuyN3k5KXgrO5M93qVSDdOycVjq6Ur8UqdukCC0/sObdJPku8jrMyseLxRnJR1jxtXon74rPjmbpnaoUn2moPD7zXr8O0cWO0CYlEjH2anLNnqxSnJEn0ebopHg3t0ORq2bLiNFmpeXfvKBgNo5xkq9XqEnuxhXtj+7kYJq07SUxayX/72LRcJq07KSbaDyKvjtDnXfn6r7cg7sJduwS4BDCx5UQA5h2aR1xWHACWnTth/9RTAMS88w7ajAxDH+/mjjTt5l5YbfwimjyRNi4IglCX3TBUFXdBUkho4uLJPiIf92gzeLChXVGq+CMNHkGlUJX/AY6uhLx0cGkGjQffvf2Dyq0lNBoE6OWzxOs4rU7L+kvr73ifHjljb/HRxRVOHVc5OeH93VrMW7dGm5ZGxDPPknXkaJViVaoVDHqxJXauFmSm5LHls9Pk5xZUaUzh3jHKSTbAyy+/zOLFiykoEP+Z7gWtTs/czRe4U0Jw0W1zN18QqeMPoq7/gwZ9oSAXfnkW8rPu2uXF1i/S3LE56fnpzDowy3BchsvrU1F7eVEQE0P8Bx+U6NPt0YZY2ZuSnpjLIZE2LgiCUGfptDpCT5dMFU//axvo9Zi3bYuJZz0AYrNiORh1EIDhDYaX/wHyMuFw4UkXga+Dwmjf7hqHHoVnh5/ZIJ8pXoedjD9JXHZcqffr0RObHcvJ+JMVHltpa0v91auw6NwZXXY2kc8/T+bevVUJFzNLNUGTW2NurSYxMpN/vjmPTivO0L4fGO1PnWPHjrFp0ybq16/PgAEDGDFiRIkvoXodDU2+bQX7VnogJi2Xl9adYO3BMPZdSeBmSjY6Memu+xQKGP4VWLtD4mXY9tZdu6gVahYGLsRMacbhmMOsvyh/aiynjRdWG//lVzKDgw19TM1VhrTxs7tvEnVFpI0LgiDURdFXU8nN0mBmqcajgS0A6UWp4kOKV51/v/Y7evS0d21PfZv65X+A46shJwUc/KF5BSbnDyrP9uDbUz5L/EDdLi6ckJ1Qre3+S2FpiddXX2LVqxf6vDwiX55M+rZtlRqriK2zOUNeao1KrSD8XBL7frqKvhx1coTaZbSTbDs7O0aOHMmAAQPw8PDA1ta2xJdQveIzypee//eFOGb/eZ6nVx+l++LdNJu9nYEf7ePl9SdZ9s9lNp28SUhkKum5mhqOWLinLJ1g5DcgKSBkHZz+6a5dfG19ebODfCbpRyc+4mrKVXmojh2xHzsWgJhZ76BNL65cXr+ZI826ewCw67uLIi1KEAShDipKFfcNcEKhVJAXGkruuXOgVGIzcCAAOr2O36/9DlSw4JkmBw5+Kl8HTgWFsjpDr7uKzhA/+T1kVP0oKmPlbOFcre3uRGFqiucnH8u1BQoKiHr9DVJ//bXS4wG4+trQb3xzkOD8vihO7ajbGQd1QQU2t9wbOp2OJUuWcOXKFfLz8+nduzdz5swRBc9qmIu1WbnaPdzanVyNjtDELMKTssjV6LgUm8Gl2Izb2jpZmeLnbImfk2Xhn1b4OVvi5WCBWmm0n+8IpfHpDj2nw54FsOU1qNcOnBqU2eWxRo+x9+Ze9t3cx/Tg6fw45EdMlCa4vDaFzH170YRHELdoMR4L5hv6dBvZgIgLSaQn5nL4t+v0GNW4pp+ZIAiCcI/odXrD0V1+AYWp4lvllT7Lrl1ROToCcDT2KFGZUViprejr3bf8D3Dye8iKB1uv4nOghbvz6Q5enSDyiPwhxYD5d+9zH2rr0hZXC1fis+MNe7D/y0JlQSunVlV6HEmtxmPxIhSWlqT+9JO8qJCZieOzz1Z6TL82znR/tCH7f7nKoU3XsXE0p0E7lyrFKdQco5tkz58/nzlz5tC3b1/Mzc355JNPSEhIYPXq1bUdWp3W0dcBd1szYtNy7/gjRwLcbM346Ik2KBVyhc4CrY6bKTncSMzkRkIW1xOyCC28js/IIzFT/joamlxiLJVCor6jReHk26r4T2dLHC1NkEQFUOPV4w0IC5a/fnkWJv4L6tI/oJEkibld5zLyz5FcSbnCp6c+5fX2r6OwsMBjwQLCx4wlbdMmbAb0x6pnTwBMzFX0HtuUPz8O4ezeKPzauuDZ2P4ePUFBEAShJsWFpZOVlo+JmRKvJg7o9XrSt8hHdNneejb2Vbng2WDfwZiryrnQUpAPBz6Sr7tPAaW6GiOv4yRJXs1e/6icbt99Klg61nZU1U6pUDK943Sm7pmKhHTHiXZ2QTYv73qZpT2WYmdmV+nHkpRK3ObMRmFlSfKq1cQvWowuMwunl1+q9HvdVr09SUvM4ezum/y75gKWdqa4+4sMX2NkdJPs7777js8//5wXXngBgH///ZchQ4bwzTffoBCFK2qMUiEx++FmTFp3EglK/Mgp+jEw++Fmhgk2gEqpwMfJEh8nS3o3KTleRq6G0MQsbiRkcSMhkxuF16GJWeRotIW3Z8HF+BL9bMxU+Dpb4V+0+l04+fZxtMRMLVK+ap1CKaeNf9EN4s7CP2/DkLLP1nQyd2JOlzn8b/f/WHt+LYH1Auno3hGLdu1wePppkteuJeadd/Hb/CfKwq0gXk0daB7owfngaHZ9d5En3+mIiZnR/bgSBEEQKuh6Yaq4d0snlGoFOefOkx8WhmRqilUfecU6LS+NneE7gQqmip/+EdKjwMoNAsZUe+x1XoO+4N4aYk7DkS+g96zajqhG9PXuy/KHlrPo6KISRdDcLNwY5DuIDZc3cCTmCKO2juKT3p/Q0L5hpR9LkiRc3ngDpbU1CR99TOKKFegyMnCZPq1SE21Jkuj+WEMyknIJO5PIts/PMHJaO+xcLCodo1AzjO5da0REBINvObqhb9++SJJEdHQ0np6etRhZ3TewhTtfjGnL3M0XShRBc7M1Y/bDzRjYwr3cY1mbqWnlaUcrT7sSt+t0emLTc+VJduGqtzwBzyQqNYf03AJOR6ZyOjK1RD9JAg9bc/ycLfEvnHgXpZ+72ZihUIjV73vG2g1GfAXrRsKxb8AnEJoPK7NLr/q9GNlwJBuvbuTtA2+zcehGbExscJ7yKpl79pAfHk7cwkV4LFpo6NN1ZAMizieTkZTLoU3X6fmUSBsXBEG4n+n1em6ckj9c9y+qKl64im3VuxdKK0sAtt7YSr4un0b2jWjm2Kx8g2sLio+g6va/MrOshFJIEgS+AT+PhSNfQ9dXwKxurpL29e5LL69eHI0+yo5DO+jXpR8dPTqiVCgJ8g/if7v+x83Mm4zeNpoF3RdUbMvCf0iShNOLL6KwtCJu/nyS165Fl52F25w5SMqKLyApFBL9JzTnt2UnSYjIYMunpxk5rR3mViaVjlGofkY3yS4oKMDMrOQPRrVajUYjCmndCwNbuNOvmRuHrsXzT/AR+gd2oksDlxIr2FWhUEh42JnjYWdO94ZOJe7L1WgJSype8b6ekGlYCU/PLSAqNYeo1ByCryaW6GeuVuJTuPLtf0vqua+TJdZmIlWsRjToC91fk9/Q/PmK/Mm3g2+ZXd7q8BbHYo8RkRHB/MPzWdxjMQpzc9wXLiR89GjSfv8d6/79se7dCwATMxW9nm7Cnx+FcG5fFH5tnfFq4nAvnp0gCIJQAxJvZpKemItKraB+c0f0Wq2h8rJtUJCh3W/XfgPkVexyr/ad3wQpoWDhCO2ere7QHxxNgsC5CSRcks8aLzreqw5SKpS0d21PvEk87V3boywsktfIvhEbhmzgjb1vcCT2CK/teY1JrSfxYusXUUiVz6p1GDsGhaUlMbNmkfrLr+iysvBYtAjJpOKTY7WpkiEvt2Lj4hOkJeTw1xdnGTolAJXI+jQaRjfJ1uv1PPvss5iamhpuy83N5cUXX8TS0tJw26ZNm2ojvAeCUiHRydeBpIt6Ovk6VNsE+27M1EqauNnQxM2mxO16vZ6krPzCyXfx/u8biZlEJGWTo9FyMSadizHpt43pbG1q2PPtXzjx9nO2wsveHJUovlY1vd6G8INykZRfx8P4v0FV+i8KC7UFCwIX8Mxfz7AtdBs9PXsy2G8wFm3b4PDssySvWUPs7NlYtG2D0s4OAK8mDrToUY9z+6LY/d0lnnxXpI0LgiDcr4qqitdv7ojaVEnWkaMUxMejsLHBMjAQgAtJF7iUfAm1Qs0Q3yFlDVdMp4N9S+Xrzi+BiWXZ7YXSKRTy2eKbnoPDn0PnSQ/kv6edmR1f9vuSZceXse7iOr44/QWXky+zIHABlurK/3vYjRiOwtKSqDfeIH3bX2izsvD8+GMUZhXPvLC0NWXI5FZsWnKSmOtp7Fx7kf7jmyOJ7E6jYHTvVp955pnbbhszRuyreZBJkoSTlSlOVqZ09C25kqnR6ohMzi7e/52YKU/AE7JIzMwjIUP+OvKf4mtqpUR9BwvDqvetRdgcRPG18lGqYeQq+LI7RJ+EnXPvWo20tXNrnm/1PF+c/oL3D79PG5c2uFu54/zq/+S08dBQYhcsoN4HHxj6dBnhT/j5JDKScjm48RoPjW5SxiMIgiAIxspQVfw/qeI2A/qjKFzNKyp41rt+7/IXnbq0GRIvg6ktdHyueoN+EDUfAbvnQ0oYnPgWurxc2xHVCpVCxbSO02js0Jj3Dr3HrshdjNk2hk96fYKXjVelx7UZ0B+FxWfcfOV/ZO3dR+Rzz+P5xecorawqPJajhxWDXmjB5k9Pc+14PDaOZnQZXvbJL8K9YXST7DVr1tR2CMJ9RK1UFE6UrejTtOR96bkaQm/d+50gp6CHFR49dr1wRfy/bM3VJfZ8F03AvR0tRPG1/7LzgmFfwIZRcGiFvD+78cAyuzzX6jkORB3gTOIZZh2Yxcr+K1GYmeGxcAFhT40m/c/N2AwYgHWfPoCcNt776ab88eEpzgdH49/GBa9mIm1cEAThfpISm0VydBYKhYRPS0d0+fmk//MPADZD5FTx3IJctoXK6eMjGpSz4JleD/uWyNedXqize4jvKaVKri6++X/ycV7tJzzQe9yHNRiGn60fU3ZP4VrqNZ7c+iRLei6hq0fXSo9pFRhI/W9WEvniJLKPHSNi3Hi8vv4KlX3FT1PxbOJAr7FN2PntRU7+HYG1ozktetSrdGxC9TC6SbYgVBcbMzWtvexo7WVX4nadTk9Meq5c9fw/1c+jUnNIy9FwKiKVUxGpJfpJEnjam+PrJK94+99S/dzNxuzBXf1uMlhOzzv8Ofz+Irx4AGxL/+GuVqhZELiAxzY/xtHYo3x/4Xueaf4M5gEBOI4fR9I3q4iZPQfztm0Nv2w8G9vTsmc9zu6NYtf3Fxn1bidMzMWPL0EQhPtF0Sq2ZxN7TC3UZOzahS4tDZWLCxYd2gOwM2InGfkZuFu609mjc/kGvvoPxJ4FtaWc2ixUj9ajYO9iuVp7yHroMKG2I6pVrZxbsSFoA6/tfo0ziWeY9O8kprabytPNnq70+z+L9u2p/+23RE6cSO7Zs0Q8/TReq1ahdqn42ddNOruTnpjLsS2h7NtwBWsHM7xb1L0j2O4n4l2q8MBRKCTq2ZlTz86cwIbOJe7L1WhvP3qssPp5Rm4Bkck5RCbnsO9KQol+5mpl4X7v4v3ffk5W+DpbYmVatW8zrU7PkdBkTiRKOIYmV2shumrTdy5EHILoU7BxAjyzRf4kvBTeNt681eEt5h6ay8cnP6aze2caOzTG6ZVXyNi9h/zr14mbv4B6S5cY+nQeLqeNpyfmcmDjNXqNEWnjgiAI94ui/di3pYoPHmyosPzbVbng2bAGw8pXYOrWVewOE8BCZDlVG5UJdHsV/npLPnu87dMP/LnjLhYurB64mvcPv8/v135n6fGlXE6+zLtd3sVMVbmVfvMWzfFe9z0R4yeQd/Ua4WPGUn/1akw8K74S3WGID+mJOVw+HMvfK88x/I22OHtZVyouoerEJFsQbmGmVtLU3Yam7rcXX0vMzL9l1TvTMBkPT5aLr12ISefCHYqvuVibFp/57VR8BFk9u7sXX9t+LuaWI9WUfHf1OO6VOFKtxqlM4NHV8FVPebK9ZyH0eafMLiMbjmTvzb3sidzD9ODpbAjagKmpqZw2/uQo0rdswXpAf2z69QOK08Z/X36KC/uj8W/rTP1m4lNaQRAEY5eelEN8eAZI4NvaGW1mFhm7dgNgM0QubhaZEcmR2CNISAxrMKx8A4fug5vHQGkKXSbXUPQPsLZPyx9ipEbA2V8g4KnajqjWmSpNea/rezRxaMKSY0vYfGMzN9Ju8FGvj3CzdKvcmA0a4L1+HRHjxqOJiCB89Gjqr1mNqZ9fhcaRJIleY5qQmZJL1OVUtq44zaPT22Nl/+Cm+tcmMckWhHKQJAlna1OcrU3p5FdyYqfR6ohIzjasft9ahC0xM5/4jDziM/I4fKNk8TUTpYL6jhbFRddu2f/tYGnC9nMxTFp3Ev1/YolNy2XSupN8MaatcU20Hfzg4Y/h13EQvAx8uoF/71KbS5LEnC5zGJEwgmup1/j45Me81eEtzFu1wnHCBJJWriR2zlws2rc3pI3Xa2RPy16enN19k93fX+LJdzthKtLGBUEQjFpoiHz0pkcDOyxsTEj7czv63FxMvL0xa9EcgN+v/Q5AZ/fOeFh5lG/golXsds+AtWt1hy2ozeUPL/6dDcHLodUToBC1aSRJYnTT0TSwa8Dre1/nfNJ5ntzyJB/1+ogAl4BKjWni5SVPtMdPIP/6dXlF+5uVmDUr5znxhZQqBYNeaMnGJSdJicliy4ozjHijrdhiVwvEv7ggVJFaqcDf2Qp/Zyug5C/5tByNYe93aGJxEbbQxCzyCnRci8/kWnwmEFein625iux87W0TbAA9IAFzN1+gXzM340odbzFCXlk4sQY2PS/vzy7jjY+juSPzus3j5Z0v8/2F7wmsF0gXjy44vTKZzD27ybt6jbh586i3fLmhT5dh/oSfSyI9IYcDv16l99impY4vCIIg1L7rp+KB4lTxtKJU8aAgJElCq9Pyx7U/APls7HKJOAxhwaBQQ9f/VX/QgqzDBNj/ISRdhQt/yL/nBQA6uXdiw5ANvLr7Va6kXGHc3+OY1WkWIxuNrNR4aldXvNd9T+TE58g9f57wZ57F66svsWjbtkLjmFqoCXq5Fb9+cIKkqEz+XnmOwS+3QimOrr2nxL+2INQgW3M1berbM7KdJ28MaMzno9uxfUoPLr43kOC3erF2fEfmPNyMp7t4072BEx62ckpPWk4BGu2dptgyPRCTlsvR/xxNZhQGLgSX5pCVAJsmgk5bZvMenj14vNHjAMw6MIu0vDQUJia4L1gISiXp2/4iffvfhvZqUyV9nm4KElw8EEP4+aQafTqCIAhC5WWn5xNzPQ0AvwBnCpKTyTpwEChOFT8YfZC47DhsTW3pXb/0DKgSis7FDhgln3Qh1AxT6+KCcsHL5H3wgoGntSffD/qeft79KNAVMOfQHOYfno9Gp6nUeCp7e+p/uwbzdu3QZWQQMWEimQcOVHgcGydzgl5uhcpEQcSFZPb9cBm9eO3uKTHJFoRaoFBIeDlY0LORM8928+W9R1qwbmInDs7ow4X3BvDmgMblGic+I7eGI60EtTk89i2oLeRV7eDld+3yevvX8bHxIT47nnmH56HX6zFv2QLH5yYCEPveexQkF3+g4NHQjla9PAHY/f0l8rIr98tMEARBqFk3QhJADy7e1lg7mJG+fTtotZg1b46pny8Av12TC54F+QVhojS5+6DRp+DaDpAU0P21mgxfAOj4PJhYQdw5uLK9tqMxOhZqC5b1XMbkALkuwIbLG3j+n+dJzq3cQojS2pr636zEMjAQfU4ON1+cRPqOHRUex8Xbhv4TmiNJcOFADCf/Dq9UPELliEm2IBgZCxMVbeuX75xEF2sjLWbh3AiGFE6u9yyAsLI/hbVQW7AwcCEqScXfYX+z5YacSuj00kuYNmyINjmZ2PfmlejTeZg/ts7mZKXmsf/XazXyNARBEISqKTq6y7+tfCxR+patgJwqDpCcm8zuSLkI2vAGw8s3aPAy+c+Wj8n1QISaZeEAHeQPvdm3VKxm34EkSbzQ+gU+6fUJlmpLjscdZ9SWUVxKvlSp8RTm5nh9tgLr/v3RazRETXmNtD/+qPA4vq2d6f54IwAO/36DK8diKxWPUHFiki0IRqijrwPutmaUtdvaXK2kTX27exVSxQWMgtZPgV4nH+uVVXZadwunFrzY+kUAFhxZQFRmlJw2vkhOG8/Yvp30v/4ytFebKOnzjJw2fulgDGFnE2v06QiCIAgVk5ulIepSCiCnimuiosg5eRIkCZvBgwDYfH0zBboCmjs2p7FDObK44i/Cxc2ABN2n1mD0QgldXgaVGUQdhxt7ajsao9Wrfi/WD15Pfev6RGdFM3bbWLaHVm71XzIxod7yZdgOHw5aLdHTppP8ww8VHqdVL09a95G3VOxce5Hoq6mVikeoGDHJFgQjpFRIzH5YrihZ2kQ7R6Plue+Ok55rxKnSg5eAUyPIiIHfJ4FOV2bzCS0nEOAcQKYmk5nBM9HqtJg3b47TC88DEDv3PQoSiyfT7g3saN1b/sWxZ90lcrOM+N9CEAThARN2NhGdTo+DhyV2rhakbdsGgEXHjqhdXdHr9Yazsctd8KxoFbvpw+DSpCbCFu7EygXaPStfF70Gwh352/nzw5Af6ObRjVxtLm/ue5OPT36M9i41au5EUqlwn/8+9mPHAhD33jwSv15Z4XG6jmyAX4AzugI92748Q0psVoXHECpGTLIFwUgNbOHOF2Pa4mZbMiXc3daMl3v5Y2GiJPhqIo9/eYjo1JxaivIuTK3g0TXyp99X/4bDn5XZXKVQsSBwARYqC07Gn+Tb898C4PTii5g2bow2NZXYuXNLFO/o9Igfdq4WZKXlc+CXqzX5bARBEIQKuHGqMFW8sKp4caq4XPDsTOIZrqddx0xpxiDfQXcfMOk6nNsoX/d4o/oDFsrW9X9yNfewYLm6u1AqW1NbPuvzGeOajwPgm7Pf8L/d/yMjP6PCY0kKBa4zZ+A4Sc72S1i+nPhlyytUyEyhkOg7vhkuPjbkZRWwZcVpcjLyKxyLUH5iki0IRmxgC3f2T+vNuvHtebqhlnXj27N/Wm/eHNCEn1/ogou1KZdiMxj22QHORaXVdrh35tZCrjgO8O8cuHm8zOZe1l5M7zgdgBUhK7iYdBHJxASPRQtBpSJjx7+kb91maK82UdK7sNr4pcOxhJ0RaeOCIAi1LT+3gIgLcuEnvzYu5F65Qt7ly6BWY9O/P4BhFbufdz+sTazvPuj+5fIWpIYDwL11jcUulMK2HgQ8JV8XVXcXSqVUKJnafioLAxdiqjRl3819PLX1KULTQis8liRJuLz6Ki5vyh8uJa1cSdy8eejvkiF4K7WJkiEvtcLGyYz0xFy2fn6GgvyKr64L5SMm2YJg5JQKiU6+DrRz0tPJ18FwLnaLerb89nI3GrtaE5+Rx+NfHWLXpbi7jFZL2o2D5sNBVwC/jIOclDKbD2swjD71+1CgK2B68HRyC3Ixa9oUpxflT3Hj5s2jICHB0N7d35aAwv1Gu9eLtHFBEITaFnE+Ga1Gh42zOY71LA0fjlr16IHS1pZsTTZ/hcp1NoY3LEfBs9QIOL1Bvhar2LWn+xS5qvu1HXKVd+GugvyCWDtoLa4WroSlh/HU1qfYd3NfpcZynDABtzlzQJJI+eFHYmbMQF9QUO7+FjYmBE1ujamFirjQdP5dcwG9ThSyqwliki0I97F6dub8MqkL3Rs4kZ2vZeLa43x/2AiPaJAkePhjsPeBtAj485Uyq5NKksTsLrNxMnfiRtoNPjzxIQBOLzyPadOmaNPSiJnzn7TxoXLaeHZaPvt/FmnjgiAItenGqXgA/AOKUsXlUyNsC1PF/w77m+yCbOpb16e9a/u7D3jgE/mDWt+e4NWxZoIW7s7BD1o8Kl+Lvdnl1tyxORuCNtDGpQ2Zmkwm75zMN2e/qdTZ1fZPPoHHkiWgVJL2x5/cnDIFXX75U7/t3SwZ9GJLFEqJ66cSOPjb9QrHINydmGQLwn3OxkzNmnEdeLy9Jzo9vPP7ORZsu4jO2D6ZNLOV92cr1HJl2GPflNnc3syeed3kY7t+uPQDB6IOIKnVctq4Wk3mzp2GN20AqsJq45IEl4/EEno6obShBUEQhBpUoNESdlY+UcKvrTM5ISFooqJQWFhg9dBDQPHZ2MMbDkeSyjpLA8iIhZPfydc93qypsIXyCnxd/vPiZrnau1AuTuZOrOq/iscaPYYePR+f/Ji39r1FTkHF6+rYBg3B89NPkUxMyPx3JzdfnIQuO7vc/es1spe32gEhOyI4u+dmhWMQyiYm2YJQB6iVChaPbMUb/eWzEL/ed4OXfzhJrsbI9trUawv9C8+7/nsmxJwus3n3et0Z1WQUALMOzCIlNwWzxo1xfmkSALHvz0cTH29o7+ZnS0Df+gDsWX9ZpI0LgiDUgpuXUtDkabG0M8XV28ZQ8Myqbx8U5ubcSLvBqfhTKCQFQ/2H3n3Ag5+CNg+8OoFP9xqOXrgrlyZydXeA4OW1G8t9Rq1U826Xd3mn8zuoJBXbw7bzzF/PEJ0ZXeGxrHv3wuvrr5AsLMg6eJCICRPRpqeXu3/jTm50GiqfMx/80xVR06aaiUm2INQRkiQxuXdDPn4yABOlgr/OxTJq5WGSMvNqO7SSOr0IjQeDNh9+eRbyyq60+Vq71/C19SUxJ5H3Dr2HXq/HceJEzJo1Q5eWRuzsOSXSrToO9cXezYLs9HyCf7pSw09GEARB+K/rhVXF/QKcQacl/S9577VtUBAAv1/9HYDAeoG4WLiUPVhWEhxfLV/3eFPefiTUvsDCffHnfpWrvgsV8njjx1nZfyUOZg5cTL7Ik1ue5Hhs2YVh78Syc2e8V69CYWNDzqlThD/zLAVJSeXu326QN027uqPXw9/fnCM+vPyTdKFsYpItCHXMIwH1+H5CR2zN1ZyKSGX45we5kZBZ22EVkyR45DOw8YTkG7DltTL3Z5urzFkUuAiVpOLfiH/54/ofSGo17kVp47t3k/bHH4b2KrWSPs80Q5LgytE4boSItHFBEIR7RafVGbbr+LdxJuvQYbTJySjt7bHs0gWNTsMf1+Wf2eUqeHb4c9Bky9XEG/StydCFivAIgAb95GrvBz6q7WjuS+3d2rNhyAaaOjQlJS+F5/55jp8u/VThfdrmAQF4f/8dSkdH8i5eJHzMWDSxseXqK0kSPUc3xrOJPQX5OrZ+doaM5NzKPB3hP8QkWxDqoE5+jmx6qSteDuZEJGcz4ouDHA1Nru2wilk4wKOrQVLC2V/g1LoymzdzbMbLbV4GYOGRhURmRGLWqBHOL8u3xS1YiCauuLK6q68NbfoXpo3/cJncTJE2LgiCcC9EX00lL6sAMys17g1sDbUzbAYNRFKr2XdzH8m5yTiYOdDDs0fZg+WkwtGv5Wuxim18ivbHh/wIaWJPb2W4W7mzdtBaBvkMokBfwPtH3mfuoblotBV732LWuDHe675H5e5Ofmgo4U+NJj+8fIVwlUoFA19oiYOHJdnp+WxZcZq8nPJXLBfu7IGYZH/22Wf4+PhgZmZGp06dOHr0aKltz58/z8iRI/Hx8UGSJD766KMqjykItcHf2YrfXupGgJcdqdkaxnxzhD9Como7rGL1O0Gfd+TrbW/etXjKuObjaOvSluyCbN7e/zYFugIcJ07ArEULdOnpxLz7bolPfzsE+WLvbklOej77RNq4IAjCPVGUKu7b2gk0+WT8+y8ANoWp4kVnYw/1H4paoS57sGMrIS8dnJtC4yE1F7RQOfU7gU8g6DRy9XehUsxV5izusZjX2r2GhMTGqxuZ8M8EEnMqtkfa1NcXn/XrMPH2RhMdTdiYMeReKd/7H1NzFUGTW2Nha0JydBbbvzqLtqD8Z3ALt6vzk+yffvqJqVOnMnv2bE6ePEnr1q0ZMGAA8bcUS7pVdnY2fn5+LFq0CDc3t2oZUxBqi5OVKT8+15mBzd3I1+p4dUMIn+2+VqkjI2pE11fBvw8U5Mj7s/NLr4ypVChZELgAS7Ulp+JPsfrcaiSVCo9FC5HUarL27iPtt98N7eW08aZIComrx+K4fkp8fwqCINQkvU5v2KLj38aFzD170WVlofbwwDwggPjseIKjgoFypIrnZcKhz+XrHm+Aos6/Zb0/FZ1ZfnItZIrfs5UlSRLjW4znsz6fYa225lT8KZ7c8iTnE89XaBy1hwfe69dh2rgx2oREIsY+Tc7Zs+Xqa+1gRtDLrVGZKrl5KYU9P1w2nveL96E6/xNr+fLlPPfcc4wbN45mzZrx5ZdfYmFhwerVq+/YvkOHDixZsoQnn3wSU1PTahlTEGqTuYmSz0e35blAXwCW/H2Z6RvPotEawSeUCgUM/wqsXCHhEvz1VpnN61nVY2anmQB8EfIF5xPPY9qgAU7/ewWAuAULSuxDcvUpThvf+8NlcjLLf46kIAiCUDFxYelkp+VjYqbEs7E96VsLU8WHDEFSKPjz+p/o9DoCnAPws/Ure7ATayAnWT6XuXk59m4LtcO3J9RrDwW5cGhFbUdz3wv0DOSHIT/ga+tLXHYcz2x/hs3XN1doDJWTE97frcWsdSu0aWlEPPMsWUfKl3HrXN+aARObI0lw6WAMJ/4Kq8SzEKCOT7Lz8/M5ceIEffsWF8pQKBT07duXQ4cOGc2YglDTFAqJt4c0471HmqOQ4KfjkYz/9hgZuUawV9nKGUZ+A0hw6ns483OZzR/2e5j+3v0p0BcwPXg6OQU5OI4bh1mrVugyM4l5p2TaeMchvjh4WJKToWHfBpE2LgiCUFOun5RXMr1bOkFOJpl79gJyqrher+f3a78DMKLhiLIH0uTIx3YBdJ8KCmVNhSxUlSQV780+tgqyjaj+y33Kx9aH9YPX09OzJ3naPGbun8my48vQ6sp/LKvS1pb6q1Zj0bkzuuxsIp9/nsy9e8v3+C2d6PGkfCTskT9DuXykfEXUhJJUtR1ATUpMTESr1eLq6lridldXVy5dunRPx8zLyyMvr/gopfTCc+w0Gg0ajRFMdP6jKCZjjO1BVF2vx6j29XC1NmHKT6cJvprIo18cZOXYtrjbmlVHmJXn2QVF4Bsog5eg3zKFApeW4Nig1OYz2s/gVPwpwtLDWHJ0CTM6zMBl3ntEPvY4WcHBJP/8MzYjit/E9RzdkN+XhXDteDw+raLxa+NcpXDF94dxEa+HcRGvh3G5V6+HXq837Mf2aeVA6vbt6DUaTBo0QOnny5HoI4Snh2OhsqB3vd5lxqM4vhZlZhx6G08Kmo2AOvR/qU5+f/j2RuXSAin+HNpDn6PrMa22Iyo3Y309zCQzlgUu44szX7Dq/Cq+Pf8tl5Iusaj7ImxMbMo3iKkJbis+JfaNN8jes5fIlyfjunAB1gMH3rVr466upMRnc2bnTXZ9dxEzayUeDe2q9qTKwVhfjyIViatOT7KNycKFC5k7d+5tt//zzz9YWFjUQkTls2PHjtoOQbhFdb0eLzeBry8puRyXycMf7+X5plo8Latl6MrTN6erVVOcMy+StfZxghu9i05hUmrzIYohfMu3/HL1F8yizWisbox93z44b/uLmAULOZyTS4G9naG9la8JGddN2bXuAufDslGaVn2fkfj+MC7i9TAu4vUwLjX9euSnK8hIskRS6DkfcRSv777HEohq4M+5bdv4NetXAJoqmrJnx55Sx5F0BfS9sBgL4IxNb8L+/rdG464tde37w8PiITpwDu3Bz9iR1oACpXlth1Qhxvp6+OLLkxZPsjF7I4djDzNy00hGW47GRXmX8+Vv1b8/bqmp2IScJvataZw+fIT0jh3u2k2vBnM3M3Ji1Wz9/AwuXbJRW92brYbG+npkZ5deO+i/6vQk28nJCaVSSdwtR/sAxMXFlVrUrKbGnDFjBlOnTjX8PT09HS8vL/r374+NTTk/kbqHNBoNO3bsoF+/fqjVd6n+KdS4mng9Hk7NYeL3J7kan8Vnl0z5+IlWPNSoaiu8VZbRDv03D2GXHcFg9WF0AxaV2TzvRB4/Xv6RbdptPDvgWewGDODmzSjyzpyh+d69eHz1JVLhkS/afjp+W3qK5OhszFN86Du+aaXDFN8fxkW8HsZFvB7G5V69Hse2hBFPJN4tnOjdpSlhs2YA0P7VV8l1seX9394HYHLPybR2bl3qOFLIelSnk9FbutBs9AKaqWo506qa1dnvD90A9F9vxyTpGgMdo9B1/V9tR1Qu98PrMZjBDEsZxmv7XiMmK4Zvcr7h/a7v85DnQ+UeQz94MAnvzyf9119x27iRFn5+2D099q79Cvpp2briLHGhGWSfd+CR1wOwsCl9AaSqjP31KMpELo86Pck2MTGhXbt27Ny5k2HDhgGg0+nYuXMnkydPvqdjmpqa3rGQmlqtNsr/REWMPb4HTXW+Ht7Oaja+1I1J605w4FoSL6w7xXuPtGBMZ+9qGb9SHOrD8K9h/UiUx79B6dcTmg0ttfnU9lM5FneMa6nXeP/Y+3zc62PqLVpE6PDh5Bw6RNbvv2P/+OMAqNXQ99nm/LLoODdOJRJ2OpmG7V1LHbs8xPeHcRGvh3ERr4dxqenXI+yMvBe3QTtXcnb8DXo95gEBWPj6suXyz+Rqc/Gz9aOdezvDh5+30RbAoY8BkLq9itrcusbirW117/tDDYFvwO8vojzyOcouk8DEeDM1/8vYX4/mLs3ZELSBN/a+wbHYY0zdN5WXAl7ihVYvoJDKUWJLrcZj3nuobG1IXrWaxCVLICcHp5dfKv37EfnfZchLrfn1gxOkJ+Twz9cXGPZ6W9QmNVsnwVhfj4rEVKcLnwFMnTqVlStXsnbtWi5evMikSZPIyspi3LhxADz99NPMmDHD0D4/P5+QkBBCQkLIz88nKiqKkJAQrl27Vu4xBeF+YWOmZs2zHXmsnSc6Pcz6/RwLt11Ep6vFIxsa9oVuU+TrPyZDSlipTc1UZiwKXIRaoWZ35G42Xd2EqZ8vzlPk/vGLFqOJKj4b3Lm+Ne0Gyh8i7Ntwhex0UW1cEAShqlJis0iJyUKhlPBp6Ujalq3A7Wdjj2g4osw39Jz/DZJvgLkDtBfvqe47LR8Fu/qQnQgnv6vtaOocBzMHvur3FU81eQqAz0M+5/U9r5OtKV8KsyRJuLzxBs5TXgUgccUK4hctvusxXebWJjw8uTWmliriwzPYsep87b5PvE/U+Un2E088wdKlS3n33XcJCAggJCSE7du3GwqXRUREEBMTY2gfHR1NmzZtaNOmDTExMSxdupQ2bdowceLEco8pCPcTE5WCDx5txev95EqSX+27wSs/niJXU/4qltWu9yzw7Ah5afDreCgofTLc2KExr7SRj/BafGwxEekRODw9FvO2bdFlZxM9a1aJXyDtB/vgWM+K3EwN+34UZ0AKgiBUVdHZ2J5N7JHio8g9exaUSmwGDuBKyhXOJZ1DJakI8gsqfRCdDoKXytddXgKT2i4UIlSYUg3dX5OvD3wMBXlltxcqTK1QM6PTDOZ2nYtKoeLfiH8Z89cYIjMiy9VfkiScXnwR15nycajJa9cS++676LVlv+ezc7Vg8KRWKFQSoacTOfjrtTLbCw/AJBtg8uTJhIeHk5eXx5EjR+jUqZPhvj179vDtt98a/u7j44Ner7/ta8+ePeUeUxDuN5Ik8Uqfhnz4RGvUSomtZ2N4auVhkjJr6RekUg2PrgIzW4g6AbveK7P5082epoNbB3IKcpgRPAOtpMd9/vtIZmZkHzpM6k8/FQ+tUtDn2aYoFBLXTyVw7Xh8TT8bQRCEOu36SXmS7RfgTNpWeRXbsksXVE5OhlXsh7wewtHcsfRBLm2BhEtgagsdn6/xmIUaEjAarN0hIxpO/1jb0dRZIxqOYM2ANTiZO3E15Sqjto7icMzhcvd3eHos7gsWgEJB6i+/Ev3mm+jzy87u82hgR99nmgFwelckp3eVb2L/oHogJtmCIJTP8DaefD+hE7bmak5GpDLii4PcSMisnWDs6sMjn8vXBz+FK3+X2lSpUDK/23ys1dacSTzDyrMrMfX1xeW1KQDEfbCE/Js3De2dvaxpN0ikjQuCIFRVelIOCREZSBL4tHIi3ZAqPoR8bT6bb2wGYHjD4aUPotfDviXydafn5Q9YhfuTyhSKip7t/1DeZy/UiACXADYM2UALxxak5aXx4o4XWXdhXbkz9OxGDKfe8uWgVpO+7S8iX3kFXW5umX0adnCl8zA/APb/ctWQxSLcTkyyBUEoobOfIxsndcXLwZzwpGxGfHGQY2HJtRNM0yDo9KJ8/duLkBZValN3K3fe7vw2AF+d/oozCWewHzsW8/bt0GdnE/P2LPS64qMn2g3ywcnLitwsDXt/EGnjgiAIlREakgiAewM7lFHXyQ8NRTI1xbpvX3ZF7iItLw0XCxe6eXQrfZCrOyD2DKgtodOkexS5UGPaPQMWjnJNlXMbazuaOs3V0pVvB33LUP+haPVaFh9bzKwDs8jTli8T0WbgALw+/wzJzIysvfuIfO55tJllL660HeBNs+4eoIcdq84TF1b+itsPEjHJFgThNg1crPjtpW609rIjNVvD6JVH+PN0dO0E0+89cG8NOcmwcWKZn4oP8RvCIJ9BaPVaZgTPIEebi8f8+XLa+JEjpGzYYGirVCno84ycNn4jJIGrx+JKHVcQBEG4s+un5C03fm2cDQXPrHr1QmllZUgVf8T/EZSKUqoR37qK3WE8WJaRUi7cH0wsocvL8nXwMnm/vVBjTJWmvN/tfd7q8BYKScGf1/9k3PZxxGeXbzucVWAg9b9ZicLSkuxjx4gYN56ClJRS20uSRM9Rjajf3IECjY6tn58hPTGnup5OnSEm2YIg3JGTlSkbnuvMgOau5Gt1/O/HU3y+59q9X/FVmcKja8DEGiIOwt7FZTZ/u/PbuFq4EpERwZLjSzDx9sbl9dcBiF+ylPzI4j1ETp7WtB/iA8C+n66QlSaKtAiCIJRXVloeMdfTAPBt5Uh64X5s26AhRGdGcyj6EHCXVPGwYLh5FJSm0KVyx6sKRqjDRHl/feJluLS5tqOp8yRJYmyzsXzZ90tsTGw4m3iWJ7c8yemE0+Xqb9G+PfXXrkVpZ0fu2bNEPP0MmvjSJ+kKpYIBz7XA0dOKnPR8tqw4TW6WprqeTp0gJtmCIJTK3ETJ56PbMaG7LwAfbL/MzN/OotHe40+lHf3h4Y/k631L4MaeUpvamtoyv/t8AH698iu7I3ZjP/opLDp0QJ+TQ8yMmSXSxtsO9MbJy4q8rAKRNi4IglABoacTQQ8uPjYob5yjIC4OhbU1lj168Me1P9Cjp6NbR7ysvUofpGgVu+3TYO12bwIXap6ZLXR6Qb7et1TOWBBqXBePLmwYsoEGdg1IyElg3PZxhoySuzFv0Rzvdd+jcnYm7+pVwseMJf9m6dv0TMxUBL3cCks7U1Jis9n+1Vm0BSJroYiYZAuCUCalQuKdoGbMHdochQQ/Ho1kwtrjZOTe408sWz4K7Z4F9LDxOcgs/RPWTu6deKbZMwDMOTSHpLxk3BfMR7KwIPv4cVLW/2Boq1Qq6PtsMxRK+ViKK0dF2rggCEJ53ChMFfdv42woeGY9oD+oVfx+7XfgLqvYEUcgdB8oVNDt1ZoOV7jXOk+S99nHnpH33Qv3hJeNF+sGr6NP/T5odBrePfgui44uQqO7+/s20wYN8P5hPWpPTzQREYSPGUPejRultreyNyNocivUpkqirqSy+/tLYrGikJhkC4JQLs909eHrse0xVyvZdyWBx748REzaPd6DM3ARuDSDrHjY9FyZ+7z+1/Z/NLRvSHJuMrMPzkbt6YnLG4Vp48uWkR8ebmjrWM+KDoVp48EibVwQBOGucrM0RF1OBcC3uR3pf8snQNgGBXE45jDRWdFYq63pW79v6YMUnYvdehTYlbHaLdyfLByg/Tj5et8SsZp9D1mqLVn+0HJeav0SAOsvrufFHS+Sklv6XusiJl5eeK9fh4m/PwWxsYSPGUvuhQultnfytGbA8y2QFBKXj8RybEtotT2P+5mYZAuCUG59m7ny8wtdcLY25VJsBsM+O8D56LR7F4DaXN6frbaQU8b3Ly+1qYnShEWBi1Ar1Oy7uY9frvyC/ZNPYtGpE/rcXKJnvl0ibbzNAG+c61uTl13AnvUibVwQBKEsYWcT0en0ONazRHXlJLq0NFTOzlh06GBITx3sNxgzldmdB4gOgav/gKSA7q/du8CFe6vrK/J++5tH5f33wj2jkBRMCpjER70+wkJlwdHYo4zaOorLyZfv2lft6or3uu8xa94cbXIy4c88S/bJU6W2927uSM9RjQA4tjWMS4diqu153K/EJFsQhApp6WnLby91pZGrFXHpeTz+5SF2Xy5fBctq4dIEBheufuyeD+EHS23ayL4RU9pOAWDJsSWEZYTjPn8+CgsLck6cIOX77w1tlcrCauMqibAziVw+EluTz0IQBOG+duOUfD6uX4Az6Vu2AGAzeBBpmgx2RuwEYETDEaUPELxM/rPFo3LdDaFusnaDtmPl631LazeWB1Sf+n1YN3gdnlaeRGVGMfavsfwT9s9d+6ns7an/7RrM27VDl5FBxIQJZB44UGr75oH1aDvAG4Dd318i8lItHf9qJMQkWxCECvO0t+CXF7vS1d+RrHwtE9ceZ/2R8Lt3rC4BT0GrJ0Gvg18nQFZSqU3HNBtDJ/dO5GpzmRE8A8nDBZe33gQg/sOPyAstTmtyrGdFxyC5yNv+n6+SlSrSxgVBEP4rP7eAiAvyG2jfJtZk7NoFgE1QEFtDt6LRaWji0IRmjs3uPED8Rbj4p3wd+Pq9CFmoTd1elffdh+6FyGO1Hc0DqaF9QzYEbaCLexdyCnJ4fe/rfHrqU3T6sguVKa2tqf/NSiwDA9Hn5HDzxUlk/Ptvqe07P+JHw/Yu6HR6tn91jqToss/crsvEJFsQhEqxNVfz7biOPNrOE61Oz9u/nWPhXxfR6e5BmrUkwZBl4NgAMqLhj5dK3eulkBS83+19rE2sOZ90nq9Of4XdE09g2bUL+txcYma+jV6rNbRv068+Lt5y2vju9aKAhyAIwn9FnE9Gq9Fh62yO+uJh9Lm5qL3rY9q8OZuubgJgWINhpQ8QXLjVp+nDcnaSULfZ1Zc/GIfiffjCPWdrasvnfT/n6WZPA/D1ma95dferZOaXPRFWmJvj9dkKrPv3R6/RcPPVKaT98ccd20oKid7PNMW9gS35OQVsWXH6ga1zIybZgiBUmolKwZJHWzG1n7wP56u9N3jlx1PkarR36VkNTK3gsW/lvV5XtsPhz0tt6mbpxrud3wVg5dmVnE44jfu8eSgsLck5dYrk74rTxhVKBX2eaYZCJRF+NolLh0TauCAIwq2Kqor7tXEmfaucKm47JIiLKRe5knIFE4UJQX5Bd+6cdB3O/SpfB75xL8IVjEH31+T991e2Q8yZ2o7mgaVSqHizw5vM7z4fE4UJeyL3MHrbaMLTy85GlExMqLd8GbbDh4NWS/S06ST/8MMd26rUSga/2ApbF3Myk/PY+tkZ8nMLauDZGDcxyRYEoUokSeJ/fRry4ROtUSsltp6NYfQ3R0jOyq/5B3drCQMXytc7ZsPNE6U2Heg7kCC/IHR6HTOCZ5DvYofLtLcASPjoI/JuFKeNO3hY0ulhPwD2/3KVzJQH81NYQRCE/yrQaAk7K2/R8fY3I+uAXBfDJmiIoeBZn/p9sDW1vfMA+z+Ut/o07A8eAfciZMEYODWA5oXHuRXtxxdqzVD/oawdtBYXCxdupN1g1NZRHIgqfb81gKRS4T7/fezHynvs496bR+LXK+/Y1sxKTdDk1phZqUmIyGDHqvP3JtPRiIhJtiAI1WJ4G0++G98JGzMVJ8JTGPH5AUITs2r+gduPh2bDQKeBX8dBTmqpTWd2mom7pTs3M2+y+Ohi7B57DMtu3dDn5REzY0aJtPGAvl64+NiQn1PA7nUibVwQBAHg5sUUNHlarOxNMb+wHwoKMGvWDH19D7bd2AaUcTZ2aiSc/lG+7vHmPYpYMBpF++8v/AEJd69wLdSsFk4t+CnoJ1o7tyYjP4OXdr7EmnNryny/IykUuM6cgeOkFwFIWL6c+GXL79jHzsWCIS+1QqlSEHY2if0/XXmg3kuJSbYgCNWmi78jm17qiqe9OWFJ2Yz4/ADHw2q4uqQkwdBPwM4bUsNh8/9K3Z9tbWLNgu4LkJD47dpv7IrYhfv781BYWZFz+jTJ335raKsorDauVCmIOJ/ExYPiOApBEITrIXJVcd+A4lRxm6AgdoTvIEOTQT2renRy73Tnzgc/AV0B+PYAr473KmTBWLg2h8ZDAL2c0SDUOidzJ1YPWM2IhiPQ6XUsP7Gc6cHTyS3ILbWPJEm4vPoqLm/K2z2SVq4kbt68EseiFnHzs6XvOLkA4tm9UZzeGVkzT8QIiUm2IAjVqoGLNb+91I3WnrakZGt46psjbD4dXbMPamYLj60BhVr+hPz4qlKbtndrz7gW4wCYc2gOqbYqXKdPAyDh40/Iu37d0NbB3ZKOQ+Vq4wd+uUpGcum/dARBEOo6rVZH6Gl5ku3tJZFz/ARIEjaDB/HbNTlV/JEGj6CQ7vD2MiMOTqyVr8Uq9oOrR+Fq9pmfITm07LbCPWGiNGFOlznM7DQTlaRiW+g2nv7raWKzyq5J4zhhAm5z5oAkkfLDj3JGYMHte68btHOh64gGABzYeI3rp+7hsa+1SEyyBUGods7Wpmx4vgv9m7mSX6DjlR9P8cWe6zWbJlSvHfSbK19vn1lmYZXJAZNp4tCE1LxU3jn4DjYjRsjHU+TnEz1jZolfEgF96+Pqa0N+rpY9Im1cEIQHWPTVVPKyCjC3VmNxfi8AFh06EGuh4VjsMSQkhvkPu3PnQ5+CNg88O4JP4L0LWjAu9dqBf2/Qa+HAx7UdjVBIkiRGNRnF1/2/xt7UnovJF3liyxOcjDtZZj/7J5/AY8kSUCpJ++NPbk6Zgi7/9po8Af28aNGjHuhhx+oLxIam1dRTMRpiki0IQo0wN1HyxZh2jO8mrwQv3n6Jmb+do0Bb9pmMVdL5JWg0UH4j9+s4yMu4YzO1Us2iwEWYKk05EHWAn678hPu891BYW5N75gxJa9YY2ioUUnHa+IVkLgRHE301lexoFdFXUx+4Qh6CIDy4bpwqTBVv5UTG1q1AYcGzwlXsrh5dcbdyv71jVhIcWy1f93hT3uYjPLiKMhlC1kN6DWe6CRXSwa0DPwb9SGP7xiTnJjPhnwn8cuWXMvvYBg3B89NPkUxMyPx3JzdfnIQuO7tEG0mSCHyiId4tHdFqdGz7/AxpCTk1+VRqnZhkC4JQY5QKiXcfbsach5uhkODHoxFMWHuczLwaOspBkmDYF2BTD5KuwZappe7P9rfz57V2rwGw7PgyIs2ycZ0xA4DETz4l7+pVQ1t7N0s6PSJXG9/zw2W2fHKW5NPmbPnkLN/NPPjApD4JgvDg0uv03Cjcj+3lqiHv0iVQq7Ho24c/rsln5pZa8OzIF6DJArdW0LDfvQpZMFbeXaF+V9Dmw8FPazsa4T/qWdXju0HfMcBnAAW6At479B7zDs1Do9WU2se6dy+8vv4KycKCrIMHiZgwEW16eok2CqWC/hOa4+RlRU6Ghi0rTpObVfqY9zsxyRYEocY9282Xr8a2x1ytZO+VBB778hAxaTX0CaaFA4xcBZISzv4sf1JeilFNRtHVoyt52jymB0/HYugQrHr2RK/R3JY2bu1gdscxslLz2P7VOTHRFgShTosNTSc7LR8TMyVW53cDYBUYyJHs88TnxGNnakcvr163d8xNgyNfy9diFVso0qPwjPTjayAzoXZjEW5jobZgSY8lvNr2VSQkfr7yMxP/mUhSTlKpfSw7d8Z79SoUNjbknDpF+DPPUpBUsr2JmYqgl1tjZW9Kalw2f315Fq2mBjMca5GYZAuCcE/0a+bKTy90xsnKlIsx6Qz77ADno2toT453F+j9tny97U2Iv3THZgpJwbxu87A1teVi8kW+OPMFbu/NRWFjQ+65cyR9IxdQ0+n07P/l6h3HKLL/56sidVwQhDrrRuEHid6tnMgsqio+ZDCbrm4CIMgvCBOlye0dj66EvDRwbgJNgu5ZvIKR8+8NHm2gIAcOf17b0Qh3IEkSE1tOZEWfFViprTgZf5Intz7JhaQLpfYxDwjA+/vvUDo6knfxIuFjxqKJLVlAzdLOlKDJrTExUxJ9NZWd311EXwffP4lJtiAI90wrTzt+e6krDVysiEvP4/EvD7Hncg2tAHd7Dfx6gSZb3p+dn33HZi4WLrzb+V0AVp1dxRlu4jpTThtP+Owzci9fIeZqKlmpeWU+XGZKHjFXU6v1KQiCIBgDvf6WVHG7LDQ3byJZWJDfpRV7I+UCaCMajri9Y34WHPpMvg58AxTibadQSJKK92YfXQk5KbUbj1CqHp49WD9kPT42PsRmxfLMX8+w7ca2UtubNW6M97rvUbm7kx8aSvhTo8kPDy/RxrGeFQNfaIlCIXH1WBxHNt9Ap9PXqZo34qedIAj3lJeDBRsndaWrvyNZ+VomrD3OD0ciqv+BFAoY8TVYuUL8Bdg+vdSm/X3684j/I+jRMzN4JspBfbDq1Qs0GmJmzCAr+c4T9P/KSi97Ii4IgnA/SozMJD0xF5VagfW5nQBY9+nD1uidFOgLaOnUkob2DW/veHwN5CSDvS80L2W/tvDgajQIXJpBfoY80RaMlp+tH+uHrKd7ve7kanOZFjyND098iFanvWN7U19ffNavw8TbG010NGFjxpB75UqJNl5NHeg5ujEAJ/4KZ/UbwXWq5o2YZAuCcM/Zmqv5dlxHRrb1RKvTM/JosBoAAPUZSURBVPO3syz661L1f2pp5SJPtJHg5Fo4+2upTad3nE49q3pEZ0Wz6Ngi3ObOQWFrS+6FCxTs31Guh7O0Ma2mwAVBEIxH0Sp2/WYOZP9dWFV8yGA2XZNTxe9Y8EyTCwc/ka8Dp4JSdU9iFe4jCgUEFp6bffhzyMus3XiEMtmY2LCi9womtJgAwOpzq5m8azLp+el3bK/28MB7/TpMGzdGm5BIxNinyTl7tkSbZt088G/rDEBedsmiuPd7zRsxyRYEoVaYqBQsfawVr/VtBMCXe6/zyoZT5Gru/Klopfk9VJyStvlVSLp+x2ZWJlYs6L4AhaTgz+t/siv7FG6zCvd1f/chllZ3/3EZfj4RbU0eUSYIglALrhce3VXPKhVtUhJKOzuuN7IiNC0Uc5U5g3wG3d7p1PeQGQc2ntDqyXscsXDfaD4cHPzldPHjq2s7GuEulAolU9pNYXHgYsyUZuyP2s9TW5/iRuqNO7ZXOTnh/d1azFq3QpuWRsQzz5J15Kjhfp1OT+yNO0/Si9yvNW/EJFsQhFojSRKv9m3Issdao1ZKbD0Tw5hvjpCclV+9D9RzGnh3g/xM+OVZKLhzWndb17aGT2jfO/QeOb06YNWnD5Imn0Y3t9z1YU79E8mmJSfr/NmPgiA8OFJis0iJyUKhlLA59y8A1oMGsin0TwD6effDysSqZCetBg58LF93nwKqOxREEwQAhVLOdAD5OC+N+P15PxjsN5jvBn2Hu6U74enhPLXtKUN9hv9S2tpSf9VqLDp3RpedTeTzz5O5V25bl2veiEm2IAi1bmQ7T9aO74i1mYrj4SmM+PwAoYlZ1fcAShWM/AbMHSD2DPzzTqlNJ7WeRDPHZqTnp/PuwXdxnf0OSltb7EO20tXrJpZ2JVPCrexNGfhCCwY+3wJTCxXxYen8PP8oV4/FVV/8giAItaRoFduzkS15/8rFjkwG9mV72HaglIJnZ36CtEi5JkabMfcsVuE+1eoJsPWCrHg4ta62oxHKqaljU34c8iNtXdqSpcnilV2vsPLMSvT621edlVaWeH31JVa9eqHPyyPy5cmkb9tW7lo292PNGzHJFgTBKHT1d2LTpK7UszMnLCmbEZ8f4HhYcvU9gI0HDP9Kvj76FVzcfMdmaqWahYELMVOacSjmED8n7sD1XXlSbvbjEh57yoa+/dV42F2ib381Y97rhH8bF/zbuvDErI64+9uSn6vln1Xn2fndRTR51Zz+LgiCcA/dKJxke5glocvKQuXhzl67GHIKcvCx8aGtS9uSHXRaCF4mX3d9BdTm9zhi4b6jVEO3V+Xr/R9BQTVnswk1xtHckW/6f8MTjZ9Aj55PTn3CG3vfIFtze8FYhakpnp98jM2QIVBQQNTrb6A/dbhcj3M/1rwRk2xBEIxGQ1drfnu5K608bUnJ1vDUN0fYcib6/+zdd1gUxxsH8O9eofduAUUEEcSGIMVGrNhLLEk0tqg/u7HEroklaoyJJprYE4011th7V+yIiGIBFKT33u7u/f2x3MIJduAQ5vM8PBx7s8vc7d3uvjsz75TeP3DoAHhN4B//NxZIKTmreR3DOpjSjE/G8uudXxHn6QD99u0BmQzhX34JxayRcDz4OxSzRiK0fXuknToFANA30ULPyU3QrEttcBwQfC0a//54C/ER6aX3GhiGYcpJWkI24sPTwXGAUeBJAIBhly7YH3IQANCzbk9wHKe6UtABICkU0DYGXIeWc42ZT1aTQXzPh7SXfE8I5pMhFUsxx2MO5nnOg0QkwakXp/D18a8RmRFZrCwnlaL6T8tg1K8fQAT5L7OgrSErYauF9Iw1Uc3eqIxqX3ZYkM0wTIVioa+FXSM90N7JEnkyBcbt8MfaiyEldj/6IG3nATXdgJxUYO8wfuxgCfrX648WNVogT5GHmVdmQquVN/+ETPVkIIuNReTESUKgLRKL0LxbHfT4tgl0jTSREpuFvctuI+BcROm9BoZhmHKgzCpezVYf+Zf4WRbSWjVGQHwAxJwYPer2UF1BoQAu/cw/9hgLaL4yVpthXkeqBXiO4x9f+QWQvznwYiqevg59sanDJphomeBx8mMMODIAt2JuFSvHicWw+uF7mAwfBg4Eu7ubARDw6jUSEQBCi372EIm4Ytup6FiQzTBMhaOjIcHaga4Y6l0bALD0eDBmH3wAWWlk7hZLgT6bAC1D4OUt4NzCEotxHIeF3gthrGmMJ4nBCP91ecnbKzgpxP64BCQv7Bpew8EYA+a4o3ZDMyhkhCv/PsWxP+4jO4N1g2MY5tOgDLKrS2NBeXnQtK+L/+APAGhZsyXMtM1UV3h8FIh/BGgaAO4jyru6zKeu2TC+B0RSKPDwoLprw3yAppZNsbvrbjiZOiElNwUjTo3Ajkc7ijUycBwHi6lTYT5pIiwSAtDgwQZo5qaolNHMTUaDoI0wj79Xfi+gFLEgm2GYCkks4jC/mzPmdXUCxwE7boRj+JbbyMgthbvbxrWA7qv5x1dXAU9LngfbTNsM873mo34EQTvpDYnYiCCLiUHW7Tsqi7X0pOg82gWtBjhALBHheWAidi28iZePkz/+NTAMw5ShzNRcRIekAgCMAvkkZ7qdfXEolM9n0avuK3NjEwGXCm5Guo8EtI3Kq6pMZaGpB3iM4R9fXsH3jGA+OVa6VtjSaQu61OkCOcmx5OYSfO/3PfLkqo0MHMfBdMQIiPT1YZEQAK/rc9Hk3ko4PdyMJvdWwuv6PFgkBBRrxPhUsCCbYZgKbVgLW6wb6AotqQgXn8Sj71o/xKTmfPyGnbrzF4IAcGAUkFby2O+2Nm3RQc/9nTYpi48vtozjOLi0qYnPZ7jC2EoHWal5+G+lP67/FwIFm1ObYZgKKiwgASDAvIY2FNfPAwAeNTFFUk4STLVM0bJmS9UVnp0FogMAqU5hoMQw78t9JN8TIu4h8PiYumvDfCAtiRaWtFiCKa5TIOJE2P90P4adHIb4LNXrpKzbd6BI5/PWcCAYpzyFVdwdGKc8BVfQhbykRoxPAQuyGYap8Do4W2H3SE+Y6WngUXQaeq65iodRaR+/4fYLAauGQFYisO+b144B6+Xxbsl7xOZmr33OrKY++s50g5N3NYCAO8df4MCKu0hLYHOCMgxT8YT6xwEAqkuiAYUC2o0aYU/mJQBA97rdIRVJCwsTAZd+4h83GwbompZ3dZnKQtsIcPuGf3z55+LjdJlPBsdxGNJgCP5o+wf0NfQREB+AAUcGIDA+UChTUuNESd61XEXCgmyGYT4JjayNcGCMN+pa6CEmLQd9117DxScfedCVagF9/wY09IAXVwsvEl9h3NwbuaZ6eFu7c9JffyM/6vXZ0KWaYvgMqo8O3zhDQ1uCmNA07F58C8/uxH34a2AYhillOZn5iHycAgAwCjgOAOA6tMaVyCsASugq/vwKEHEDEGvy03YxzMfwHAtItIEofyDkrLprw3wk7xre2NllJ+oY1kFcdhyGnBiCQyGHAAASc/N32sa7lqtIWJDNMMwnw9pEB/v+5wWPOibIzJNj2N+3sPNmydNwvTNTO6DbKv7xxZ+A0IvFiig44J/2GuCAYoG2AgABkIuAzAsXENK1GxL//hske/3Ycftmlug/2w2WtgbIy5bh5IYHOL8tGPl5n96YI4ZhKp/n9xOgUBBMzDUgvncFEIlw3j4XClKgqUVT2Braqq6gHIvddBCgb1X+FWYqF10zoFlBD7JLK9RbF6ZU1DKohe2dt6ONdRvkKfIw+8ps/HTrJ2g0bQSJlRXw6lSAShwHiZUVdJq5lm+FSwELshmG+aQY6kixdVhz9G5SA3IFYeb+QCw7EQyF4iO6lLl8DjT9GgAB+0cAGaot5Hfj7uKUbRpW9BYhSV911SR9YEVvEaYNE0PewB6UlYW4pcvwvF9/ZD8Ieu2/NDDTRq+pTeHaqRbAAQ+vRGHPj7eQ8DLjw18HwzBMKQjxL5i6S8TPc6vj6YF/E/hpCnvZv9KKHXETCLsIiCSA98RyrSdTiXmNB8QaQPg14PlVddeGKQV6GnpY5bMKoxqOAgD88/AfjDk/DvrTCo4brwbaBX9bzpoJTiwuz6qWChZkMwzzydGQiLCiXyNMamcPAPjzQggm7PJHTv5HtAR3WgaY1wcyYoEDI1WymioTddysJ8LYMWJ8/6UIq7qL8P2X/N8364nw0pxD+NKRsFrwA0QGBsh5+BDP+/VD7JIlUGSWnJlcLBbBo6cdekxsDB1DDSTHZGHv0tsIvPCSzanNMIxa5OXIEPEwCQBgdI9PPJXU0hkR6RHQleqiQ60Oqiso58VuNAAwsinPqjKVmUF1oPFX/OPLP6u3LkypEXEijGsyDr+0+QXaEm1cj76OoTnr8GxqLyTrqwbZyQYiJM4ZBoMOHV6ztYqNBdkMw3ySOI7DpHYO+LlvI0hEHI7cj8bAjTeQnPmB81Br6PDjsyXaQMg54OpK4SlzncKxQCTi8LCWCFedRXhYSwQSFZ4UTHXNYdyvH+yOHoFB586AQoGkLVsR0rUb0s+df+2/ruloggFz3FHLxRRymQKXdj3B8bWByMnM/7DXwjAM84FePEiEXKaAgaEYGo9vgtPQwIHqfK6JTrU7QUeqU1g4OgB4ehLgRECLyWqqMVNptZgEcGL+nBz56WWXZl6vfa322NZ5G2ro1cDLjJeYJTmE/43mVBoxRv+PwxjZVpx5cUbd1f0gLMhmGOaT9rlrTWwd5g59LQluv0hG7z+v4XnCG+a0fhMLR6BzwdjCc4uA8OsAgKYWTWGpYwkOrxkzVODX278iID4AEnNz1PhlBazXr4O0Rg3IoqPxcswYvJwwEfmxJSc509bXQJcxDdGirz1EEg5hAQnYvegmop6yObUZhik/ofcKuopzkeAAaLZugWNxFwAAve17qxa+XDBetkEfPr8Fw5Qm49pAw378YzY2u9JxMHbAdt/t0BBpACjeiKEoiFKX3VwGueLTy1lTJYLsNWvWoHbt2tDS0kLz5s1x8+bNN5bfs2cPHB0doaWlBRcXFxw7pjpP35AhQ8BxnMpPp06dyvIlMAzzBl51zbB/tBdqGGkjLCETvf+8hjsvkj5sY00GAi79AJIDe4cDWUkQi8SY4T4DAF4baGuKNRGUFISBxwZi5uWZiM2MhV6rVqhz+BBMhg8DxGKknzqF0C5dkLRjB0he/ITBcRwatbXG5981g5GlDjKSc3HwF3/cPBzK5tRmGKbMyfLleBGYCAAwuncUABDsao5ceS7qGtWFi5lLYeG4YOAhnyEYLaeUd1WZqqLFZAAc8PgoEPv6PCfMpyk0LRR5itf3QCQQYrJicDfubjnWqnRU+iB79+7dmDx5MubPn4+7d++iUaNG6NixI+LiSm5NunbtGr744gsMHz4c/v7+6NmzJ3r27IkHDx6olOvUqROio6OFn507d5bHy2EY5jXsLfVxYKwXGtY0RFJmHr7YcANH70e//4Y4Duj6C2BiB6S9BA6OAYjQrlY7/NLmF1joWKgUt9Kxwq9tfsWJPifQs25PAMCR0CPodrAbNtzfgHxNMSynTYPtvr3QatgQiowMxC5YiOdffomcx49LrIK5jT76zmwGR08rEAG3jj7HwV/9kZ6U8/6vh2EY5h1FPEpGfq4cOrocdJ77Q6Svj22GfGDTq24vcEUTE135BQABjl0Bi/rqqTBT+Zk7AE49+MeXWWt2ZaPMeVNa5SqSSh9k//LLLxgxYgSGDh0KJycnrF27Fjo6Oti8eXOJ5VetWoVOnTph2rRpqF+/PhYuXIimTZti9erVKuU0NTVhZWUl/BgbG5fHy2EY5g0s9LWwa6QH2tW3RJ5MgbE77mLdxZD3TyKmqc+PzxZrAk+OA9f/BAC0q9UOJ/ucxPq269FXpy/Wt12PE31OoF2tdjDTNsNC74XY2WUnGpk3QrYsG7/5/4YeB3vgzIsz0KxXD7V37oDlnDkQ6eoiJ+A+wvp8jrgVK6DIzi5WBQ0tCdoOdkL7YU6QaokR/SwVuxfdRKj/p3eiYRjm0xDqzzdAVFO8BAeCorU77qcFQyKSoJtdt8KCSaFA4B7+caupaqgpU6UoP2MP9gMJz9RbF6ZUFc15UxrlKpJKHWTn5eXhzp07aNeunbBMJBKhXbt28PPzK3EdPz8/lfIA0LFjx2LlL1y4AAsLC9SrVw+jR49GYmJi6b8AhmHem46GBOsGuWKIV20AwJLjwZhz8AFk79vdulpDoONi/vHpeUAk31VJLBKjmWUzNNJohGaWzSAWqU4r0cCsAf7x/QdLWi6BhbYFIjMi8e2FbzHi1Ag8TQuBycCvUOfoEei3bwfIZEjcsBGh3boj40rJU5Q4uFuh/2w3WNTSR26WDMfXBeLijseQsTm1GYYpRXK5AmH3EwAARvf5YXJXnPjLRB9rHxhrFWlMuPIrQAqgbnugepNyrytTxVi5AA6dABD/2WMqjcKcNyXjwPcYbGrRtDyrVSok6q5AWUpISIBcLoelpaXKcktLSwQHB5e4TkxMTInlY2JihL87deqE3r17w9bWFiEhIZg1axZ8fX3h5+cH8WvmccvNzUVubq7wd1paGgAgPz8f+fkVL4Owsk4VsW5VEdsf72+2rwNqGGnix+OPsf1GOF4mZWFl/4bQ03yPw17jwRCHXoQo+DBoz1DIhp8DtAzeaX90tO6IllYt8ffDv7H10VbciLmBvof7ok/dPhjdcDQsf/kFuufOI+HHH5H/8iUivvkGep07w2zaNEjMTFW2pWMkRbdJDXH76AsEnHmJB5ciEfk0GW2HOsKkmu4HvT+VCft+VCxsf1Qs77o/Ih8nIzdTBk1NgkFkAERmptiieRuQAd1tuxeunxYJyb2d4ADIvL8Fsf38Xtj348NwXpMgeXICdH8XZC2mAIbWpbJdtj/Ub5qlD6aF8scUKjIkhSvohTjVsg0UckWFyE3zPp8TjirxZKxRUVGoUaMGrl27Bk9PT2H5d999h4sXL+LGjRvF1tHQ0MCWLVvwxRdfCMv++OMP/PDDD4iNjS3x/4SGhsLOzg5nzpxB27ZtSyzz/fff44cffii2fMeOHdDR0SlhDYZhSsP9JA5bn4qQr+BQQ4cw0lEOI813X18iy0Sbx3Ohm5eASCN33K41GqaZT6CVn4IcqRES9erx09e8QZI8CSdzTiIonx/bqM1po61WW7hpuEGSJ4PZyVMwunYNHBHk2tqI7+yLtGbNAFHx7eYkiJEUoAVFnggQEYzq50LXOh/cmxOfMwzDvFFykCYywzVglvsIDf1W46lnPcxuEwJDzhBTDKZAVHCcc3n5D+rEn0a8Xn1cs5+p5lozVYnX06Uwz3iIMLO2uG89WN3VYUoDKdAhaDKuSLOxzNQYsZLChhArmQzfJSbDO18bp51/eeu1VnnIysrCl19+idTUVBgYGLyxbKVuyTYzM4NYLC4WHMfGxsLKyqrEdaysrN6rPADUqVMHZmZmePbs2WuD7JkzZ2Ly5MI5JNPS0mBtbY0OHTq8dSepQ35+Pk6fPo327dtDKpWquzpVHtsfH64zAN+XqRi1zR+RmXn485kuNgxqCkcr/XfeBhdpB9raGTVSbqJ6zmNwOanCc6RfHfIOP4Icu75xGwMxELdib2H5neV4lvIMR7KP4JHGI0z1nAq3Xn8iJygIcd//gLzgYFjt24/aYc9hMX8eNOrUKbatrLQ8XNj2BC8fJSMlSAvG0hpo9YU9NHWq5meDfT8qFrY/KpZ32R+kIGy7egNAPmqGXQYA3PTk70b2c+6Hrg0Ljm8ZsZCsGQkAMO6xGJ1rtyrz+lc27Pvx4bgXBsC2nqidfAU1v1wF6L/+2vxdsf2hXtyLK5DcS0L7fOCzrGzc1dJEvFgMc7kcTXNywfcPzkaXBkagWi3UXNvCnsjvolIH2RoaGnB1dcXZs2fRs2dPAIBCocDZs2cxbty4Etfx9PTE2bNnMWnSJGHZ6dOnVVrCX/Xy5UskJiaiWrVqry2jqakJTc3izWdSqbRCf6krev2qGrY/PkwzWzMcHOuNIX/dREh8Jr7YeAtrvmqK1g7vmEijdnPApS8QsFMlwAYALj0akn1DgX5bAafub9yMV00v7Km+B/uf7sfv/r8jJDUEo8+Nho+1D6Y1m4Y6e/cgaes/iP/9d+TcvYvwz/vCbMQImI4aCVGR44ehqRTdxzfGvbMRuH4wBGH3EhEfnoEOw5xRra7R+749lQb7flQsbH9ULG/aH9EhqchOy4dUQjCKfQCuZnX8pxEEgENvh96F691aB8hygJpukNT9DKwLzYdj348PYNcGsG4OLuIGpLfWFuZNKQVsf5QzIiDiJnDhR2GRGIBbTm6JxSXZiUAF2D/v8xlRf7t7GZs8eTI2bNiALVu24NGjRxg9ejQyMzMxdOhQAMDXX3+NmTMLuztNnDgRJ06cwIoVKxAcHIzvv/8et2/fFoLyjIwMTJs2DdevX8fz589x9uxZ9OjRA3Xr1kXHjh3V8hoZhnk7axMd7B/tDY86JsjIlWHY37ew62b4u62skANhF1/zZMGImxMz+HJvIRFJ0K9ePxzpdQRf1f8KYk6M8xHn0eO/HlgVsBpag/qhzuHD0G3dCsjPR8IffyCsR09kXlcd3sKJODRpb4M+37nC0FwbGUm5OLDiLm4dDYNCUWlHATEMUwZCCrKKW+SHQ0RyhLrXADgOzas1R039mnyhrCTg1ib+catpLMBmyh/H8Z89ALi9GchkSYc/OXlZwJ0twLqWwOYOwMub77aenuXby1QwlT7I7t+/P37++WfMmzcPjRs3xr1793DixAkhuVl4eDiiowvn0vXy8sKOHTuwfv16NGrUCHv37sXBgwfRoEEDAIBYLMb9+/fRvXt3ODg4YPjw4XB1dcXly5dLbKlmGKbiMNSRYsswd/RqUgNyBWHG/kAsPxn89qD0xTUgLeoNBQhIi+TLvWtdNA0xw30G9nXfB89qnshX5GPTg03odqAbTuTeQY0//0CNlb9CbG6GvOfPET5kCKJmzoIsOVllOxa1DNBvthscmluCCLh5OAyHVvojI7nku8EMwzBFEZEwNaBJ8BkAwI6a/A3I3nV7Fxa8/ieQn8lnerbvUO71ZBgAQN12QLVGQH4WcONPddeGeVeJIcDJ2cAvjsDhCUBMICDRAhp/CeiaA2/KL25QA6jlVZ61LRWVuru40rhx417bPfzChQvFlvXt2xd9+/Ytsby2tjZOnjxZmtVjGKYcaUrE+KVfI1ib6OC3s0+x5nwIwpOysfzzhtCSljw7ADJKTnr4weWKsDOyw7r263Ah4gKW316OiPQIzL4yG7uCd2GG+ww4Hz2KuF9/Rcqu3Ug9cAAZFy7AcsZ0GHTvDq6gJUlDS4L2Q51hU98EF3c+QeSTFOxadANtv64P20af3tySDMOUn4SIDKQn5kAsIpjEB0JW1wYBulEw0DBA21oFeWZyUoEb6/jHrBWbUSeOA1pOBf4dBNxYD3iNB7QM1V0rpiQKBfDsNHBzA/9byagW4PYN0GQgoGMCPDwE/Ps1+EC7aKNHwXGm01JA9Jrrswqs0rdkMwzDvIrjOExu74Cf+zaCRMThcEAUBm26geTMvJJXeNduSinh/EnlA+rjY+ODgz0O4lvXb6Ej0UFgQiC+OvYV5gYshXjaaNTasR2a9vaQJycjavoMhA8bhrznz1W2U8+jGvrNcoO5jT5yM2U49mcgLu16Alk+m1ObYZiSKbuKm+e9gFiRjzuN+BlPutTpAk1xQQ+9WxuB3FTArB7g2E1dVWUYnmNXwNyR/0ze3KDu2jCvykoCrv4G/N4E2NGvMMCu2x748l9ggj/gPYEPsAE+n02/rYDBK7mtDKq/U76biooF2QzDVFmfu9bElmHu0NeS4NbzZPT+8xpeJGYWL1jLiz/Yv7Y7U4GzPwB/NOfHG+XnvHd9NMQaGNZgGI70OoIedj0AAIdDD6Prga7YIbmD6v/ugPm334LT1ESW33WEdu+BhLVrQXmFNweMLHXQ5ztXNGrHzyEaeOEl9i69g6ToEl4XwzBVntBV/Ml5gOOwrfpzAECvur34AnmZgN8a/nGrqSVOLcgw5UokAloUzNjjt4b/jDLqF3UP+G8s8Et94PRcIPk538vAcxww/i4wcC/g0LHkVmmn7pBPCERg23+wx2QMAtv+A/mE+59sgA2wIJthmCrOu64Z9o32Qg0jbYQlZKLXH9dw54XquGeIxECnZQV/vBpoc/xPPV9A0wBIeMKPN1rZALj40wclZjHXMceiFouws8tONDRviGxZNlbdXYWexz5HQCc72B76D7penqC8PMSvXIWwPn2QdfeusL5YIkKLz+3RdVwjaOtLkRiZgT1LbuHh1SgQsaRoDMPwkqIzkRyTBRFHMEt8gHQna8TqyVDfpD7qm9bnC935G8hKBIxrA86937Q5hik/Dfrwn8nsJP4zyqiHLBe4/y+wsT2wvjXgv42fgcDKBej2GzA5mM8Cb2r3xs2ceBCNFssvottRMaZFtUC3o2K0WH4RJx5Ev3G9iowF2QzDVHkOlvo4MMYLLjUMkZSZhy82XMexwFcO7G/rzvTFLuDbIKDjj4ChNZAZD5xfDPzqDByZzCf9eE8NzBrgH99/8GOLH2GhbYHIjEhMujAJYx8vRM7PM1B9+U8Qm5gg9+kzvPjyK0TPmw95kTkcazUwRf857qjpaAxZngLn/wnGqU1ByM2WfcjbxDBMJRN6j2/FNs15AYk8B2fr8b1ietkXtGLn5/DdPgG+5VBcJVL5MJ8CsQRo8S3/+OpvH9R7jPkIqS+Bswv5a5z9I/gs4SIp0OBzYNhJYNRlwHUwoKHz1k2deBCN0dvuIjpVdR/GpOZg9La7n2ygzYJshmEYABYGWtg9ygPt6lsgT6bAmO13sf5SiGrLr1N3YNIDyAYexO1aoyEbeBCYFFjYnUnLAPAcy4836rMJqNYYkGUDtzcBv7sCO78EXvjx80O+IxEnQje7bjjc6zBGuIyAhkgDN6JvoO+Rvlhtfh/mB3bCsA/fupTy778I6dwFaceOCfXWNdRE9wmN4dnLDiIRh2e347B70U3EhKa+6d8yDFMFKLuKm4ZdAUnEOGQdDw2RBjrbduYL3NsGZMTw2X0bfaHGmjJMCRp9wX82M2KAe9vVXZvKjwgIvQjsHgisbAhc/plvUNCvDvjM5hsaPt8E2Hi8c3JEuYLww+GHKOmqSLnsh8MPIf8EpyZlQTbDMEwBHQ0J1g1qhsGetQAAPx4Lxtz/HkAmL5LMTCQG1WqBSBNPUK0WJY8tEksBl8+BkReAIUcBh04ACHh8FPirE7CxLRB0AJC/e4uyjlQHE5pOwMGeB9HOph0UpMCux7vQ7fyXuDioAWps2QwNW1vIExIQOXkKIkaNQt7LSAD8nNpNO9ZCr2lNYWCmhfTEHOz/+S7unHgO+gRPXAzDfLy0hGzEh6cDIJglBiLa2RIZOhza1WoHQ01DQJ4PXFnFF/aeBEg01FldhilOogl4TeAfX1nJf2aZ0peTxieYW9Mc2NodeHQYIDlQuyXfk2/SfaD1d4D++89lfTMsqVgLdlEEIDo1BzfDkj7iBagHC7IZhmGKEIs4fN/dGXO7OoHjgG3XwzFi621k5n5AF2uOA2q3AL7cDYy9BTQdDIg1gcg7wJ4hfObN62uB3Ix33qS1vjV+9fkVGztsRF2jukjNTcWPN37E4JifEL92JszGjgUnlSLz0mWEduuGxE2bQTK+7la2hug32x32bpYgBeH6wVAc+u0eMlPYnNoMU9Uou4obZ4dDIz8Dh+1SAAC97QvGXd//F0gNB3QtgKaD1FRLhnmLpl/z8yynhgOBe9Rdm8olLhg4OoVPZHZsKpDwGJDqAs2GA2OuA0OOAE49+IaF90BEeBCZihWnHuPb3f7vVpX0T284AAuyGYZhXsFxHIa3sMWfX7lCSyrC+cfx6LfOD7FpOZArCDfCknAngcONsKR378Jk7gB0/43vTtV6BqBjyk/5dWI68KsTcHo+kBb1znVsXq059nTbg9nNZ8NQ0xDPUp5hxIUxWNzgGbR3/AkdNzdQdjbili9H2Od9kR0YCADQ1Jag/TAnfPZ1fUg0RHgZnIxdi27ieWDCh7xVDMN8opRdxc1e3oBCU4ortrmooVcDblZugEIOXF7BF/QaD0i11VhThnkDDR1+mBbAf2YVbMrKjyKX8fNW/92Vny3l1kYgLwMwtQd8lwNTgoGuvwAW9d9vswrCjdBE/HA4CC2WnUfX36/g93PPEJP2bjf5LfS1PuTVqBXLYMEwDPManRpYYaeBB77ZchtBUWnouPISJCIOCRl5AMTY+vQ2qhlqYX43J3RqUO2t2wMA6JkDPjOBFpOAgJ3AtdVAUghwdSU/FYnL5/x0F1YN3ropiUiCAY4D4Gvriz/u/YHdj3fjbPhZXHp5CYMnfY0vQnyRvGIVcoOD8bxffxh/9RXMJ02EWE8P9b2qwaqOAU5tCkJCRAaOrrmPRp9Zw7OXHcRSdv+VYSqzzNRcRBfkZTBPCMCjBgbI1UhFr7q9IOJEwIO9/HFJ2xhoNkzNtWWYt2g2nO8unvgMePgf0IBlwX9vGXH89KN3/gLS+KFm4ERAvc6A+wjAtvU7j7NWysmX41pIAk4+iMWZR7FIzCycblRbKkZrB3O0d7LATycfIy4tt8Rx2RwAK0MtuNuafPhrUxMWZDMMw7xBExtjHBjjjb7rriG2hDuuyuyXfw5s+u6BNsC3DDUbBjQdAjw5AVz7HQi/xgfeATuBOj58C5LdZ289sRlqGmJm85no69AXy24tw/Xo69j4YBP+0zbH1D8mouGuu0g/fATJ27Yh/fRpWM2dA/127WBspYvPv2uGawee4f65lwg4F4HIp8no+E0DGFm+PSMowzCfprCABIAAw6wIaOWm4FAdEUScBD3q9gAUisJWbI8xgKaeeivLMG+jZQA0/x9wcSn/2XXu9d4BYZVEBLy8BdxcDwQdBBQFY9p1zPjM4K5DASPr99pkek4+zj+Ox8mgGFwIjkNmXmHPAiMdKdo6WqKjsyVa2ptDW4PPaaOrKcHobXfBASqBtnIPzu/mBLHo09ufLMhmGIZ5ixrGr+8qSeBPBD8cfoj2TlbvfyIQiQDHzvxP5B2+ZfvhQSD0PP9j4Qx4jeOnxXhL4qG6xnWxvv16nI84j+W3luNlxktMf/AjGrZpiBk+s6G98h/kh4fj5bjx0GvXFlZz5kBqZYWW/Rxg7WiCs1seISEiA7t/vIXWAxxQz8MKHLtQYZhKJ+RuHADALPo28vS0cN82H17VvWClawU8OgLEPQQ0DQD3kWquKcO8o+ajAL/VQOwD/sZ1PV9116jiysvie6vc3ADE3C9cXqMZ/5137sknlXtHCRm5OPMwFieCYnDtWSLyiiSLtTLQQgdnS3RytoKbrQmk4uI95To1qIY/BzbFD4cfqiRBs3rfnoIVDAuyGYZh3uJmWFKJrdhKRbNfetqZfvg/quEK9P0LSP4euLGW77oVFwQcHA2c+QFoPpJv/dY2fu0mOI7DZzafoUWNFtj6cCvW31+P+wn38SXuo9eMLhh2qxWy/9mFjDNnEXrND+aTJsH4qy9Ru6EZBsx1x+m/ghD5OAVntzxC+MMktPmyHjS02amCYSqLnMx8RD5JAcB3Fb/RQAS5mOMTnhEBl5bzBd1HANpGaqsnw7wXHRPAbThwdRX/GXboxFqzX5UUCtzaBPhvA3JS+GViTcClL+D+DVC9yTtvKiIpCyeDYnAqKBa3XyShaHqaOua66OhshY7OVmhYwxCid2h86NSgGto7WcHvWRxOXb6BDi2bw7OuxSfZgq3ErpwYhmHe4l2zWn5/KAiDvWqjo7MlTPXe/S5wMca1gE5LgNbTgTt/8wF3ejRwdgFwaQXQZCDgMRowsX3tJjTEGvjG5Rt0t+uOVXdX4VDIIRyIOIqTNXUwcemX8Nh2D7n37iP2xx+Revgwqi34Abr166P7xCa4e/IFbh4Ow9NbsYgNS0WH4Q1gaWvw4a+HYZgK4/n9BJCCoJcVBZ3seJyuJ4aJlina1GwDhJwFou8BUh2+qzjDfEo8xwE31vG9wkIvAHY+6q6R+ikUwLMzwK0NwNPTEDpkG9Xib0o0GcTfoHgLIsLj2HScfBCLk0ExeBidpvK8Sw1DdHS2RKcGVqhrof9BVRWLODS3NUHiI0JzW5NPOsAGWJDNMAzzVu+a1fJxbDpmHQjE3P8ewKOOCTq7VENHZyuYfWjArW3EJ0jzGAME7ee7kscGAjfX8SfM+t34OUJrNnt93XUssLjFYgyoNwBLby7F/YT7WJK4A9Z9amBeqwEw3nwEOYGBCPu8L0wGD4b5uLFo5lsbNesZ49TGIKQl5GD/8jto3qMOmrS3AfeJn/QYpqoLKcgqbh7rj3RjLTyumY9BdbpCKpIAFwtasZsNA3TN1FhLhvkAehb8VJk31/Fjs6tykJ2VBNzbzrdcJ4cVLq/bDnAbAdi3B0TiN25CoSD4R6TgVFAMTgbF4HlilvCciAPcbU3Q0dkKHZytUMOIzUDwKhZkMwzDvIW7rQmqGWohJjXntdkvzfU1MdirNk48iEFgZCquPkvE1WeJmHvwAZrbmqJzw2ro5GwFc/0PCLglGkCjAUDD/vzdeb/V/J3ph//xP9Ye/Ljtep1fe9J0MXfBP53/wdHQo/j1zq+IyIrECN29aDu9Kf53Xgo6dxVJmzcj/cQJWM2fB6vWrdF/jhvOb3uMkLtx8DsQgpePk9F2cH3oGn5EKz3DMGqTlyNDxMMkAIB5gj/OueSDuIKu4i+uAhHXAbEG3yLIMJ8i7wnA7c3A88tA+HXAxkPdNSpf0QH8WOvAvYAsm1+mZQg0Hsi3XJvavXH1PJkC10MTcTIoBqcfxiIuvXConIZEhFb2ZujgbIV29S1hovvmPDFVHQuyGYZh3kIs4jC/m9Mbs18u6OGMTg2qYaxPXYQnZuHYg2gcC4zG/Zep8AtNhF9oIub/9wDutibo4lINHRtYvf+8jxzH35m38wFiH/JTft3fzV8Y774OmNThW70bf8XPHfoKESdCN7tuaGvTFhsDN+LvoL9xNusuLniIMb6RD1rueoT8qChEjPof9H07wXLmTHQc4YxHV01wefcTRDxMwu5FN9FuiBNsnD9i7DnDMGrx4kEi5DIFtLPjoJsZjctOYjQ0bwg7Izvg0GS+UJNBgMGnmWiIYWBYE2j8BXB3K3DpZ2DgXnXXqOzJ8vgb7rc2ABE3CpdbuvBjrV36Ahq6r109K0+GS0/icTKIn2orPUcmPKevKYGPowU6OluhTT1z6Gqy0PFdsXeKYRjmHbxP9ksbUx38r7Ud/tfaDhFJWTgWyAfcAS9TcT00CddDkzDvUBDca5ugS8Nq6PQhAbelE9BzDdB2Lj/9xq1NfFKTY1OB8z/yd6zdR/Ld516hI9XBhKYT0Mu+F1bcXoGz4WexUusydgwxwLyHnrA4fAPpx08g88pVWEyZjPr9+sHKzhCnNgYhMTIDh38PQOP2NvDoUQdiCZtTm2E+FaH3CrqKx99DrIUGXljI8X3d3kDELb6XjEgCeE9UbyUZ5mO1+JZP7vXsNBDl/14JvT4pqZH8vNZ3/gYy+e82RBLAqQffJdzG47XJ31Ky8nDmURxOBsXg0pN45MoKM4Kb6WmivRM/1ZaXnRk02Hn+g7Agm2EY5h19SPZLaxMdjGpth1EFAffxB9E4GhiDgIgU3AhLwo2wJMw/FAS32nwLt28DK1gYvEfArW8FtJ0HtJwC+G8Hrq8Bkp/z2VWvruK7mHuOAywci9dN3xorfVbievR1LLu5DM9SnmGc0y20rGWDMScAPHmOmO9/QOp/h2D1w/f4fIYrru19hsCLkbh3OhxRT5LRfrgzjCzYnNoMU9HJ8hV4EZgIALCI98exJjJoS3XRybYTsGcYX6jhAD7xIsN8ykzq8NNeBv7Lj83uv03dNSo9RHxX+JsbgOCjABXMQ61fjZ/X2nUwf11QgujUbJwK4hOX3QhLgrxISnAbEx10dLZER2crNLEx/uSTjlUELMhmGIZ5Dx+T/dLaRAcjW9lhZCs7vEzOwvHAGBwNjMa9iBTcDEvCzbAkfH84CG61TNDZxQq+LtVg+a4Bt4YuP8WX23Ag+Ahw7Xfg5S3A/x/+x74DH2zbtip2Z9ujmgf2dNuDPU/2YLX/alzGS1zpRZgY6givoy+Q7e+PsN59YDp8OFqM/h9q1jfBuX8eIe5FOv5dfAutv6yHes1LPqkzDFMxRAYnIz9XDs3cZOinh+Oqkxgda3eEbkIIP68wJ+JbABmmMmg5hQ+yHx0G4h4BFvXVXaOPk5sOBOwCbm0E4oMLl9dqwXcJd+wKiKXFVguJz8DJoBicDIpFQESKynOOVvrCVFv1q+mDY1OelSoWZDMMw6hBTWMdjGhVByNa1UFkSjaOB0bjaGA0/MNTcPN5Em4+T8IPRx6iWS1jdHapBt8G1WBl+A4Bt0jMdxVz6gGE3wD8fgceHQGenuJ/rBoCXuMB514qJ2SJSIIvHL+Ab21f/BHwB/59/C9W1n2GXcMlmHOtNizuPEfiunVIO3Ec1ebPx4A57ji9+SGinqbgzF8PEfEoCa0GOEBDi51WGKYiCgvgW7HN4+8hpIYIscYFCc8urOALOPcGzOqqsYYMU4osHPkZOB4dBi7/AvTZoO4afZj4x3yrdcAuIC+dXybVBRr157uEWzqpFCciBEamCoH1s7gM4TmOA1xtjAsygluilunrx2kzH49dDTEMw6hZDSNtfNOyDr5pWQdRKdnCGO674Sm49TwZt54n44fDhQF3Z5d3DLhtmvM/iSHA9T/56Txi7gP7RwBnvgea/4/vWqZlKKxipGWEWc1noa9DXyy7uQw3Ym5gXPsItLM3xrDTcuBFOMKHDYdB927o8t10BNwwxq0jYXh8PQYxoanoMNwZFrXYnNoMU5GQAkJXcfP4ezjoBtQ2qI3G0OYTJgF8yx/DVCYtp/JB9oO9QJsZb82sXWHIZcCT43y+lbBLhctN7QG3b/jEbkXO2zK5AjefJ+FUUCxOBcUgqkjeGKmYg6edGTo6W6K9k+X7539hPhgLshmGYSqQ6q8E3McfxOBYYDTuvEjG7YKfBUcewlUIuK1QzfAt81Oa2gFdfgZ8ZgG3NwE31gNpkcDpucDFn/hAu/n/ACNrYRV7Y3ts6LAB5yLOYfmt5ThjG4mrgwnjbpqj2dUEpB06jMyLl2D/3XeoMdkHpzc/RGpcNvb9dAeevezQ6DNrNqc2w1QQuUli5GbJIM1Lh0FaCK7VF2G4fW9wV34FQHxX01daxBjmk1e9MVC3PZ8A7epKoPvv6q7Rm2XEA3e3ALf/AtJe8ss4EeDgC7iPAOq0EYZ75eTLceVpAk4GxeDMo1gkZ+ULm9HREKNNPXN0dLaCj6MFDLSKdyNnyh4LshmGYSqo6kbaGN7CFsNb2CI6NRvHA/mA+/aLZNwp+Fl45CGa2hgJLdzVjd4QcOuYAK2mAV4TgPv/8vNtxwfzv6//yXch9xonZGLlOA5tbdqiRY0W+OfhP1h/fz2Wt0xG3ToiTDunB+OXqYiePRs67u7oOX0url3ORui9eFzd+wwRj/g5tXUM2DyaDKMuCgUh6mkK0kP576FZQgAe1AIy9CXoZtIQCCxovWat2Exl1WoaH2Tf2wm0ns5P8VWREAEvb/Ot1g8PAvI8frmOKdB0MNBsKGBkAwBIy8nH+WA+I/iFx/HIypMLmzHWkaJdfT5xWQt7M2hJxWp4MUxRLMhmGIb5BFQz1MawFrYY1sIWMak5OF4wD/ftF8m4G56Cu+EpWHT0EZrYGPFZyl2qocbrAm6JJtB0ENBkIPDsDJ8kLewi36XuwV6gdks+SZp9B0AkgqZYE9+4fIPudt2x6u4qHMIhjPkyAz3vaOLzy3Jk3byJ7AG94TpqFGr264qrB8IQHpTIz6k91AnW9U3K981iGAYh/nG4vPspMlNyobzcSzBrhDD9YLSqaQaz21v4zMR12wE1mqq3sgxTVmya8+e055eBq78BnX9Sd414+dlA4F5+buvogMLlNVz56TedegJSLcSn5+L0jXCcDIrBtZAE5MsLM4JXN9RCh4Lx1e61TSARs6m2KhIWZDMMw3xirAy1MNTbFkO9bRGbloMTD/gs5beeJ8E/PAX+BQF3Y2tlwG2FmsYlTLPFcYB9e/4n+j7fov1gH38x8vwyYOYAeI7lp/WRasFCxwKLWyxGv3r9sOzmMuxzD8QlB8K4s9qo/yQbiatXQ6fOMXSbOBeXbnBIisrEod/uoWkHG7h3rwMxuwBgmHIR4h+HE+seFFueL9VDjdzhsMnKAO4N5xe2mlbOtWOYctZyCn9Ou7sFaDUV0LNQX12SwvhhW/7bgOxkfplYE2jQh88SXsMV4YlZOOkXhZNBMbgTngwqjKtR10JPmGrLpYYhywhegbEgm2EY5hNmaaCFwV61MdirNuLScnAiKAZH70fj5vMk3ItIwb2IFCw+9giNrI3QxcUKvg2qwdqkhIC7WkOg93qg7Xzgxlrgzt9AwhPg8ETg7EL+zrrbN4CuKRqZN8K2zttwJPQIfr3zK+b3jofXIxFGnBMDoaHImzgU3n36IqT553h4IwF3T4Yj8kkKOgx3hoHZW8aPMwzzURQKwuXdT0t+kuMAEJLOSKAwlENk2xKw8SjX+jFMuavTBqjRDIi8zd9Mbr+gfP+/QgGEnOWzhD89BaAgaja0AdyGg5oMxKNUDT4j+J5LCI5JV1m9UU1DdGzAT7VlZ65XvnVnPhgLshmGYSoJCwMtfO1ZG1971kZceg5OFrRw3wxLQkBECgIiUvDjsWA0rGmIzi7V0MWlhIDbsAbQYSHfuuX/Dz9WOzUCuPAjcOUXoPGXgMdYiMzqortdd7S1aYsN9zdgq3grAmzz8NUFEdrdUyBz3x7UMD0Hs8GzceORLmLD0rB70U20+coR9m6W6nmDGKaSIiJkp+cjOSYTYQEJBV3ES8aBQ2aONqK166NGq6nlWEuGUROO41uwdw4Abm0CvCfxOUrKWnYy4L+db7lOCi1cbtcWimbf4K6mG04+isfJNYEIT8oSnhaLODS3NRGm2nprclOmQmJBNsMwTCVkoa+FQZ61McizNuLTc3EiKAbH7kfjRlgi7r9Mxf2XqVh6PBguNQoDbhvTIgG3lgHfVdx9FJ+MxW81EOUP3N7MZz6t5wt4jYeujScmuU5CH/s++Pn2z1ivfQ4XG3AYfRKoHp8IyS+T0cK7PR7YDkDsyxyc2hSE8EdJaNXfAVJNlpiFYd6HQq5AWkIOkmMykRyTheTYLCRHZyIlNgu5WbL32lamkStg27qMasowFYxDJ8DSBYgNBG6sA3xmlt3/ir7Pj7W+vweQZfPLNA0hb/Qlbpv3wsEIHZzeF4uEjJvCKpoSEVo58BnB2zpawFiXJQ391LEgm2EYppIz19fEII9aGORRC/HpuTgZxGcpvx6aiMDIVARGpmLZiWA0qGEgBNy1THX5lcUSwOVzfrzYi2t8krQnx4HHx/if6k0Br/Gwrt8dqz5bBb8oP/x06ydMqf4UPa6L0OcaQXr1NJxuX4Fpr1l4GGuC4GvRiAlJRYdvnGFura/eN4dhKqC8HBmSY7KQUjSYjslCalwWFEUSH6ngAANTLWjpSRH3PL3kMkXoNOkiTAfEMJUexwEtJwN7hwI3/uRvImsZlN72ZXnAo0N8l/CI68JiuYUzgmr0w9YMd5y8kY703AThOX0tCdo6WqCjsxVa1zOHjgYLyyoTtjcZhmGqEHN9TQz0qIWBHrWQkFEYcPuFJOJBZBoeRKbhpxOP4Vy9MOCubabLX6DU9uZ/4p8A19fwU6JE3eUvWgxtAM8x8GwyEHu67cG/j//FGu018HNMxTcnFXB5kQ2rXXOh18AHD2r1RUpsFvYuuw2v3nXR0KcmS97CVDlEhMyUPCTHZiI5uiCgLgim39TdWyIVwchKB8ZWujAykUBPnA29/ERopr4ERUYgy/8Bzmn3Ra6mUclBNBE0c5NhqF/BpjJimLLm1AMwtQcSn/JduFt8+/HbTIvie3fd+RvIjAMAkEiCCKt22EkdsSnCCnnhBIBPcmaur4kOTnziMo86ptCQsISglRULshmGYaooMz1NfNW8Fr5qXguJGbk4GRTLB9yhiQiKSkNQVBqWn3wMp2oG6NKQn4fb1kwXMHcAuq0CfOYAtzby3eJSw4ETM4DzSyBpNhRfNh+FzradsebeGiw23Y0WgXIMPquA/oPzcH18C8/afofoHFNc+fcpXj5KwmeD60Nbj3WPYyofuUyB1LhsPpiOyUJyTCZSYvhgOj9X/tr1tA00YGypDUMDEfQlWdDNT4JO2ktIYsIgux+OvGMvIU9MBABkFvwo2ZsBD5xH8HPwFg20C9IU2z/bC0XioDJ4tQxTgYnEfGv2wdHAtdX8cChO+v7bIQKeX+HPfY+O8FPhAcjSMMdRzY74OcETsaHGysKoZaqDTs5W6OBshSbWRhCJ2E3lqoAF2QzDMAxM9TTxZXMbfNncBkmZeUIL97WQRDyMTsPDaD7grl/NAF1crNDZpRrqmJvz49paTAICdgJ+a4DEZ8DVlYDfahg1+Byzvcahb72+WGa1DJPsbmDQOQXaBGbA8cQ8GNbrjCc1OuN5YCJ2L7yJdsOcUbOe8duqyjAVUk5mPlJiswrHSxcE1GkJOSBFyV28OREHA1NNGOoD+tJsPpBOj4Rm3DNwgaHIPxEJyssTymeXsA2xoSGk1taQWteERk1r5OdlA1u2oUHQBjyt2xe5WoXfKc3cZNg/2wuLhACIzCaW9lvAMBWfS1/gwhIgJRy4uxVwHf7u6+amA/d3Azc3AvGPhMUPJA3wZ9ZnOJnTDLKC0MqpmgE6OluhYwNL1LPUZ721qiAWZDMMwzAqTHQ18IW7Db5w5wPuU0F8lvJrIYl4FJ2GR9Fp+PnUEzha6aOLSzV0blgNds2GAU2HAE9P8uO2X1wF7u8C7u+CQ5022OgxDuccv8Byi59x0SUCI44rUP3xMei/DMAjt3HISDXAfyv90cy3Nty61IaIzanNVECkIKQn5xSMl85SSUCWnZb32vWkGiIYGgD60hzoypKhkx4FrdinkEY8AiXEFSufX/QPiQTS6tWhUbMmxDWrI8fSCKmmWkgwlSBSPx9RSEVMVgxiMyMRm3UXyVmJWKMPmCUEwDzhPlKM6iJXwwCaeWkwSnkGAiFBH8iwFsGt9N8ihqnYxFI+u/jRyaArKxGca468F7cRfJ2Dk2dniCUlhEbxT4BbG0H3doDL4/MdZEMT+2UtsFXeHo9zbMBxgFttE3QomMO6xKkymSqFBdkMwzDMa5noamCAuw0GuNsgOTMPpx7G4GhgDK49S0BwTDqCY9Kx4jQfcHd2qYbOLi1Rd6gvEHmH74738D8g9AK40Atoa+GEFs3/h612WZhXazM6XcpET79IuF6ajyeO/RFt4YHbx57jZXAS2g9jc2oz6iPLkyMlLlsIopXjpVNisiDLV7x2PR1tgr5GDvRkKdDOiIJ2XAg0Ih5CIyMeJbVjKdu3xYaGkNSsCXk1M2RZ6CPFTAtxRsBLfRmea6UjOicOsVnPkJRTkI04F0DUayoh4vB3exGm7FeAQDBOKZwzWwGAA/B3exH65Ca+9/vCMJVC46+Qd+p7aGREw+X8MLgAwNk/EHvWFFGe89Gk42BALgOenIDi5nqIwi4C4L87IYpq2CZvh33yVsgR68PLwRRDnK3Qrr4lzPU11fmqmAqGBdkMwzDMOzHW1UB/Nxv0d7NBSlYeTgXF4mhgNK4WCbh/Of0E9Sz5gLtL699Qt933wI21fLe8uIfQPDwBI/Qs0b3pV1g1KA3TnE5i5PE81H/4D0zig/C4/kDEhKZh9+Jb8BnoiLquFup+2UwlRUTIycgXunXz02FlISU2E2mJOYUR8CtEHEFPIxe68hToZkRDKz4EWnEh0MmKhUT+moRlEgk4KwvIrEyQYa6HZFMNxBgSIgzy8Ew7HS8oAUk5jwE8LlwnH0BS8U1pijVhpWsFSx3LEn9HZURhAiZgRW9gyGkFzIokGk/S5wPsm/VE+J+O+Ye+dQzzSfM/txuN89Lw6p0vc0qE+bUJeBl+EqZJd6GdHQ0RADlxOKdoii3yDrgnaYjWzlZY5GwFn3rm0Nf6gDHdTJXAgmyGYRjmvRnpaKCfmzX6uVnzAfdDPmna1WcJeBybjsex6fj1zBM4WOqhs8tQdBs4FnYR+4Dra4H0KFhe+gU/SnVwr0EXLBubCMtLT/DV+btwu/kcQU7DkAZbnNzwABGPqqNFP3tINdic2syHUSgIaQnZQrKx5Fg+8VhSTCZyM18/t7SUk0FPkQKdzBhoJ4RBJ/0ldLJioZWTCBEVb80mAz3kWJojw0wXiSYSRBsq8EI/B0+00xCimQKFKA5A8a7hRQdaa4o1Xxs8W+pawkrHCoaahm8c31nXwBaWcsItBw637MWoH0EwzgCS9YBH1hzAAVZyQlOzRu/zNjKlQK4g3AhLwp0EDqZhSfCsawExS4JVruQyGar7/QBCsRgbIo7PaVYz8igAIIn0sFvugyMavmjg4oJhDSzhZWcGLSk7HzFvx4JshmEY5qMY6WigXzNr9GtmjdSsfJx6yCdNu/IsAU9iM/Ak9ilWngHsLRqja4O96KdzC9UebARiA9HYfw+2cyIcdvPCQqdkdDueDC//XxBWuwte2HTEwytRiH6Wgo4jGsC0hp66XypTgeXlyAoSj2WpJCBLicuCQvaaZmkA2vJ06GTFQCclAjpZsdDJioFuVgyk+RkqF+EkFiHX3ADxdSyRYCxGlKEMYbrZCNHNQKwRkKWVAyDmNf+FEwJoZbCs8rsgmDbSNCoeQCsUQG4akJ0MJIbyv4WflFf+ToY4NRIzZAmYbGEGcMDDWoX5DbiC7OLTExIgjrgB2Lb8kLea+QAnHkTjh8MPEZ2aA0CMrU9vo5qhFuZ3c0KnBtXUXb0KSaEg5MkVkCkI+TIF8hUK5MsJMrkC+XIF8mQEmYJ/nC8n5MsVkMkL1in4u+jjfLkCitDLGInE4hF2AeXX7y/0RGTTiWjnUgsjahlDwvKEMO+JBdkMwzBMqTHUkaJvM2v0bWaN1Ox8nClo4b78NAFP4zLwa1wGfoU56povwijnl/BN2wO9iAvo8eQK2nEc1ndzxHKXLAw5eRiNk5/gYf2vkRwD7FlyC96f26NB6xosS2sVRkTISsvjW6SjleOk+WA6I/n1c0uLFPnQyY6DTmY0dLJioZsVWxBQx0KsKEwzlqergRRTLTypLcZLfR280M9BrBEQa8Qh0QBQiDIAZLyydQ4aIg1Y676mBbrgt5FED1xuarGgGCmPgOxrxZcrf3JSgRJazt+kHYBf4hKw1NQYsUUSOVnK5ZiemIx2WdlARux7bZP5cCceRGP0trvFRiDEpOZg9La7+HNg0zINtOWKwiBTCDgLAleZgg9W8+WFj18XuKqsLzynQF7RwLfI48L/Ufj/80tYP1+uWj/l/3pNUv6P0l30BHiH2SJdXL0xtFvT0q8AU2WwIJthGIYpE4baUvRxrYk+rjWRllMYcF96koBn8ZmYFm+MaRiJtia98a3eaTglnMS3Lx4hXCrBysHVYHHjCTrcWYIn9QYi0dQFl3Y9QcSjRHz2tRO0dNk4uIpIni9D6OHrUNyMRKj8Oup284RY+v6XGnK5Amnx2SrzSicVJCDLy3n93NLSvPSCIDqmoEU6tqCLdxI4EOQiDqnGUsQaAfdryAoCaBHijLiC1mgFgKwiWxRBQ6QBS11LNFUGzVqmsJLqw1KsDStIYUkcjPPzwOWkFATNyUD0XdXW5pwUvjX6Y0h1AW3jgh+jgh/j4j+pL4GTs9AuKxs+Wdm4q6WJeLEY5nI5mubkQujoqmf5cfWpQhQKQr6CDwBl8sLH+QWtrMpAUaZQDUzzFQrk5SswY39giUP8lcum7bmPwMhUyBVQCVxLDEIVhLwigWvRx4WtuKrrlEWwqi4aYhGkYg4SsQhSsQgawmMO0oJlEuFx4TLlY8vEWkD82/+PtkmNsn8xTKXGgmyGYRimzBloSdG7aU30bsoH3GcfxeLo/RhcehKPs0lmOJv0Bczhi4kG59FHcRK/xEbgmqMW1jsQfE+vhX1SGzyz64mwgETsmn8NHUY1RHV7Y+RlZ+Hypj+R9iwSlyKeoeXw0dDQZlOnqMODv07j+pUM5EoNATgi6lQ+Lh89DI8WemgwtH2J6+Rmy4QgWkhAFp2B1ISc1zfekgLa2QlFWqRjCoLqOEhlmcjQ5hBrCLw0BmKtURBAc4g1EhW0Ris3LIJUJIGlpgmsNAzgJNaBlUgTliSClYJgmZ8Pq7xcGOekgYtJBrILumrLcj7ujdIyVA2KtYxKDpZVfowAyTtmLlbIkX3pN2hmxUDMAW45qi38CgJydaygXcvr417HWxBRQQBaNEDlWzeLBqVFu/e+7vnXBbOyYoFt4fr8/323bSnrmC8ruu3CbZV1kJqeK8Oa8yFl+0+K4DjwgaeIg1QigkRUcrCqDGY1igSuGiUEsRKRCFIJB6moYD3hcfH1Xw2G+edeWS7ioCF5pYyIg1jEfXRPJrnMBbGLlsKcElHScHgFAXGcKRybd/yo/8MwVSLIXrNmDZYvX46YmBg0atQIv//+O9zd3V9bfs+ePZg7dy6eP38Oe3t7LFu2DJ07dxaeJyLMnz8fGzZsQEpKCry9vfHnn3/C3t6+PF4OwzDMJ81AS4peTWqiV5OaSM/Jx9lHcTgaGI2LT0SYk9YLi+GLz8WXMJo7gd8pCv920cPNFxfR8XYIQuyHIhOWOLDiLgz1IpCVpI98jSYAmiDtARAy9jiq2z5E17lz1f0yq5QHf53GxesiQGKgsjxXYoCL14Gc3NOwbOWKpJhMvpt3RAqS47KRnfX66EUsz+XHShe0RutmxUInMwYaufFI0pch1ohDmDEQW4tvheaDaTGytPgrZylEsBRpwZKToJYCcJfJYZWbC8vcTFhlp8EyLxcmCgU4hL7/C+bEJQfCbwuWtQwB0fslTSIq6D6bK0O+jB9jmlfwW9mSqRyfmidXIDdPjhM5A/EzfoaCoBJIKIPFmVlfwerkk4JuxEUD0OLBqLKFVAg+X31e9mpgqyxfiZpPSyAWcZCIOCEQlAhBZWGwKRFzSM+RITwp663ba2lvBgdL/SJBpWqwKpWI+McS5f8qEuQWrPPGwLVImaqcbE0skSDKcz7Mr0147fcj2nM+rEqaL5th3kOl/wTt3r0bkydPxtq1a9G8eXOsXLkSHTt2xOPHj2FhUXxqmGvXruGLL77AkiVL0LVrV+zYsQM9e/bE3bt30aBBAwDATz/9hN9++w1btmyBra0t5s6di44dO+Lhw4fQ0tIq75fIMAzzydLXkqJnkxro2aQG0nPycS44DkfvR2P3k07Ynt0O7UW3MTLtKDqbh2BT9yyYXl8Gq9S+iKnmidQMG0CqeiGfLzXCi5deOLJwYaUMtIkIIEBBBCj4vxWKgmVFfhMRqOB5UhCIUPC7yHKVMkXL8n8r/4fiLduQ58lx9WoOINYpzBqkxPHpem/cEwP37pX4mjRyU4QWaWUwrZDHIFknBfHGHJ4YAXG1OZWx0SJOAku5Alb5ebCUy+Esk+EzmRxWqXJYJspgJZPDWKHAW1MViTVBOiaAlhEUWsaQaxlBrmEAmaYR8jWMkCc1RJ7UADlSQ+RIDJAjMUCW2AA5nDZyC4JSZZCbL1cgV6bgA+FkBfITlMEwIU+WgXx5mhAU5xUNjuV8C6qwvrBMGUh/SLDaFJmiSZgv3YrqReYBi4EpfsgfhJMKV+DiB9xc+EgiDnzwJypsNZWICls5JSrLVcsULatsGS26LWVwW/K2lMuLb0tZVhnEFi332m2JOIjeMVD1C0nEFxuuv7XcmDZ14Wln+rFvMfMOmnQcDH8A1f1+gCUK54uP40wRrZwnm2E+EkdElfpWY/PmzeHm5obVq1cDABQKBaytrTF+/HjMmDGjWPn+/fsjMzMTR44cEZZ5eHigcePGWLt2LYgI1atXx5QpUzB16lQAQGpqKiwtLfH3339jwIAB71SvtLQ0GBoaIjU1FQYGBm9foZzl5+fj2LFj6Ny5M6RSNvZR3dj+qFjY/ih7GbkynH3Ej+G+8DgezvJgfCM5hjpa97AvyQRmKT9DJikhqAMAIojk2XDxNIFIJAbJqTCAVAaJcmVwSgVBJL+eQlFSwElQCAEmCv4ueKz8rQAIyu0UtogoiP+7IDYuWIeD8szLL1f+zRX+DQDEgQpS4PLLOLw2Je4nQDM7CfoZ4dDNioVWdgzyRLFI0YxDglFOQXduvjU60ZCgL1HAUs4Hy5YyOaxkMljKC37L5DB5JYDOEekgS6SPDLE+MjgDpIv0kA49pEEPqdBDCukiueAnUa6LRIUuEuS6SJdLkC9X4FO6EhKLOGFcqoaE74orVf4Wi5CZK8OLgpZTERRwFwXDAimIgxFuKhyhfOdaO5ihnpVBYQApdB0ubB1VBp2vD2ZLCFZfadktuq13DUwrE7mC0GLZOcSk5pQ4LpsDYGWohSvTP6vSLczqIJfJ8NDvGIL9/eDYxBNOnp0hZi3YalXRr6/eJ36r1J+kvLw83LlzBzNnzhSWiUQitGvXDn5+fiWu4+fnh8mTJ6ss69ixIw4ePAgACAsLQ0xMDNq1ayc8b2hoiObNm8PPz++dg2yGYRjm9fQ0JejRuAZ6NK6BjFwZzgU3wqH7rfDs8X0MzD2JTKnu61fmOCgkOgi49TFjZz/tgBakAEcEjm/uBlcQ/XOk4EP4tzzPFdw5EMoWW1ZYNk+qh0y9tycJypb/hwsud6HQU0BDVwZLRWEA7SyXwyJfDp1cTSBKB+nQQwrpIQW6SCE9xEMXz4gPllMKnksteC4VupB98OVM8YHfHMcnV9IoErjy40M5aEjE0BAXjheVFilXWEZ1eeH6ymVvWl/5mIOGWMx3F35lW28LxIq2nCogwnWFU4nl/teatZyWB7GIw/xuThi97S44QCXQVu7J+d2cWICtBmKJBI4evghNIjh6+LIAmylVlfrTlJCQALlcDktL1QyalpaWCA4OLnGdmJiYEsvHxMQIzyuXva5MSXJzc5GbW5h8JC2NzzKan5+P/Pz8162mNso6VcS6VUVsf1QsbH+UL00R4OtkDl8nc2TmOmHv3CeA7O3r6acGQyMvHoACBAIfUPFNyyQ8Vj5XuEx4jitcjwBQ0b+5wnWLLldwhc+T8LugDFe0jLLcK2U5BaD8u2A9iAgKrqBuRZ/nSFgXwroF/0dU8EoLmnuJI+G+QUGjOf+YQ8G6BRf8BcuIA7iC5VRw7c8J5ZXbBIjjYBTjAMN81ZvTJRGb5qBxfnOkJusjI1UfySI9vBQZIFtsgGyxPvI09CCRSIoEm5xKIKoMYC3EItRUtuQWacUVyryyjvQ1zxVtDVYGt8pETRWTAgq5AorXJ1cHADSpqQ8rA03EpuW+oeVUE01q6rNjWDlpW88Mvw9ohEXHghGTVngtaGWoidm+jmhbz4ztCzVh5/OKpaLvj/epV6UOsiuSJUuW4Icffii2/NSpU9DRqbiZcE+fPq3uKjBFsP1RsbD9oR65UjmQ/fZyaRZXkWXFARCBwIET2o1EfIsSJwJXpBs2EQcRJ+IfgwMHEcAVRKsQFXRNLwjAODG/FnFQFKzDQQSOU25PVJAFt/B/AXy378IyHEjo7s4VbL/wN1ekrBjg61KkLCnLcZzw+oRlBeX5AFlU0Ku+cPvKegj/ExwfRCvrKxIJ24CyNsriKEwWxAFI0D8Ps3vJyJcavbb7vjQ/GQkNa8Cxmg9EXMnFis8//RaKgp83XPPICn7e4eNSqXS24rA5rcjnRcDfDvK1zMLJE8fVULOqbboTEJLGIS0fMJACdgaZkL+4g2Mv1F0zhp3PK5aKuj+yst6exFCpUgfZZmZmEIvFiI2NVVkeGxsLKyurEtexsrJ6Y3nl79jYWFSrVk2lTOPGjV9bl5kzZ6p0Q09LS4O1tTU6dOhQYcdknz59Gu3bt6+QYyKqGrY/Kha2P9Trmm1DPF55961BXb0xi+BVv2a516+qyctriyVzR6Ba5jf8IPOi+6RgsHO40T7MHLwBGhrvOA0V81E6A2gaFFus5bSaoRZm+zqiozObI1td2PmjYmH7o2Kp6PtD2RP5XVTqIFtDQwOurq44e/YsevbsCYBPfHb27FmMGzeuxHU8PT1x9uxZTJo0SVh2+vRpeHp6AgBsbW1hZWWFs2fPCkF1Wloabty4gdGjR7+2LpqamtDULH5xIZVKK+SHSKmi16+qYfujYmH7Qz1aNKiNG1p/QFfe6bVBXYrWDbRo0IeNcywHUqkUNq3r4PGlTaiX3Af5GsaFz+Un47HxftRrVQe6unpqrGXV07VxTfg2rAG/Z3E4dfkGOrRsDs+6Fuw7UUGw80fFwvZHxVJR98f71KlSB9kAMHnyZAwePBjNmjWDu7s7Vq5ciczMTAwdOhQA8PXXX6NGjRpYsmQJAGDixIlo3bo1VqxYgS5dumDXrl24ffs21q9fDwDgOA6TJk3CokWLYG9vL0zhVb16dSGQZxiGYcqOWMSh4ZjJuPP7CpjnexYL6uKl1+E6ZgoLJsrRyB6LsR6zsSt2Huq+qAvjLEMk66QipNYz9LfsiZE9Fqu7ilWSWMShua0JEh8RmtuasO8EwzBMOan0QXb//v0RHx+PefPmISYmBo0bN8aJEyeExGXh4eEQiQqTnHh5eWHHjh2YM2cOZs2aBXt7exw8eFCYIxsAvvvuO2RmZmLkyJFISUlBixYtcOLECTZHNsMwTDnp1KAaMH4KFh3wR4vokzCWyZAskeCKbWvM6TWFf54pVyN7LMaQvHk4cOFPPAy5i7Z2Pljd5l/WRZxhGIapcip9kA0A48aNe2338AsXLhRb1rdvX/Tt2/e12+M4DgsWLMCCBQtKq4oMwzDMe+rUoBraO1nB75krTl2+ga4tm2Mx6w6rVhoamujtMxZa2cfQ2adiznPKMAzDMGWtSgTZDMMwTOXEusMyDMMwDFPRVNTJIBmGYRiGYRiGYRjmk8OCbIZhGIZhGIZhGIYpJSzIZhiGYRiGYRiGYZhSwoJshmEYhmEYhmEYhiklLMhmGIZhGIZhGIZhmFLCgmyGYRiGYRiGYRiGKSUsyGYYhmEYhmEYhmGYUsKCbIZhGIZhGIZhGIYpJRJ1V6CqIiIAQFpampprUrL8/HxkZWUhLS0NUqlU3dWp8tj+qFjY/qhY2P6oWNj+qFjY/qhY2P6oWNj+qFgq+v5Qxm3KOO5NWJCtJunp6QAAa2trNdeEYRiGYRiGYRiGeRfp6ekwNDR8YxmO3iUUZ0qdQqFAVFQU9PX1wXGcuqtTTFpaGqytrREREQEDAwN1V6fKY/ujYmH7o2Jh+6NiYfujYmH7o2Jh+6NiYfujYqno+4OIkJ6ejurVq0MkevOoa9aSrSYikQg1a9ZUdzXeysDAoEJ+yKsqtj8qFrY/Kha2PyoWtj8qFrY/Kha2PyoWtj8qloq8P97Wgq3EEp8xDMMwDMMwDMMwTClhQTbDMAzDMAzDMAzDlBIWZDMl0tTUxPz586GpqanuqjBg+6OiYfujYmH7o2Jh+6NiYfujYmH7o2Jh+6NiqUz7gyU+YxiGYRiGYRiGYZhSwlqyGYZhGIZhGIZhGKaUsCCbYRiGYRiGYRiGYUoJC7IZhmEYhmEYhmEYppSwIJthGAZAcnKyuqvAMAzzzsLCwtRdBYZhmLeKiIhQdxXUggXZVYxCoVB3FZgSyOVydVehStu9ezdsbGzw5MkTdVeFYRjmrb777jtMnDgR/v7+6q4Kw3wSWJ5n9diwYQMGDBiAixcvqrsq5Y4F2VWETCYDAIhE/C5nBxv1unbtmvD4p59+wq5du9RYG8bW1haenp7w9fXF06dP1V0dpkBQUJC6q8AUCAoKQnp6OgBg2bJl7IaUmtWvXx8xMTH47bffWKBdQeTn56u7CkyBgIAAhIeHAwAmTpyIs2fPguM4NdeqavL29kZiYiJ+/vlnXLp0Sd3VKVcsyK4CZDIZpk2bho4dO+LIkSN4+vQpO9ioUXh4OHr37o2+ffti6tSpmD9/Ppo2barualVp7u7uWLZsGerVq4d27dqxQLsCWLNmDVxcXFiX2ArA398fAwYMwNq1azFu3DjMnDmT9YpSs6FDh2LKlCkIDAzEypUrWaCtZnfu3MHw4cORmpqq7qpUaUSEp0+f4rPPPsPGjRsxatQorF69Gubm5uquWpUkl8vh5OSEQ4cOISwsDEuXLq1SgTabJ7sKyMrKQnBwMLZu3YqwsDAEBARgwYIF6N27N/T09NRdvSonNzcX58+fx+effw6O43Dnzh04ODggPz8fUqlU3dWr0u7evYs5c+YgKCgIZ86cgb29vbqrVCWtW7cOEydOxPbt29GnTx91V4cBMH36dGzZsgUZGRk4efIkvL29IZfLIRaL1V21KoeIhBvlu3btws8//wxnZ2dMmjQJTZo0UXPtqp6AgAA0b94cY8aMwS+//KLu6jAA/vnnH0yYMAHZ2dk4ePAgOnXqpO4qVVnK88Tjx4/Rp08f2NjYYMaMGWjVqpW6q1bmWEt2FaCjo4OmTZti5cqV+Pnnn/G///0Pw4YNw4wZMxAcHKzu6lUZypYfTU1NaGpqQkNDA7q6upg/fz4AQCqVCt36GfVo2rQpFixYAGdnZ9airSbr16/H2LFjsXPnTpUA+86dO2qsVdWkUCiE41bjxo0hk8lgY2ODa9euISkpCWKxmLVolyNlm0jRnmgDBgzA5MmTERQUxFq01SAgIACenp6YOnUqC7DVTKFQCN8Ra2trSKVSGBgY4Nq1awgNDRXKsbbF8iUWi0FEqFevHvbu3Yvw8PCq06JNTKWjUCiExzKZrMQyR44cISsrKxo2bBiFh4eXV9UYIgoNDSWFQkERERF0+PBhsrGxoc8//7xYufz8fDXUrupQfk8iIyMpKiqKQkNDhedu3bpFvr6+ZGNjQ0+ePFFXFaucPXv2EMdxdP78eZXlPXv2pJYtW1J2drZ6KlbFvXjxgrKysigiIoKmTZtGTZs2pQULFlBSUpK6q1ZlyOVy4XF4eDgFBwdTVlaWsGzbtm3k6upKX3/9Nd29e1cdVaxy7t+/T/r6+jR79myV5XPmzKHvv/9eTbVi7t+/LzzeuHEjVa9enb777juVc7xS0etlpvQo39e4uDgKCwujvLw84Zr24cOH5OzsTL6+vnTx4kV1VrPMsSC7Ejt48CDFx8cTEVG3bt1o8+bNRFT44T916hTp6enRwoULVZYzZefYsWPEcRwdPXqUiIgyMzPp33//JRsbG+rfv79QbuLEibRv3z51VbPSU37W//vvP2rWrBnVrVuXGjVqRKtWrRLK3L59m3x9fcnOzo4ePXqkrqpWGfn5+bR8+XLiOI62bdsmLO/Tpw+5uLjQ8+fP1Vi7quvAgQNkZ2dHBw4cEJZNnDiRXF1d6ccff6TU1FQiIhozZgyFhISoqZaVW9EAe+7cueTq6kpaWlrUt29fWrdunfDctm3byM3NjYYOHUo3btxQR1WrjNzcXGrWrBkZGhpSbm6usHzp0qVkaGhIhw8fVmPtqq5Dhw5RvXr16LfffhOWrV69mqpXr06zZs0SjlG+vr506dIldVWzUlNeXx08eJAaNWpENjY21LRpU1q7di3FxsYSUWGg3a1bNzpz5ow6q1umWJBdCSkUCnr+/DlxHEeDBw+mtm3bkoODAyUnJ6uUISLasmULSaVSunz5sppqW7VkZWXRN998Q3p6enTs2DEi4gPtPXv2kLW1NTVq1Ih8fHzIxsaGtWSXsSNHjpCuri6tXLmSrl+/TvPnzyeO42jJkiVCmTt37pCXlxe5uLhQXl6eGmtbNaSkpNDixYuFQHvgwIHUoEEDCgsLIyLVG4Hp6elqqmXVcvnyZerbty95enrS/v37heWTJk2iZs2aUc+ePalt27ZkZmbGjlllbP78+WRhYUH//fcfPXjwgNq1a0eOjo70008/CWW2b99ONjY2ws1zpuz4+fmRhYWF0BNt6dKlZGJiQqdOnSpWljVilI/w8HD68ssvqXXr1iqB9po1a6hWrVrUqVMnat68OdWsWZOd08vQ0aNHSV9fn5YuXUrh4eE0ePBgsrW1pblz51J0dDQR8YF29erVqW/fviq9cioTFmRXYv7+/qShoUFGRkYq3WeKksvlNGrUKPrmm28oNzdX5Y4583FePakq39vs7GwaNWoUaWlpCYF2Tk4OXb9+nUaMGEETJ04ULlZf192f+TiRkZHk6+tLv/76KxERRUVFUe3atcnT05NEIpHKBaq/vz8bUlGGEhMTVVpAs7KyaOHChSSRSMjY2JjS0tKISLU1r23btrRixYpyr2tl97pA4Pr16zRgwAByd3dX6WGzfPly+uabb2jQoEHCBSs7ZpUNPz8/atSoEV24cIGIiC5cuEBaWlrUunVrcnZ2Fo5lREQnT55k+6Gc3Lhxg4yNjcnOzo7Mzc2FALvod2nLli105coVdVWx0nrd8SoyMpIGDRpE3t7eKr3Tdu3aRdOnT1e5xmI3BktfdHQ0tWnThpYtW0ZE/Dm+du3a1KBBA6pbty7NmzdPaNEODg6u1D2gWJBdyShPrDKZjM6cOUM2NjakoaFBQ4cOpRcvXgjlil6wbtu2jZo2barS5YkpPb/88gsFBAQQUeFJITs7m0aOHEna2tp08uTJEtdjB/+yk5ycTIsWLaKIiAiKjo4mJycnGjlyJKWlpdGIESOI4ziaO3euuqtZ6e3fv5/69OlDTZo0UemOnJaWRitWrCCO42jTpk3CcoVCQV27dqVatWqxVogytGvXLiGYU/Lz86MvvviCXF1d6ciRI8LyoucSdswqOwkJCbR69WrKzs6m06dPk5mZGW3atIlSU1Opfv36VKdOnWJjg1mgXbrCwsLo999/p3HjxlFKSoqw/NatW2Rra0vNmzcvljdi7ty5xHEcy+1Rhnbs2EHbt29XWfby5UsaNGgQNW7cWGVIRdHvBDtelY309HT666+/6Pnz5xQbG0sODg40atQoIiLq3bs31axZk7799luhRbsyY0F2JVL04HHo0CEhaL59+zZpaGjQoEGDVFrkinbP8PX1ZWOIykB6ejq1atWKDA0NKSgoiIgKA+3ExERyc3MjS0tL9t6rgXIs6eLFi6ljx46UkJBARESLFi0ie3t7srCwoLi4OHVWsVLbuHEjWVhY0MaNG+nmzZvCcuXFa1ZWFi1YsEAl0Pb19SUHBwchwGYXSaXv2bNn5OHhQe3ataNr166pPHflyhWqVasWNW7cmHbv3q3yHOsOW3pK6lEmk8koIyODZDIZ9evXj2bMmCF8/gcMGEANGzakCRMmsP1QRgIDA6lBgwY0btw4mjx5srBcuQ+ULdp9+vQRjmHz5s0jHR0dun37tlrqXBUkJCSQu7s7+fj4FMtjk5CQQA4ODtSgQQNaunSpmmpYNSmvnb7//nvq1q2bMFz1+++/pxo1alCnTp2qxPUVm8KrklAoFMJ8pf369cMPP/yAv/76C7m5uXB1dcW5c+ewe/duzJ8/HyEhIUhPT4e3tzf27NkDAFVmzrqy9up0Nnp6etixYwdat26NVq1aISgoSJh+xcjICA4ODtDQ0MCKFSvUUd0qgQqm63j06BFOnjyJFy9eICcnBwYGBpDL5QgMDISmpiZMTU0BAElJSZg8eTJCQkJgbm6uzqpXWocOHcLUqVPx+++/Y/jw4XBzcwPAT0fUq1cvREdHQ1tbG1OnTsWCBQswatQoVK9eHaGhoXjw4IEw3Z1EIlHzK/n00SvT2djZ2eG7776DhoYGFixYgGvXrgnPeXt7w9nZGdnZ2Th37pzKekWnlWI+nEKhgEjEX5pdu3YN586dQ3R0NMRiMXR1dSESifDixQtkZ2dDIpFALpeD4zjMmjULK1euBMdxbIqiUvbo0SO0bNkS3bt3x9KlS4Xz9e7du/Hff/8hJycH7u7uOH78OM6fP4/x48djxowZ+Omnn3Dp0iW4urqq+RVUHsprLOVn3NTUFBs3boSWlhb+/PNP7Nu3TyhramqKZs2aITc3F7Gxsex7UQaU72lQUBBu3LiBW7duAYBw7RQXF4f8/HxIpVIAQHp6OhYvXoytW7dWjesrdUb4TOkbOnQo1atXj0JDQykzM5OIClu4r169Svr6+uTh4UE1a9akjh07Cuuxsdgfr+h7+PLlS5VxJklJSeTr60tmZmYUGBgolO/fvz9dv36dtT6UsT179pCFhQVZWlqSnZ0d/fDDD8KYoA0bNpBEIqEJEybQoEGDyMTEhIKDg9Vc48orJyeHvvrqKxo3bpxKl29fX1+qW7cuWVtbU4sWLYSuZFlZWTR//nxq06YNa8EuZUWPWYmJicJsFEREx48fpw4dOpCvry/5+fkREd/LYPDgwbRz5052zCpj06dPJ0NDQ6pZsyYZGRnR/v37KTc3l3JycmjkyJHUqlUrGjVqFH322WfUqFEj4TzPzuWlKz09nTp16kQjRoxQWb5w4ULiOI4MDQ1p//79lJOTQ0R8i7ampiZxHMemUitlRT/boaGhFBMTI0wjGBQURB06dKD27dvTv//+S0T8eWLIkCG0b98+YV123Cp9e/fuJSMjI7K1tSVdXV1avHix8Nz8+fOpUaNG9L///Y8GDx5Murq69OzZMzXWtnyxILsSef78OTVv3rzYGF+5XC4cWPz9/Wn58uW0Zs0aleeZ0jN9+nRycnIibW1t6tq1q5B4Izk5mXr06EHa2tr0+eefU+PGjalJkybs4qgMFP3Mh4WFUatWrWjdunUUHh5O06ZNo+bNm9OECRMoNjaW8vPzadmyZdSsWTPq3Lkz3bt3T821r9wSEhLI0tJSJVHTvXv3aODAgRQVFUXh4eFUv3598vDwoKioKCLiL3SV+5MF2KVv/vz55OLiQg4ODtSxY0chODh9+jR17txZGFPXsmVLat68uXCsYses0lP0vbx27Ro5OTnR5cuXKSgoiKZMmUJSqVQYNvHkyRMaPXo0tW/fnvr37y/cfGL7o/RFRUWRo6Mj7d27V1h28uRJ0tTUJD8/Pxo8eDAZGhrSvn37hCF4AQEBVSqQKG+zZs0iW1tbqlOnDtWvX58OHTpERESPHz+mbt26UdOmTemzzz6jli1bkouLCztelQHl+Tg5OZkaNGhAf//9N92+fZv+/PNPkkgkNGXKFKHsmDFjyNfXl3x8fIT8RFUFC7I/Ya8eMB4/fkx6enolju+Nj48XxpwWXY8ddD5e0fdww4YNVL16ddq5cyft3buXBg4cSE2bNqVZs2YJZX788UcaNmwYjR8/nmURL2WvXtjcuXOHpk2bRl9//bXQs4OIH4ft7u5OkyZNEr4X6enpKmWYsvHixQuqWbOmEDDI5XLKz89XSRj04MED4jiONm7cqLIua4UoHUWPWWvXriUjIyNas2YNbd68mdzd3cnOzo4OHjxIREQ3b96kefPmkY+PDw0bNowFdKXs1XGJK1eupAULFqicM4iIZs6cSRKJRPjevHrOYDefysaVK1dILBartErHxcXRgwcPhL8HDRpEmpqadOfOHXVUsdIretzft28fmZmZ0d69e2nXrl00atQoEovFtHr1aiLiW7h/++03GjhwoMo1Fjtefbyi0wATEZ04cYLmzJlDY8aMUbl22rlzJ2loaNC3334rLJPJZJV2mq43YUF2JfL06VOyt7en5cuXFzsBHzx4kKZMmcKCiDJ04cIFmjt3Lq1du1ZYFhcXR4sWLaLGjRurzDFb9KTBLo5Kx4YNG6hnz55CciAiohEjRpCBgQE5OjoW++wvXryYvLy8aPjw4ULXcaZ8NG3alDw9PYW/Xz1eKecAvnTpUnlXrUo5fvw4/f7777Rjxw6V5d27dyc7OzuKjIwUlhXt2s+OWaWjdevWNHPmTJVlvXv3Jo7jqHfv3sW+F7NmzSItLS1avXq1yv5gN5/Kjr+/P4nFYuHmRknn7jt37pCrq+trp0plSse///5LM2bMUJmWi4jvui8SiYRhLa9ix6uPt2LFCmrRooXwXioUClq2bBlxHEf16tUrllV/586dpKurS//73//UUd0KgwXZn7iZM2dS165dhb+nT59OWlpatGvXLiGoePHiBTVu3FglIyZTehQKBYWGhhLHccRxHP3www8qzycnJ1OzZs1U7uoxpevq1au0fft2YRx8YmIiEfF3r6dPn042NjY0f/58lWlXiIjmzJlD7dq1o5iYmHKvc1WkDAy2bt1K2traNHDgwGJlsrKyqGvXruTr68taH8rQvXv3SFtbW6XHQNELpTp16ghd/ooGFiygKx1Xrlyh58+fC2N5i84HP3bsWNLR0aHjx48XW2/s2LHUqlUrth/KWNH3t0uXLmRtbU3Pnz8nIlIJNIiIpkyZQp06dRLGBzOlLzAwkJo2bUpaWlq0fPlyIlK98depUyf68ssvSSaTqQTV7Hvy8Q4cOEARERFCrhrlMSsjI4NWr15NIpGIVqxYUWy9v/76iywsLKp0IwYLsj9hOTk59Ntvv1G9evVULlbHjx9PWlpa1K5dO+rSpQvVq1ePunTpIjzPDjofr6T38NKlS6StrU1t2rQp1m157Nix1LVrV3ZHtQyMHz+erKyshAucmzdvUtu2benEiRNEVHjR6ubmRj/++KNwMauk7C7OlD7l/KWvjsOKjY2lSZMmkba2NnXr1o3u3LlDjx49ogMHDlDbtm3JycmJdUsuZa8esxITE2nDhg1Uo0YN+vzzz4Xlyve9V69eNGbMmHKtY1XRsmVLcnZ2Flqqf/zxR+revbvKFJuDBg0iQ0NDOn36dLH1lfuSnctLV2xsLKWlpQnvq/K7cOrUKbK1tSVbW1u6f/++sN+ioqLou+++I0NDQyGhKVM6Xv1sy2Qy2rp1KzVs2JDq168vnLeV11RDhgyhfv36lXs9K7vp06eThYUFRUREEBF/c7Bx48b04sULIuJvzCpbtH/77bdi6yunSq2qWJD9CSlp3G56ejpt3ryZHBwcVALtbdu20dy5c2n8+PEsyVkpK/oeKseoKPfN6dOnSSwW0+DBg4UxW+np6eTm5kajRo0q97pWdoGBgeTo6Ehnz54lIr416N69e9SyZUvq3LkznTlzhoj4fTZmzBhq1qwZLVu2rMof+MuaQqGgZ8+eEcdx5OHhQQMGDKCvv/6aQkNDhd4EUVFRtGzZMqpWrZrQourq6kq9evViWcRL2at5OJTHq+TkZNq4cSPp6enRN998o1K2SZMmrPdNGdi9ezdZW1sLrUGxsbF08+ZN4jiOhg4dKlzMEhENHDiQjIyMhONYUSzALl3h4eGkpaVFQ4YMoZEjR1JKSgrl5uYSER9s79mzhxo0aEDa2trUoUMHateuHbVq1YpsbGxYFvFSVvR4lZOTQxkZGUTEnw927dolJDYrev3l7e1Nw4cPV0d1K62goCCqVq2akEw5IiKCwsPDycbGhry8vIRjVdFAWzk2nuGxILuCU55Iix509uzZo1ImPT2dNm3aRA4ODjRo0KDXbosF2B+v6IXNkiVLyMfHhzp06EDbt28XDvgnTpwgiURCtWrVol69elHPnj3J1dVVOGGzi6PSExISQrq6urRz5046ceIEGRsbk0wmoxMnTpCvry916NBBJdAeP3482dnZ0a+//sr2QzkYOHAgDRgwgG7evEk+Pj7Upk0b6tatG/n5+QmBXlZWFh09epQOHjxIT58+ZVnES1nRz/ny5ctp4MCB1L17d2H8aGZmJm3cuJF0dXXJw8OD+vfvT/379yd7e3u2D8rAhQsXyMHBgQ4dOkTTpk2jbt26ERHR5cuXSSqV0tdff60SaH/99dfEcRzdunVLXVWuEq5evUocx9Gvv/5KX3/9NTk5OdHkyZOFAEOhUFB0dDTNnDmTPv/8c+rRowetWrWKQkND1Vzzymvx4sXUtm1b8vHxoX/++YeI+IB6x44d5OTkRJaWltS2bVv66quvyNHRUbg5y87tpSM0NJRcXV1pw4YN9Pfff1PLli0pOjqaIiIiyNHRkdzc3IRjVU5ODi1fvpw4jqN169apueYVBwuyK7CcnBxq2bIl3bx5U1h2+PBhMjU1pRkzZqiUTU1NpSVLlpC2tjbr4ldGih64V69eTUZGRrR8+XJq3bo1ubm50XfffSd0YTp//jxpaGiQs7MzHTx4sFj3M+bjKd/T3377jUQikZCLQOl1gfbUqVPZhVEZUwbQu3btoi+++EJYfufOHSEbbP/+/V9715vdECwdRd/HhQsXkqmpKY0YMYI8PDxIX19fmJZIGWjb2tpSvXr1VKaxY4F26Xr58iV9/fXXZGdnR1KpVBjnSMQPOSop0F6wYAHbD+Vg0qRJNGHCBCLix5POmjWL9PX1adiwYbR161aVsiyQK31Fj1fLli0jc3Nzmj59On355ZfEcRwtXLiQiPjzy+7du8nd3Z3s7e3p6NGjwnrse1J68vLyaOLEiVS/fn3iOI7++OMP4bmSAu3s7GxatWoVBQUFqavKFQ4LsiuwhIQE8vHxITMzM+Gi5+XLl7Ro0SJq0KBBsUD77t27VK1aNdLS0lLpIs6Urlu3btHYsWNVDuyzZ8+m5s2b07Rp04RA++zZsyQWi+mbb76h+Ph4dVW30tuzZ4+QdO7AgQMqzykD7c6dO5eYRIgpG8oL0JiYGKpRowYtWLCAiPiLqMaNG5ObmxtNmTKFDA0NycHBQeXmCFP6oqKiaPz48XT16lVh2ejRo0lbW5v+/fdfIuJ7RG3cuJGsrKxUbtSy6QVL38CBA0lDQ4Pc3d2FqdKULl26RJqamjRkyBAh0ZYSCyBKn0KhEI5XO3bsoFatWqlMVeTl5UU1a9akOnXqkJubGy1fvpxevnypptpWDQ8fPqRVq1bRqVOniIg/Bq1du5bEYrFwLpHJZPTPP/9QmzZtqHPnzpSenk5E7AZtaVG+j//88w9xHEd2dnb0zz//qMzSogy0PT09hTHajCoWZFdgcrmcYmNjqXfv3mRgYED+/v5ExF8wLVq0iJycnFQC7Xv37tGQIUPoypUraqpx5TN16lSV+TD/++8/ql+/PtnY2Ki8z3K5nGbPnk0eHh40ffp0Ye7TM2fOkLa2NvXv359lsC4FRRP+KH82bNhA27dvp/nz5xPHcbR9+3aVdU6ePEne3t7Uu3dvyszMZC0QZeTRo0f05MkT4W/lSXrr1q3Uq1cvCg4OpkaNGlGrVq2EC6LQ0FCaPn06C+RK0YoVK4Ts+kT8GGCO48jBwUGlVxQRH2jr6OgIQ5CUgXb16tXfOPSI+TDKY8+iRYto79691LdvX/Lx8Sk2hdrly5dVWu6Y0lc0k37Rc4KLiwtNnTqViPiu+jVq1CA/Pz+KiIigXr16kbe3t0qCOubjjB8/XiUr+4ULF4jjODIxMRGCbKV169aRRCJRadHevn07tWrViry9vYvN48x8vMOHD9P69etpyJAh5OrqSuvWrVOZ7zoiIkLots/O48WxILuCKtqt+MaNG9S4cWOqWbOmkKU3KiqKFi9eTHZ2dtStWzf666+/qF69ekJXJyJ2R+9jXbx4kUaNGqXSepCRkUEjRowgIyMjmjp1arET9bx586hOnTq0evVq4f0/fvw4mZmZUVRUVLm/hspC+V4qg7OilBdI8fHx9N1335UYaJ85c4ZdGJWhnTt3kpeXF40cObLY5/z27dvUuHFj0tfXJ19fX4qOjiai4scndoL+eFeuXKFGjRqpvJe5ubk0aNAglZ4eRYOKcePGEcdxdO7cOSLikweuXr2a7O3thX3FfLiiNzxevcEXEBBAPXv2LDHQvnfvHmu5LiNPnz6lESNGCON8iQqPP/v27aM+ffqQj48PWVlZ0e3bt1XWZdN0lZ4XL15Qx44dVa5309LSaPHixaShoUG//vorEal+bzZs2EAcx9HmzZuJiD+PbN68mTp27MjO8R9IeS7Ozs5+7XSNubm59MUXX5CrqyutX79eJdCOjIwsNqMOw2NBdgXXv39/6tChA7Vs2ZL09PTIxMSE7ty5Q0REcXFxtGfPHmrYsCE1b96chgwZIqzHWutKh/J93LlzJ126dImI+ERN33zzDTVr1ox+/fVXIUussvz69euFE7byd9EuNsz7UZ4A7t27R+7u7vT06VOV54t+1hMSEmj69OnEcRzt3LmzXOtZVW3evJkMDAxo9erVKmOxigZ68+bNIwMDA5WWbqZ0vTqt0/HjxykyMpKI+AukXr16kbm5Ofn5+RVb9+eff1YJ6NLT04vNKc+8vz/++IOGDRv2xtkM7t+/Tz179qS2bduWOGyCBdql6/79+2RtbU1Dhw4tMUHT48ePydbWlkxMTFTGxbNGi7L1zz//CL390tLSaO7cucRxXLGx8ER8j8Ki3wu5XM5mDPlAys/17du3qWPHjiU2BinP5Xl5eUKgvXHjRnZd+w5YkF2BzZ07l2rWrElhYWGUkpJCN2/epK5du5KRkZHKlBH5+fkqXZHZyeDjFQ3cgoODyc3NjTp27ChcoGZmZtKQIUPI3d29WKCtVDTIYDc9PkzRAFtDQ4Nmz5791nUSEhJo5syZxHFcsUz8TOm6fPkyWVlZ0e7du4s9p8ymT0T04MED8vLyEnoYsGNU2ZHL5fTkyRPiOI7+97//CeeGvLw86t69O1lYWJQYaCvLMKVj7dq1xHEc7du377VllOeFwMBA6tOnDzVs2LDEebGZ0vHkyROysrKimTNnqrTEvWr9+vVkZ2fHWufKSWJiIunq6lKrVq2EoXYZGRk0e/bs1wbaRPy1L7u2+nDK87C/vz9pa2vTxIkTX1tWeT2bn59PgwYNojp16tDff/9dHtX8pLEgu4JSKBQ0dOhQGjFihMrykJAQ8vb2purVq1NgYGCJ6zEf7tXWIKV9+/ZRp06dqEuXLnTt2jUi4gPtoUOHkpeXFy1cuJBdoJYy5Qng0aNHpKurS3PmzHnndePi4uj7779nWS7L2O+//05du3ZVCZrPnTsnJAKcM2eO0BrUtWtXcnNzU1dVq5z//vuPpFIpjR07Vuj2nZeXRz169KDq1avTxYsX1VzDymvHjh3EcRxduHCBiN58XlY+d/fuXZoxYwYbNlFGFAoFzZkzh7766iuV4Cw2NpYCAgJox44d9Pz5c8rLy6NHjx6Ru7s7bdq0Sc21rjqCg4OpTp069Nlnn6kE2nPmzCGpVEpr165Vcw0rF+U5++HDh6Snp0eLFy8mojcfq4oG2iNGjGCztLwDFmRXYGPHjiVnZ+diy5cuXSpkU341+yjzcV7NGlo0eNi/fz+1b9++WKDds2dPGjFiBLvBUYqKtmCbmJiQWCwWur6+7zaYsrN06VJq0KCBcByaOnUqtW7dmlxcXGjo0KGkqakpJNA6c+YMtW3blu2XUlZ0zK+S8j0+fPgwcRynEmjn5+eTt7c3de3atVzrWVVs3ryZOI6jOnXqCPvmbYHzq+cOFmiXje7du1Pfvn2Fv/fv309fffUVGRkZkYaGhsp0UJ07d6YmTZqw7vqlLCEh4bXXSo8fPyYbGxuVQDszM5MmTJhA3t7e5VnNSk15frh//z4ZGxuTkZERnT9/Xnj+XQJt5t2wILsCeN2H9syZM9SwYUNatGgRZWRkCMt37dpFkydPpo0bN5ZXFauEdevWUZMmTYSDu1LRA44y0O7atavQ5TI7O1s4aLFA++MV7cKko6NDEyZMoBYtWpCTkxM9fvxYzbVjijpx4gS5ublRw4YNqW7dulSrVi1as2aNMJ3Hnj17SCQS0ePHjykrK0v4frBAu3S8aczvq4H2uHHjhK7jMpmM7YMysH79ehKLxTRt2jTq0qULtWzZUmjtYe+3+i1fvpxcXV1p1apVNG3aNKpevTqNHDmSDhw4QLm5ueTu7k4+Pj5ERPT8+XPWXbyUrVq1itq2bfvGfA/BwcFkY2NDbdu2Fa7FiibkYtdYH6doA4aOjg7179+f+vfvTz4+PnTkyBGhHHufSwcLstWsaIC9ZcsWWr58OW3ZsoVevHhBCoWCZsyYQR4eHjR27FgKCgoSMo3PmzdPWI+dvD/eunXrSpxnWanoAefAgQPUqVMn8vDwUJnei+2H0hMcHEw6Ojo0ffp0IuIzh7u5uZGTkxNLnqVGJX3G//33X/rpp59oxowZFBcXJxzTFAoF7d69m9zc3Cg2Nra8q1rpvcuY36KBtlQqpYEDB6q0fLNjVunZsmULcRwntIQeOHCA2rZtS61ataKwsDAiYu+3uj148ICGDBlC9erVI3t7e9q7d69KoqeffvqJXF1dWUKnMrB27VqSSCTvlJD0yZMnVKdOHXJxcVGZlosFfqXj2bNnxHEczZo1i4iILl26RD169CAfHx/h+EXE3u/SwILsCqJ79+5Uq1Yt8vb2Jj09PWrRogX9+++/pFAoaOnSpdS8eXMSiURUu3Zt6t69u7qrW6ns3LmTOI6jQ4cOEdHrDyxFl+/YsYMmTZrELppKUdH38saNG7R69WoiKrwRxQJt9Sq6f168ePHWfZCbm0vdu3enAQMGsJN1KfuQMb979uyhFi1asGNWGbl69WqxpGUHDx5kgbYaKT/7fn5+dOTIEZLL5ZSTk0Pp6eklTgc5bNgwGjhwoErSRubjbd++nTiOozNnzhDRu3U5DgoKot69e7PuyaWk6HHn/v379O+//6o8zwLtssGC7Apg6dKl5OjoKIwHDgsLo4EDB1LLli3p5MmTRMQnrLl27Rrdu3dPWI+drD/e+vXrieM4MjU1fafWtpIOOGw/fDzlexgREUHbtm2jXbt2CVPVERWelBMSEligrWYzZ84kW1tbsrKyop49exYbK5+ZmUk3btwgX19fatCggTCmkX1PSseHjPl99b1n+6L0XLt2jbZs2UKrV6+m1NTUYvviv//+KxZos8Ch7CnP1fv27SNjY2NatGgRhYSECM8X/Q5kZGTQzJkzycLCgh4+fFjuda3MNm3aRBzHUd26dVWSLb5P8Ma+Lx9H+VkPDw+nPXv20C+//CKct4vuBxZolz4WZKtB0e6URESjR4+m3r17E1Hhl+Hp06fUqlUrGjhwYInbYBdJH0/Zfenvv/8Wxvy+S+DGDjilS/lZDggIIBsbG3JxcSGO48jZ2bnEjKLKQLthw4b06NGj8q5ulVP0WLN9+3aysbGhHTt20Pbt26lu3brk7u5OwcHBRESUk5NDEyZMoA4dOlCXLl2EjPvsIql0fOyYX3bsKl0bN24kKysrsre3J47jqFmzZnTz5k0iUp3f+r///qN27dqRj48PPX36VF3VrXLOnj1LBgYGtHHjRpVpNovum40bN9KXX35JNjY2KlOjMh9v7dq1pKGhQQsWLKABAwZQmzZt2Ljfclb0+srOzo7s7e1JU1OTzM3N6cSJE0SkOnWjMtBu37497d+/Xy11rkxYkF3Oih5U1qxZQ5GRkTRu3Djq1KkTEfFfCOUF6bZt20hXV/e9syozb7dnzx6SSCS0d+9eIuKnfGItpOWvaJZLHR0dmjNnDkVGRtLt27fJy8uLXF1dS0w+k5CQQPb29uTh4cGmTisnhw4doj/++IM2bNggLIuPjydHR0dyc3MTktKdOXOGDh06JOxblp23dLAxvxXLhg0bSCwW0/79+yksLIwCAgLI1taWfH19hTJFz/eHDh2iRo0a0ZgxY9RR3SppwoQJ9MUXXxAR38Pm+vXrNHbsWJo0aRJdvXqV0tLSaPr06fTtt9+y834pUyZcPHjwIBERXbx4UWglZYF2+Xg1ydnMmTMpIiKCbt++TV27dqVq1apRUlISEanuhytXrpCPjw9169ZNJeky8/5YkF2OirbmDB48mGxsbCgrK4tOnDhBHMcVyxa+a9cu8vDwUEn8wJSOq1evCuMZlUEaG/OrHuHh4WRubk49e/ZUWb5//37S0NBQ6TZeVGJiIpunsZzExcWRlpYWcRxHCxcuJKLCk3JCQgI5OTlRs2bNivUsYEFf6WFjfisO5Tl727ZtKssXLFhANjY2wnRpRKoXr5cvX2b7pxwo3/Px48dTx44d6b///qOBAweSr68vNW7cmLp06ULu7u6UlZVFqamplJ2dreYaVz737t1TSQxLxLojq0NUVBSJxeJiN/f27t1Lurq6dOPGDWFZ0f1w7do1ioiIKLd6VlYiMOVGLBYDAO7evQsbGxvs378f2tra6NixIxYvXoxRo0ZhyZIluHDhAvz9/bFw4UI4OjrCyMhIvRWvRC5evIhly5bh2rVrwv6QSqWQy+UwMzPD8ePHoauri549e+Lp06dqrm3VkJiYCEtLS0ilUpw6dUpYrqOjAz09vdeuZ2JiAltb2/KoYpVDRCp/m5ub49atW3BwcMCpU6cQGxsLjuNARDA1NcXly5cRGhqKn376SWU9kYidYj6Wn58ftm7dCn9/f7i7u0MulwvP9ejRAxMmTIBUKsXgwYPx/PlziEQilTJM6bO0tIShoSGOHDmChIQE4fuSlZUFPT09aGlpCWWV3xMAaNGiBds/ZUT5Ht+6dQuXL18GAPTq1QuJiYkYOXIkFAoFxowZA39/f/Tv3x9SqRREBAMDA5X9xXycCxcuYOXKlfj777+hra2t8lzLli0xZcoUGBgY4Oeff8axY8cA8N8RpmxkZmbCzc0NZ8+eRVRUlLDcxMQEEolEZR8VPVZ5enqiZs2a5V7fSkeNAX6l9ro7c3///TdxHEdGRkZ069YtYblMJqN169aRlZUVVatWjRwcHKhv375v3R7z7jZs2EAmJibk4eFBGhoa5OjoqNL1VSkhIYHc3d3JxcWFJUEpAyV9lq9evUotW7akrl270s2bNykuLo6srKxo2rRpaqhh1Va0pS0xMVEl025AQABZWlpSly5dKCEhgYgK92dJSZ+Yj8PG/FZc/v7+ZGFhIcz2ceDAAZJKpUL3WKb8KI9B+/fvJ1NTU/rhhx/oxYsXREQUGRkpDGVRlvvuu++odevWJc4vz3y4DRs2kJWVFTVp0oSMjY3JxMRE6H1T9Hh16dIl6tmzJ7Vt2/aNUxAy7+/V6yuFQkHPnj2jFi1aUO3atUkmk1FiYiKZm5vTd999p6ZaVh0syC4Dr071NHfuXBoxYgQ9e/aMkpKSaPbs2SQWi2nz5s3Fyj9//pwePnxI/v7+wjLWvezjbdy4kaRSKR04cIBycnIoODiY3N3dycvLi2JiYoqVT0hIoNq1a9NXX32lhtpWXsrPckJCAt29e5cuXLggdNe/du0atWzZktq1a0cmJiY0bty4YusxZavoseiHH34gHx8fatiwIW3fvl3IDaEMtLt27Spkty66Hgu0Swcb81uxhISE0OXLl1XmGb979y6ZmppSo0aNyMjIiNavX09E7HilDqdOnSI9PT3atGnTa8eRXr9+naZNm0YGBgYqM7UwH2/dunUklUpp7969lJycTE+ePKEOHTpQrVq1hP3x6tCJli1bqpznmY9T9Prq3r17KnHEs2fPyNPTk6pXr05WVlY0adKkYusxpY8F2aWs6EFk6tSpZG1tTc2bNydzc3OysrKi69evU35+Po0ZM4akUqkwLkUul5fYwsdasD/e2bNnieM4mjVrFhEVvqcbN24kExMTev78eYnrpaSksIChFCkP5EFBQdSuXTvq0aMHLV68WKXMpUuXyNvbm+zs7Oj48ePF1mXKTtH3+I8//iBTU1P65ZdfqE+fPmRtbU2zZs0Sxv0GBARQ9erVycPDg7UGlQE25rdi2bVrF7Vq1YpatGhBa9asUXnu7t27ZG9vT/Xq1WNJgtRo7NixNGTIECLik5zduXOHJk6cSAsXLqQrV65QUlIS9ezZkzw9PSkgIEDNta1c/Pz8iOM4oWeg8pj0559/kpWVlco1VtHjVUBAADtelRLl+/jw4UNq164d9ezZs9jsRM+ePaOePXuSRCIRxluza9yyxYLsMvLtt9+Sqakp3b59m9LS0ujZs2fk7u5OjRo1IplMRikpKTR27FiSSqV07NgxImKBRFm5fv06ubq6Ur9+/YT3mohoyZIlZGNj89bs7ewg9PGKZhE3NzenefPmUVBQkPC8n58fZWZmElFh1/Hu3bvTqVOn1FLfquz+/fs0btw4Onz4sLBs2bJl5OjoSDNmzBAumJQZStlxq/T5+/uTkZERDRgwgOLj44UL0xkzZpCTk1OxZJiv3oxlx6zSs3HjRjI0NKTt27cLXZCJiO7cuUPp6elExO8vc3Nz6tGjh5Ctlyk/MpmMvvzyS+rSpQtdvHiRBg8eTB06dCBnZ2dq06YNde3alYiIHj9+XGLPNebjPHr0iLy8vMjR0ZHCw8OF5StWrKCaNWsWe89fPV6xc8jHKXp9ZWJiQrNnz1ZJ3vvgwQPhnPDkyRPy9vYmW1tb4dqXvf9lhwXZZWDPnj3EcRxdvHiRiAo/wN9//z05ODgIF0jJyck0btw40tDQYPPRlTFl4NatWze6c+cOHTt2jLS1tWnPnj3qrlqV8fLlS3J0dKTx48erLP/555/JwsKCxo8fL7QEXbp0iXx8fKh169Z09uxZdVS3Sjp58iTp6+uThYUFHTp0SOW5n376iRwdHWnWrFnFplVjJ+nSx8b8qt+ZM2fI0tKStm/frrK8b9++ZGhoSLt37xYyU9+9e5eqVatG3t7elJaWpo7qVhlFgzTl40uXLlGNGjXIzMyM+vfvL1xT/fHHH+Th4cEyiJehV8f9EvHnEg0NDTbmupwor6++/fZbleVLly4ljuNoyZIlwrKQkBBq3bo1GRoaqvSKYkofC7LLwL1796hZs2bUuHFjlTt4I0aMIFdXV6HFjogPtL/66iuhmxNTOoKCgujUqVN05MgRIeGGcsyvu7s7aWlp0V9//UVEbB7fsqa8CNq6dSt5eHiodB1bunQpGRkZ0RdffEHe3t40adIkIdA+d+4cde7cWeXOOFP2Zs6cSVpaWjR16lSKj49Xee7nn38mIyMj+vPPP9VUu8qLjfmteGbPnk09e/ZUCZo7d+5MDRs2pL59+5K+vj7t3r2bsrKyiIjoxo0brHdHGVOeT86dO0fz5s2jXr160d69eyknJ4eSk5MpMDBQpdy0adOoffv27MZHKXvw4AEdO3aM/Pz8hGXKcb/Gxsako6ND//zzDxGxYY9lSfne7t69m7y8vOjp06fCshUrVpCOjg6NHj2apFIp/fjjj8J6jx8/Jl9fX5Yos4yxILsUFT2xhoSEUKNGjcjJyYmIiFauXEl6enrCWKCiZYsG3czH2759O3l4eFCfPn2Eg7zS5cuXydvbm5o0aaLSQsouisreqFGjyM3NTWXZzJkz6cqVK6RQKGjx4sXk4eFBI0aMoJycHCIi1vpQht70mZ80aRLVqlWLVq1apRL0EfHfL9YduXSxMb8VT15eHrm6utLgwYOFZampqbR06VKhm+Xo0aNJS0uLduzYwbrAlqN9+/aRgYEBDR06lEaNGkXVqlWjfv36qTRq+Pn50YwZM0hfX5+NwS5lO3bsIE9PT+rbty8tWrRI5bmnT59S7969ycDAQLhJy74LZW/s2LHk6Ogo/J2RkUGrV68WrnM3bdpEIpGIFi5cKJRRJp1lyg4LskuJMiEQET+Gi4goODiYGjduTAYGBmRsbEyXL18motcfcNjdvo+3efNmMjAwoD179giJHYj4k7LyAvXKlStszK8aTJ48mWrXri2MY3xVbm4u9ejRg3r27FnONat6ih6Dtm7dSlOmTKF58+apJNoaN24c1a5du8RAm4iN+y0tbMxvxZSbm0s+Pj7Uv39/ksvlwuf91fO0k5MTTZ06VR1VrJJCQkJUpt+Uy+WkpaUlJDYlIgoPD6cuXbpQkyZNWIBdyjZv3kx6enq0Y8cOla7GFy5cEM4rT58+JS8vL6pTp45wQ4pd35atSZMmUaNGjYio8Nz8ahA9YMAAatWqldCIwZQ9FmSXgnXr1lHr1q3pzp075OvrSyYmJkLr9MOHD6lz585UvXp1YRm7q1c2Ll++TDVr1hRucih98cUXZGxsTBMnTlQZ89umTRvy9vamGzduqKO6Vc769etJT0+P1q9fLxzklScD5Xdi+PDhNG3aNBbAlZNp06aRubk59enThzw9PcnCwoKGDx8uPD9hwgSys7OjxYsXsyziZYCN+a3YvvvuO9LX16f79+8TUeFxShkwREb+v737Dosi6fYA/GuSgCQJIiiCYsSIioqKCQOIYdcsRswoBoyAYkDSqpgVc1gVM6KrYnZVzBEwrJgVFUEBCZLn3D+808uIu9/uOjAI532e+9y1u2e+onu6uk5Xnao35OjoSL/++qvCyljaPHr0iBo3bky5ubn06NEjqlSpEo0cOVLcL1226PHjx/T27VsFlbJkkua9b9u2TWZ77969v5n327p1a9LQ0CiQdsTk7/DhwyQIgkxdlL8dlZ2dTS4uLuTl5cXtqyLEQbYcPH78mGrXrk0VKlSgqlWrFuhpePDgATVq1Ijq1q37zXVl2feRnktfX1/q3LmzzPnv06cP1a5dmyZPnky2trYFcn5dXV35pUcRatq0KVWqVIn27NkjkyaRlZVFHh4eVLFiRZlZMVnhOXfuHJmYmIgjbD59+kS7d+8mXV1dGj9+vHici4sL9erVi+usQsA5v8WT9Lf+6NEjql+/PpmZmdGjR49kjklJSSFHR0eys7PjRmsRioiIoCpVqlB0dDRVrVqVRo0aJd4P165doyFDhtCDBw8UXMqSRXo/LFq0iBwdHenDhw/ivkGDBlHNmjXJ19eXVFRUZALtP/74g1xdXfn+KALv3r2jbt26kaamJu3du1dmX25uLs2aNYvbVwrAQfZ3kk6a1b9/f1JXV6eOHTvS9evXCxz38OFDaty4MRkYGHBenZxJHwBt27alAQMGyGwLCAigDx8+UG5uLgUGBpK1tTWNHz++wDAabrQWLulD9s2bN1SvXj0yMjIiV1dXunnzJm3YsIFGjhxJenp6dPv2bQWXtGTL/zvfsWMHVa9eXWboWGZmJq1bt45q1aolcy2+7sFj349zfos/iURCoaGhVLNmTTI2NiY/Pz86cOAALV68mNq2bUt169YVnyUcSMif9Dd/5coVCgkJEbc7OTmRIAg0ePBgmeM9PDzI1taWl+kqJA4ODtSpUyfx38nJybR161YxzWXr1q0kCAIFBgYW+CzfH4Xv3Llz1KZNG1JRUaGZM2fSkSNHaOPGjTR48GBuXykIB9n/0deNzvDwcLp8+TLVqVOHunTpQhcuXCjwmT/++EMmb4jJl7OzMzVs2PAvh1KmpqZShw4daOLEiUVcMkb0572SmppKffv2JTMzMxIEgapUqUK9evWie/fuKbiEJdfvv/9OixYtIk9PTzGAi4iIIGNjY3GpQam7d++StrZ2gaXTOKiTL875Ld6k1yEvL48uXbpEAwYMIF1dXVJVVaWWLVvS6NGjxZfsvEKF/EnP//79+8nY2Jjc3NzEZ8Tp06fJzs6ObGxs6O7duxQeHk7Tpk3jSc4KUW5uLvXu3ZvatGlDRH8+D75+Ljg6OtLw4cOLunilWv5nxvXr12nq1KmkpaVFOjo6VKNGDerZsyfdv39fgSUsvVTA/rW8vDwoKysDAOLj46Grq4vWrVtDU1MTISEhGDBgAAIDA6GsrIwWLVogIyMDq1evxrRp0+Dn5wcAkEgkUFJSUuSfUeLUq1cPR44cQWhoKPr3748yZcrIXKu8vDyoqqrC0tJSwSUtnQRBgEQigZaWFvbs2YOPHz/i9evXsLS0hJKSEsqWLavoIpZIW7ZsgZ+fHxwdHdG0aVOYmpoCACpWrIhq1arh119/hZ6eHurXrw8AKF++PCwsLEBEMt/D9ZV8qampwcbGBsHBwbh//z7q1asnPheICIIg4O3btzA3NxevDSs6giCAiKCkpIQWLVqgRYsWePnyJbKyslCxYkWxvsrLy4OKCjel5E0QBFy5cgXDhw/H4sWLMXz4cPFZ3rZtW2RlZWH58uWws7ODmZkZDA0NcfHiRb5XComysjJ69OiBIUOGICQkBM7OzgAA+tJZB0EQ8OHDB0gkEjRu3FjBpS1dpHWVIAiwsbGBjY0NpkyZgsTERBgbG0NLSwsaGhqKLmapJNDXLSn2t/IHx2PHjkVUVBSSkpJgZWWF2bNnw9raGtHR0XB2doaxsTHat2+PPXv2QFVVFTdv3lRw6Us2IoKNjQ3i4+OxcOFCdOvWDWXLlkVeXh4+fvyIoUOHIjk5GREREeLDmn0/aeX+T+W/h/7tZ9m/s3fvXgwfPhxbt27FTz/9VCAYOHjwIGbNmgUrKyt06NABtWrVgr+/PxITE3Ht2jW+TwqJ9HcfExODPn36ICkpCadPn0aNGjXEY1JTU9GvXz+kpaXh3LlzfC0U6K/qKa6/Cof0vAYFBeHixYs4ePCg+MIjJycHqqqq4rGRkZGoWLEiVFRUoKenp7hClwLPnz/HuHHjcPHiRWzevBl9+/YV9yUmJmLQoEFISkriNpYc/Zc6huul4oOD7P+oX79+iIqKgo+PDx4/foyrV6/i3LlzOHnyJGxtbfHw4UN4eXkhLS0N5ubm2LhxIwD+8RcWaY/1mzdv0KVLF7x58wYdO3ZEnz59cPv2bVy+fBmJiYm4ceMGVFVVZXq42X+X//eckZEBDQ0NEJHYu8O/d8X58OED+vTpg7Zt22Lu3LnidmmVL70uR48eRUhICI4cOQJLS0vo6+sjPDyc75MiQEQICwuDp6cnkpOTMXHiRNSqVQvPnz/HkSNH8OHDB9y+fZuvhRxJ66SvR5NxXVX8TJkyBRcvXsTVq1ehrKwsc41u3ryJJk2aKLiEpc+5c+ewYMECnD9/HiNHjkS9evUQHx+Ps2fPIjU1FTdv3uT6qhBJJBIQkcz9wHVXMVb4I9JLnnv37lGdOnVkJjh7/fo1DRo0iCpWrEhPnz4lIqL09HRKTk4Wj+GcxsIlzUvJyMigESNGUJ06dUhNTY1atmxJ7u7unD9XiFauXEkLFiyQ+b0/f/6cfvnll79cF5sVrj/++IOMjIzoxIkT39z/9X0QFxdHsbGx4n3E90nh4pxfxZBIJJSdnU22trZ0+vRpIvrz2RwaGko+Pj6KLB6jP++NoKAgMjc3p7t378rMg5OZmUkuLi508OBBBZaydMmf93v79m2aP38+mZmZUfny5aldu3bcxipEI0aMIF9fXyL689y+evWK3NzceCLlYo6T7P6DlJQUPHjwQOYtXaVKlTBjxgwYGBjg1q1bAAANDQ3o6uoCgDjUiRUeQRCQl5cHdXV1bNy4EZGRkXjy5AkiIiKwZMkSqKiocP5cIfnw4QPu3LmDzZs3i/9u0qQJHj9+DC0tLQWXrnRKTExETk6OOIQyLy9PZr+Kigri4uLg6emJDx8+wNjYGBUrVhR7+fg+KVxf5/yGhIQgMjIS9+7dw4kTJ7Bu3TquswqBIAhQVVVF48aN0bNnT5w/fx5KSkoIDQ3FkCFDYGxsrOgiljr0/6Nr3rx5g7i4ODx//hwA4O7uDl1dXQwfPhx37tzB58+fkZWVhQULFuDs2bNo2LChAktdukjrKwCwtrbGnDlzEBkZiTt37uD48ePcxipEHTp0wPz587Fs2TKoqKjg5cuXaNasGc9l8wPgO+Eb6KuhF6mpqSAi6OjoAPgywZaNjQ0OHz6MGjVqiEFEzZo1kZ2djVevXgGAzHfwUI7/7uvr8XfyD6FRVlZGxYoVZb6Hhy/Jl/Rcz5s3D4sWLcKNGzcwZ84cbNq0CYMGDcLixYsVXcRSy9TUFKmpqQgPD0fTpk0LDLcEgNOnT4uTN+bHLwSLhvRaSK+Lubm5zH6us+RPeq5XrlwJbW1tODk5wdPTE7/88gsWL16M0aNHK7qIpYr0ehw8eBBz585FamoqJBIJevXqhYULF+Ls2bPo3Lkz+vbtCyUlJZiZmYkvoiwsLBRd/BKB8qUQ/V17K/92IkK5cuVQrlw5mW1cX8lf//79oaamhn79+iEpKQnbtm1Djx49EBQUpOiisf+Bg+yv5K9g9u3bh6tXr+LgwYPQ1dWFo6Mjhg4dipo1a6JNmzY4fPgwKlWqhGHDhkFFRQXx8fEAgAoVKijyTyhRpNcjNzcXmZmZ0NLSQl5eHpSUlCAIArKzs6GmpibzmfwPgvzBAr/okL/8uY3Tp0/HrFmzsHjxYlhbW2Pu3Lmcl12Evj7P5ubmGDt2LHx9fVGjRg0MGDBA5pisrCwcPHgQFhYWMhMJse9D/yHn999uZ/+d9HmioqICf39/xMTEYM6cOZgyZQrGjBmj6OKVOoIg4OzZs3B2dkZQUBBMTU2RnJyM8ePH4/Xr19i3bx9u3ryJHTt2IDY2Fvr6+ujQoQOqVq2q6KKXGPnvCemIQGkb6+uJ5vJ/5p9sY98vNzcXPXv2xJo1azB27FjY2toiODgYAK9UVNzxxGf55G8EzZ49G6GhobC1tYWenh6Sk5Oxa9cu1KtXDytWrECzZs0wYsQI3LhxA1paWrC2tsapU6dQo0YNHDlyRMF/ScmSm5uLmTNnQl9fH+PHjxeHvx44cAD379/HjBkzoK6urthClmLSCU7i4+PRuHFjmJmZoUKFCmjdujVcXFygq6vLgXYhy/+glTaWACAqKgoTJkzA9evXsWzZMgwYMADKysq4d+8e5s6di7i4ONy8eZNfhsgRESE3Nxdt2rTBggULYG9vL16fgwcP4t69e/D29lZ0MUs16W/90KFDGDJkCFq2bIkLFy4gPDwcdnZ2fC8UEel5dnd3R2xsLPbt2yfuu3v3LmxtbTFp0iQEBgYqsJQlX15eHpo1a4aePXvCy8tLrK8OHz6MQ4cOYc2aNShTpoyii1kqSZ/nL168QOvWrVGnTh2cPn0aixcvxqRJkwDwpI3FGb/++H/5f6Tu7u5Yt24d1q9fj5UrVyIoKAibNm3C+fPnERMTg/Hjx+PVq1fYtGkTZsyYgQYNGiAtLQ2DBw8WA2yJRKLIP6dEUVFRgba2Nu7evYstW7YAAI4fP47+/fvDyMiIA2wFU1ZWxuvXr1GzZk307NkTly5dQosWLXD58mWsXr0aKSkp/AAoRPnne1iyZAlcXFzQtWtX7Ny5E+bm5li3bh26dOkCV1dX1K9fHxYWFhg/fjxycnJw48YNMY+Or5F8cM5v8ScNsAcMGIDFixfj2LFjmDRpErp27YrTp0/zvVDIpH07T548AQC8evUKWVlZ4v7s7Gw0bNgQixcvxqFDh/Du3TuxTcX9QvKnrKyM6dOnY8GCBVi2bJn4QnDAgAFo2bIlB9gKJM3BtrOzg5OTE8LDw7F//354enoiICAAAI8gKNYKb061H9PUqVNJR0eH7t69S0R/zjoqndHv5s2bpKWlRRMmTJD5XP6ZF3kWcfnJf14XLlxIAwcOJBcXFypbtixt27ZNgSVjUnl5ebRixQqaPHkyZWdni9vnz59PgwcPpo8fPyqwdCVb/rrGx8eHtLW1yd3dndq2bUv169en7t2709u3b4mI6MKFC7RkyRJasmQJnTt3rkDdxuQjf53l6elJZcuWJV9fX9LW1qa1a9cqsGSM6Mv1+fz5M/Xt25c2btwos2/8+PFkb2+voJKVLgcPHiR1dXV68eIFbd26lYyMjOjs2bMyx2zdupWsrKzo06dPCipl6RIaGkoqKio0fPhw0tHR4fqqkOV/VvwdLy8vGjNmjMzzfs+ePWRoaEiJiYn/+HtY0eMgO5/Pnz+TgYEB1alTh169eiWzZAQRUW5uLhERTZ48mSwsLCg+Pl5hZS1N8gcBQ4cOJVVVVerTpw99/vyZiP55RcX+HenvPy0tjbKysv722L9aqo4D7KLx/Plz6t27N505c0bctmvXLurYsSP169ePPnz48M3P8QvBwpG/zurVqxcpKSnRtGnTFFgi9rX8S9/kvw/4eVL4Xr9+TaNGjaLg4GAi+rLcYJ8+fahDhw4ygfaMGTOoRYsWMs8XJn8SiUS8B2bOnElKSkrUt29fcT8/J+RPek7fvn37l8/n/4WXRy3+Sv1w8YyMDNy8eRPAlyW37t+/j5SUFAwcOBB//PEHgD9nXJTOmli5cmV8+vSJh4QXEel5Dw8Px4EDB9C9e3dIJBKsW7cOnz594qEyhURJSQlv3rxBu3btcOjQIWRnZ//lsflnp1ZSUhLvDX19/UIvZ2mzYcMGvH//Xubf9evXx71792TOd//+/dG3b1/cuXNHPP7rOosnTCkc0jrr0KFDOHXqFDp37ozg4GBcvHgRAA95LUz/9NzmX/pGSUmpwAzLrHDcuHEDrq6uiIyMhJ2dHYAvK7OMHj0a+vr66NWrF9q2bQt7e3usW7cOq1evLrD6AZMv+v+Uo9DQUAQHB2PMmDEIDQ3F8uXLAcjeH+z7SXPe79y5g4oVK+LGjRt/eezfnXdeHrX4K/UtrNWrV8PV1VX8t7GxMa5fv46nT5/C1dUVDx48APDnLMp5eXmIi4uDo6Mj59YVEUEQ8Ntvv8HJyQmrVq3C/v37YWtri0uXLmHlypVIT09XdBFLrIoVK0JVVRVz587F8ePH/zbQzo+Dt8Kxd+9e7N27F0ZGRuK2UaNGoWHDhnj06BFu376N3NxccZ+Liws+fPiAM2fOAODrUlQ451cxJBKJeG7z8vLEe+GfBAi85Gbh+PjxI27fvo27d+8iIyMDiYmJeP36Ne7du4ekpCTxuA4dOiAgIAAbNmxAzZo1YW9vj2vXrvFa2EVAGmAPGzYMCxcuxJo1a7B//354eHjAz88PAN8T8iINsCMjI9GmTRtMnz4dDg4OAP6sp/LXV3zef3AK6kEvNmJjY8nBwYGuXLlCRESZmZlERPTu3TsyNTWl1q1b0/3798WhHc+fP6emTZtSUFCQwspc2mRkZNCiRYto8+bNMtvnzp1Lnp6ePLxPjiQSiXg+pfcCEVGXLl2oWrVqdOjQof85dJwVLmlddO7cOYqJiRG3N23alKpUqULnzp0Tt3348IFq1apFO3fuLOpillqc86t4gYGB1Lt3b+rYsaP4bGdFLzo6mho1akSVKlWicuXKUb9+/SgtLY0uXrxI1tbW1Lp1a7p9+7Z4PD/LFcfd3b1ADvauXbuodevWMu0C9t9Jn92RkZGkoaFBXl5eMvsjIyMVUSxWiEr9El5paWlwcnKCtbU1li1bBgDi2stxcXFo3LgxLC0tsW3bNujq6qJt27aoUaMG9u/fD4CnzpeHt2/fwsDA4G9nsPz8+TM0NTUByC5XJD3/fB2+n/S8pqSkQEdHp8B+BwcHPH36FEFBQXBwcCiwPjkrXNKlPIgIN2/ehJ2dHaZMmYKRI0eKa8Y2btwYb9++hbOzM2rXro3Dhw/j6dOniIyMFJf1YkUjPT1dHJL8rTqLyU/+8+vv74+lS5eif//+ePjwIS5duoR169ZhwIABvB58EYqMjETLli0xYsQI/Pzzzzhx4gR27dqFjh07YsOGDdi/fz/Wrl2LsmXLwsfHBw0aNOB7o5BcvXoV1atXh4GBwX/+Dr428vH48WPUrFkTHh4e8Pf3F7f7+vpi/vz5ePnyJUxNTRVYQiZPpXrsIBFBS0sL8+fPx8aNG7F9+3YAgJqaGrKzs1GhQgXcunULz58/x8CBA2FjYwMTExMxwM4/NI39NxcvXkSzZs1w9OjRvx2KLA2wgW/nz/F1+H5KSkp49OgRatasiQEDBmDhwoV48uQJPn78CODLsml169bFpEmTEB4ejszMTAWXuPQgIjFITkxMhI2NDfz8/BASEoItW7bg2bNnAIBbt27B0tISS5cuxYULF9CoUSPcv39fXKaLfb9/+l6ac36LjjTAjo2NRVJSEkJDQ7Fy5UqcPn0akydPxsiRI7Fz506ZVApWeJ49e4YWLVpg7NixWL58Odq2bYuAgAC0bNkSv//+O3JyctC7d2+MGTMG6enp8PHxwa1bt/g5XgiCg4PRokULxMXFfdf38LWRj+fPnwP4sjSXtD4KDAzEypUr8dtvv3GAXdIUfed58ZOWlkZTpkyhJk2a0LFjx8Tt0uGycXFxpK2tTZ07dxb38WyL8tOuXTuqXbs2D0VWsLy8PFq8eDEJgkBaWlrUo0cPKlu2LLVs2ZKmTp1K169fp7y8PHJycqLGjRtTWFgYZWRkKLrYJdrXQ/QWLlxIvXv3Fv+9dOlSqlixIs2ePZuePn0qbm/evDk1bNiQh8rKWf56Pzc3V5xFnIdSKl5YWBgJgkDm5uZ08eJFmX2enp6kpqZGW7dulVlmkBWOJUuWUKVKlcjDw4PS09PF7WvXrqXatWvTmzdvxG379u2jxo0bk7Ozs0yKEvt+a9eupTJlytCBAwcK7OM6q2h86zzv27ePVFRUaMGCBeTv70/6+vp08uTJAsfFxcUVRRFZIeIg+//duHGDevfuTa1bt6aDBw+K26WNqPxT5XOA/X2k5y//A9XR0ZFzfhXg69/yu3fvyM/PjwRBoAMHDtCNGzcoKCiIrKysqEqVKlS/fn1yc3MjQRCoZs2adPToUQWVvOQbN24cnTlzRuYajR07lnx9fWWO+6tAu1GjRlSnTh06f/48N6jkjHN+i5/c3Fyxbtq+fTsRyTZwZ8+eTYIg0JEjRxRVxBJPer4zMjLI39+fmjRpQhMmTCCiL8s86urqko+Pj8yxRF/WzH758mXRF7gE27BhAykrK8u0Z4mITp8+rZgClUL5l0GNjY2V2bdnzx5SV1cnQRAoPDy8wGc9PT3JxcWFOzJ+cKUqyP5fwfGFCxdo6NChZG5uToGBgTJrnUo/ywH293v+/DkRFTyXnTt3JktLSw60i9ijR49o5syZ4r8TEhJoypQppKysTIcOHSIiovT0dHr58iUtWLCAJk6cSFpaWqStrS0T1DH5qlq1KlWrVo0iIiLEF1J9+vShX375hYhk75+lS5dS5cqVaeLEiTIP8ypVqpCNjQ0/qL9T/nPt5+dHhoaG5ObmRvb29qSurk7btm3jHtIilP96fP0cGTp0KGlra3+zZ2jdunUyz3VWOD5+/EgSiYR8fX2pRYsWNGzYMDIxMaGJEyeKx+Rfm5nJV2hoqPiiPL+uXbuSubm5zBrxrHBIf9vSNeBbtmwpPrulDh06RGpqauTp6SnT5p07dy4pKSnR9evXi7TMTP5KTZC9Zs0a2rt37zeHI+V/o/rs2TNav3496erqUr9+/WjSpEmUnJxMubm5RVncEmvPnj0kCAJ1796dJk+eTLdu3aIXL16I+3v27EkWFhY8FLkI7d69mwRBoEmTJonbPn78KAbae/bsKfCZFy9e0Nu3b4uwlKVT69atydLSUhz+2q1btwI92VL+/v7UvXv3Ao3XZ8+eFUlZS4PXr1/TtGnT6MKFC+I2Dw8PUlVVpS1btnAAVwTyP683b95MU6dOpRUrVsj0hA4aNIh0dHTo1KlT3/wOvk7y9ejRI5ozZw4REe3du5caNGhA79+/p4yMDPLx8SFLS0uysrISRwTy+S9cBw8eJEEQxGtCRNSrVy9q2LCh2MnBCk/+WcRNTU3Jy8uLIiIixP3v3r0TX8ru3r2bVFRUaPr06URENGfOHCpTpgzdunWr6AvO5K7UBNktWrQgCwsLOnz48D/qJX358iVt3LiRfvrpJ/L19ZUZLs7+m6ysLJozZ46YN+fs7Ew6OjpkZWVFI0aMoMOHD1NGRgZ16NCBmjVrxoF2Efn8+TNt376d1NXVafz48eL2xMREmjZtGikpKdG+ffuI6MvDg3sfCteJEyfI19dXXJ6rWbNmZGFhQTdv3qQePXpQUFCQOPwsNjaWEhMT6d69e0T0ZwAikUi4IStnnPOrePkD7Dlz5lDZsmWpW7dupKqqSt26daMTJ06I+wcPHkz6+vr022+/KaKopYr0RW3Pnj1JEATaunWruC8zM5MWLFhAzZo1o8mTJ4s52vwcKVzSvF9vb2/q3bs31a1bV+zQ+Lpjicnfs2fPyMzMjKZNmyazffHixdS0aVM6f/682Hm3e/du0tDQoJo1a5KWlhbdvHlTEUVmhaDEB9n5e6C7dev2j/J+v678P3/+XGjlK23evXtHCxYsICUlJTp+/DjFxMTQtm3bqFWrVmRpaUm1a9em3r17kyAIZGFhIdNjxL7fX6U9pKWl0bZt26hMmTLfDLTV1dVpx44dRVrW0mjz5s1UsWJFcnV1pUuXLonbGzduTGZmZmRmZkaCIFDDhg1JX1+fdHR0yNzcnBwcHMRjOf+6cHDOb/ERHR1NPXv2FPPhHz58SDY2NtSlSxc6fvy4eJyTkxN17NhRUcUsVcaOHUuCIJCTk5O4TfrCKTMzk+bPn0+tWrWiESNGcJuqELx584YiIyNl6qQ9e/aQhoYGlSlThh4/fkxEsnWWvb292IPK5Mvf3586duxIHz9+FLd5e3uTvr4+WVhYULVq1ejixYtijBISEkKVKlWSWTee/fhKXJD9vxqYDg4O/3iCLW6sysf+/ftpzJgx4r8TEhLI3d2dlJWVKTQ0lIi+PIQ/ffpEwcHBNG/ePCpfvjzZ2tryMP1CEBMTQ7Nnz6b9+/fLpELk5ubS1q1bqUyZMjR27Fjx+MTERHJ1dSUDAwNKSUlRVLFLvF27dpGmpibt2bOHPn36RESyLwkdHR1JRUWFVq5cSY8fP6YHDx7Q7du36dGjR3yfyBnn/BZfq1evpnbt2lH79u1lGrBRUVHUtGlTcnJykunR5h7TwpO/jeTv709Dhw6lMmXK0NSpU8Xt0nZWZmYmeXl5Ubt27XjWZDnbt28fde7cmaytrSkkJERmX1hYGKmqqpKHh4dMuqSTkxNVqVKFR94Uko4dO1Lfvn3FfycmJtLAgQPFjqNWrVpR1apV6cyZM+J9xLnyJU+JC7KlFi5cSJ6ennThwgVKTk6W2SetXMLCwnjJiEKWl5dHmzZt+sucXyUlpW/m/L5//15sHHEAIT/JyclkbW1NgiCQIAjUrl07atWqFe3du5fu3LlDRF+GLunp6ZGbm5v4uaSkJHr//r2CSl3yxcfHU9u2bWnVqlUy21NTUykiIoIePnxIRH/Own/16tUC38H3iXxwzm/xdurUKapUqRIZGBjQuXPnZPZFR0eTra0tNWvWTOYe4UBb/qT3ybVr1yg8PFx8Afvrr7+SmpqaTKBN9GXCU4lEQh8+fCjyspZkGzduJAMDA9q+fTs9ePBA3B4TE1Mg79fT05Oys7OpS5cuVKNGDXE/11fylZ2dTR06dKBBgwYR0Z/P5q/THw0NDWncuHFFXj5WdEpkkB0RESEGEW3atKHy5cvThAkTaP369WJl0rdvX6pTpw7n/RaBf5Lzu3//fiL60hjK38jlxpH8LVq0iNq0aUNOTk60dOlSGjduHNWvX5/KlClDvXv3ptGjR5OHhwcJglAgn4gVjvj4eLKyspJZbmXNmjVi6oSRkRF1796diL68IdfQ0KDIyEgFlbbk4pzf4uWv6v+IiAiysLCgvn37Fhheefv2bRo5ciQ/OwqR9D45cOAAlStXjvz8/MTc3uzsbNq+fTuVKVOG3N3dKSUlhebOnUtNmjShpKQkBZa65AkPDycDA4MCvdf9+/cnKysrunDhgkzer7q6Oqmrq5OVlRUH2IVs6NChZGJiInZO5K+PcnJyKC0tjfr370/BwcGKKiIrAiUiyM4/2Q/Rl7xfHx8fUlFRIT8/P9q5cyf179+fdHV1qXbt2uTk5ES7du0iQ0NDatmyJe3atYsrGjn7eoRAamrqX+b8Tp8+ndTU1OjXX38t6mKWePkr9vwBxOLFi6lDhw40YsQIys7OptzcXDp58iR5eHhQo0aNqGrVquKLqoSEBE6dKGTx8fFUqVIlGjlyJJ05c4Z69epF9erVI1dXVzp58iTt27ePzMzMaPXq1URENHLkSO65LkSc86t4+euuS5cu0eHDh+ny5cviyLTTp0+ThYUFOTs7/2UeIwfahef06dOko6NDGzZskHneS58VISEhpKKiQnXq1KFy5crxZE5yJD3H48aNoyFDhsh0FHXs2JHq1q1LDRs2JEtLS4qIiBCfFbt27aIOHTpwgC0n36pfpOc2KiqKTExMyM7OTub6SK+dl5cX1ahRg2d7L+FKRJAtlb+XIT4+nmbMmEEqKirikL64uDg6ePAg9evXjzp37kyampokCALNmjVLUUUukQ4cOEBDhw6lzZs307Nnz2RyfrZs2ULq6uoFcn5Hjx5NrVq1UkRxSyzpA+DVq1e0ceNG8vPzo+joaHH/kiVLqFmzZjRixAh68+aNzGdv3LhB27dvp/v37xdpmUuz06dPk66uLlWtWpUaNGhAZ86cEYdWJiYmUsOGDcnT01PmMxxoyx/n/BYv06dPJwsLCzI2NqaaNWtS/fr1xVmST58+TVWrVqVBgwZ9M4WCFR43NzdydnYmIqL09HS6fv06ubm50dSpU+nGjRtERPT48WPavXu3TKoFk4+0tDSytLSUeSa8e/eO3NzcxHqrdevWZG5uTr///nuBz3OALR/379+nFStW0OXLl2W2p6en05o1a0hHR4eaNWtG4eHh9Pr1azp16hSNHz+etLW1eZKzUqDEBNlRUVEkCAK5urqK2z5+/ChOsLV7926Z4z98+EBXrlzhoRpy9vTpU6pQoYLYC9qqVSuqU6cOrVy5ki5cuEDp6em0Z88e0tPTo8mTJ4ufS0lJ4d5SOZI2/KOjo6lu3bo0bNgwCggIKHBcUFAQtWjRglxcXHgymmIgPj7+m0uqJCYmkp2dHa1bt46IeFLGwsQ5v8XHunXrSF9fny5dukSxsbF05swZ6tChA1WoUIFev35NRETnzp0jDQ0NmjdvnoJLWzpI6x43NzdydHSk3377jQYPHkwODg7UoEEDcnBwoObNm/McHoUsJSWF6tWrJ6Z0fWvlkKysLNLT06NffvlFIWUs6dLT06lBgwZUrVo16tevH3Xp0oVu3bpFCQkJRPTlGu3evZvq1KlDSkpKpKSkRDVq1KBWrVpxulcpUWKC7M+fP9OWLVtIS0vrf671S1SwB4gbSfKzZMkS6tixI/Xq1YtCQkLIy8uL7OzsSEVFhTp16kQ9e/ak0aNHkyAI5O7uLvNZDh6+n/Qc3r9/n8qVK0ezZ8+WmRV8x44dYg480Z+B9qhRo+jdu3dFXl729+Lj48nJyYmaNWvGPddyxjm/xVdeXh5NmDBBZmUKoi+9o61bt6aePXuKwzDv3LnD90Yh+tZz+dixY9SoUSMyNDQkZ2dnOnz4MBF9Ga1mZ2fHy3QVgR49elC1atUoMTGRiArWZy9fviQHBwcKCwtTRPFKhalTp1KDBg3oyZMn1L9/f7K3tyc7Ozs6cuSIuEoIEYlpXw8fPpQZIcVKth8yyP6rQCw9Pf1v1/pVVVUVA20O5uQvf97J4sWLqW3btjRq1ChxuPiVK1doxYoVZGdnR3Xq1CFBEMjS0pKvRSFISkoie3t7Gj16tMyDNyAggFRUVKhKlSq0a9cucfvSpUvJysqK3NzcOHgoJhISEiggIICcnJzIxsZGvI84mJAPzvktXqKjo+nMmTMy64y7uLiQjY1NgWMXLlxI9erVK7ByCN8b8id9Pl++fJnWrFlDnp6e4koUSUlJ4ozW0uNmzJhB7dq1kwkwmHxJl0W7dOkSlS9fnlq3bk3p6enifolEQmlpadSlSxeyt7fn+6IQSOv+Fy9eUPfu3cVnRFRUFC1ZsoQEQaAOHTrQvHnzKDs7m5dKK6V+yCBbKiIiQsz9kUpPT6etW7eSmpqazNT40gm2BEGg69evF3VRS7Rz585RQEAATZs2jZ48eSJul+b8Dh8+vEDO77Nnz+jEiRNiXhAH2vL1xx9/kKWlpcwkTTt37iQTExNavXo1jR07lurUqSMzK2lwcDBPwlGM3Llzh7p27UqTJk0S7xPOo5M/zvlVvO3bt1PTpk2pV69etG7dOjEoCAkJoYYNG9LOnTtlJtcKCwuj+vXr88ibIrJv3z7S1tamVq1aUY0aNUhPT4+8vLxknhc3btyg6dOnk46ODt29e1dxhS2Btm/fTj4+PjIvoIi+TCi7evVq0tXVpcaNG9P27dvp6tWrtGnTJmrfvj3VqVNHDO74heD3+bqNKv13eno6OTo60rBhw8R9rq6uVKFCBfL29iZjY2OqUqUKeXh4kEQi4bZuKfPDBtkrV64kQRBISUmJBgwYQJMmTaI//vhDnCho+/btpKurK5Oj/eHDB5neO/b9tmzZQtWqVaMZM2YUWEaCSDbn969ytDhwkL+9e/eSpqamTE/PnTt36NatW0T0ZSj56NGjydDQkK5du6aoYrL/ISkpSXwoc2+E/HHOr+Jt27aNNDU1affu3eI5l0pNTaXu3btT69atac2aNfThwwd6+/YtderUibp27coN1iLw6NEjqlixIm3evFnsQV28eDE1aNCA5syZQ6mpqRQTE0M9evSg5s2bc66pnL19+5b09fXJ3t6eevToQfb29nTq1CnxBVNqaiqFhYVRo0aNSENDgwRBoCZNmtDAgQP55aycSF9QvH//XmbyWGn9c+vWLbK0tKTbt2+Ti4sLmZiYiMclJSWRl5eXTAcUKz1+2CA7KCiIateuTQ0bNqTevXtTly5dqHz58lS9enWaNWsWbdiwgYKDg0lFRYVmz55d4PP8Vu/77dixgzQ1NenAgQPiw5eIyNPTU+ZlRlBQELVs2ZJGjhxJb9++VURRS51bt26RsrIybdmy5S+P2bp1K9na2lJsbGzRFYz9JxxMyB/n/CpeVFQUVa9evcAEpBKJRDzfycnJ1L9/f6pXrx6pq6tTgwYNyNramnvoCklsbCzt3r2bQkJCKDo6ml68eEHm5uZ0584dmXpo4cKFpK+vTzExMUT0JRjnkQWFw9XVlezt7SkuLo5GjhxJnTt3poYNG9Lu3btlRglGRkZSREQExcfHi9eKA+zvI61f7t+/T61btyZHR0e6d++euF8ikVBSUhINGjSITE1NqXr16uJoWX5msB82yCb6kkfatWtXGjp0KH369Inu379Pq1evJjs7O6pevTpVrFiRypQpQ4IgUGhoqKKLW6I8evSIGjVqRMuWLZPZ3qtXL1JXV6datWrJ9GwvWbKEatSoQf7+/kVd1FLp5cuX1LhxY2rTpk2B9Ajpw3f69OnUo0ePAnmNjJVEnPNb/Bw8eJAaNGjwl2kq0kA6KyuLXr58STt27KATJ06I14EDCPmKjIykqlWrkpWVFSkrK1OtWrXI1dWVqlWrJi7nmD/318zMjBYvXqyo4pZ40gDvzp071K1bN7E3NDExkXx9fUkQBGrWrBlNmzaN3r9/L5NSkf/z7L+RtpWio6PJwMCApkyZQleuXCmwn+jL6FlBEOjYsWNFXk5WfCmhmCMi8b+jo6MRERGBmzdvAgAmT56M9u3b448//sC0adNgYGCAcePG4dixY7h58yZmz56N8ePHY+DAgfj5558V9SeUSK9fv0ZCQgLatGkjbps7dy5evnyJ0NBQtGnTBv7+/ti5cycAwN3dHQsWLMCMGTMUVeRSpXLlypg6dSouXLgAHx8fnD9/XtyXmJiI6dOnY8OGDfDz84Ourq4CS8pY4duxYwdGjBiBNWvW4M2bN8jLywMAdOzYETk5OQgJCUFWVpZ4fI0aNSAIAjIyMmS+R1lZuUjLXdJFR0cjOTkZZmZm39yvqqqKp0+f4vjx46hcuTIGDhyITp06QVlZGXl5eVBRUSniEpdcUVFRsLW1Re/evXHq1CkcOnQIlStXxs2bN5GRkQFnZ2cAgKamJgAgJSUFhoaGMDU1VWSxSzRBEAAA1atXR2JiIoKCggAAurq6OHToEFq2bIlBgwZh9+7daNiwIVauXCnzeSWlYt/EL9YEQUB8fDz69++PkSNHIigoCM2bN//msYMGDYKjoyNOnDiBnJycIi4pK7YUHeX/nfxviQICAqhjx440YMAAOnXqlMxxQUFBZGtrSy4uLn87XInf6n0/6TVZuHAhmZiYyOy7d+8excfHExHRw4cPaeDAgWRiYkKPHj2SOY57gwpX/vtm48aNpKOjQ2ZmZjR8+HAaOHAgdevWjSpWrPiXMyYzVpJwzm/xtXz5ctLQ0BDTiL61tObs2bN5BFQhe/XqFRkaGlKfPn1ktgcHB5OOjg4dOHCAmjRpQvXr16erV6/SpUuXyNvbm4yMjOjZs2cKKnXJdOvWLZkJfaXt1t9//50aN25M58+fp4YNG1Lr1q0pKSmJiL6sx7xmzRpuWxWCy5cvU7NmzWTS6m7fvk2rVq2ili1b0sSJE+nChQtERLRgwQKysLDgtEgmKtavuaRv8WbOnInVq1fD09MTCxYsQIcOHSCRSHD58mUAwJQpU9C7d288evQIs2fPRnx8PABAIpGI30VE/FZPDqTXpH79+khISMDevXvFfXXq1IGRkREAoFatWmjcuDGsra1RsWJFme/g3qDCJQiCOAJkxIgROHjwIHr16oU7d+7g3bt3aNSoEc6fPw9ra2sFl5SxwhUdHQ1fX18EBQWhX79+qFSpEoAvz4O8vDxoaWnh119/hampKYKDg1GpUiU4OjoiISEBoaGhEARB5jnC5ENaP/Xu3Rva2toYPXo0gC/Phvy9QOnp6YiOjoa+vr5Cylla5OXloUqVKsjKykJERIS43dLSEhoaGqhSpQo2bNgAIyMjdO/eHYMHD8b+/ftx/PhxVKlSRYElL1l27tyJ4cOHIyAgAC9fvgTwZ2909erVUaFCBXTs2BGVK1fGnj17oKenh7y8PGhra8PV1VUc4cHkJy0tDffu3cOzZ88AABs2bIC7uzvWr1+PSpUqYe/evZg9ezaysrIwatQoaGpqIjs7W8GlZsWGYmP8/239+vVkYmIiviki+vJmr02bNmRgYCAzwdaSJUuoZcuW9PPPP/MajYXs4cOHVKtWLbK3ty+wjBoR0efPn6l79+4y65Wzwpd/tAb3wrHSjnN+i49vjST7/Pkz+fn5kYaGBv38888yPXGvXr2iLl26kK2tLffQFYGYmBhycHCgTp060YMHDyg1NZWMjIxo2rRpMsfdvn2bHj169JerhbD/ZsuWLaSlpUWbNm0SJ5Mjkr1vNmzYQMrKyuI65azwvXz5krp3707m5uZka2tL6urq5O3tLc518/jxYxIEgQ4cOEBEsnMWMCYQ5Ut6LkYkEgny8vIwYMAA1KhRAwsWLICysjKICLa2tsjLy0PdunVx7do1eHt7Y8CAAQAAX19fEBG8vb0V/BeUfDt27MCQIUPg6OgId3d3dOjQAZmZmXj16hXc3Nzw/v173Lp1CyoqKiAisRecfT/p+bx58ybu3r2LjIwMtGrV6i97pyUSCQRBEHu5+Vqw0mDBggXYtGkTnj59+pcjaJ4+fYr79++je/fuMtvz8vJ41I2cSCQSsUfu8OHDiI2NReXKldGkSRMYGhpiwYIFWLJkCXR1dWFvb4/U1FS8efMGRIRLly5BVVWVr0cRePz4MSZNmoTPnz8jKioKQ4cOxdKlSwEAOTk5UFVVVXAJS6Zbt26hZ8+e+OWXX9C/f3+ZfampqdDW1gYAfP78Gd26dYOdnR3mzJnDozMLibSNlJKSAh0dHVy/fh2XL1/Gs2fPMHz4cNSvXx9KSkqQSCR4/Pgx+vfvj9WrV6NFixaKLjorZortrCFKSkr49OkTzp8/DwcHBzHA/uOPP9C2bVv4+fnh4cOHWL9+PTw8PKCmpoZevXph9uzZ4ndwMFE4pOd10KBByMnJwcyZM9G/f380b94cycnJYkB38+ZNqKiocOOoEAiCgAMHDmDChAmoUaMGtLW1MWnSJGzatAnDhg0r8LvP/zDme4KVFrq6uoiPj0d8fDxMTEwK1EUSiQRbt26FpqZmgSCb6yz5kdY/Hh4eWL16NSwtLfHy5UtYWVnB3d0d8+fPh6OjI1atWoW3b9/CyMgIAwYMgJubG1RUVJCbm8uTnBWB6tWrY/ny5Rg7dix0dHRkJozl8194nj17BjMzM/To0UPcdvToUZw8eRJhYWFo06YNhg8fjrZt26JBgwbYunUrJk2ahHLlyimw1CWTtH179OhR7Nq1C2PGjEGrVq3QtGnTAscqKSlhx44dEAQBVatWVUBpWXFXrF+DqampQUlJCbGxsQC+BAe1a9eGn58flJWVUbduXfTt2xfq6upITU2V+SwH2PKVf8BD/pxfFxcXHDx4EFOmTEFubi7q1auHESNG4MKFC1BVVUVubi43VgtBVFQUxo0bh7lz5+L3338XZxV98uQJ/+5ZqUec81ss5M9nv3nzJk6dOoUTJ07g7t27OHv2LOrWrQs/Pz/s27cPzZs3x44dO3D06FGEhIRg8uTJ4ktaDvCKTvXq1bFu3TrUrl0b/v7+uHTpEgB+OVuYcnNz8eTJE0RHRwMAJkyYgICAANy+fRvDhg3D77//Dl9fXwDA+PHjUb9+fV4VpJAIgoDQ0FD069cPNWrUgImJyTd/+9HR0ZgxYwZWrFiBLVu2oEKFCgooLSv2inyA+j+Ul5dHGRkZ1Lt3b6pfvz5dvnxZ3JebmyvmqURHR1OrVq3o7NmziipqifRXM7H/m5xfzqMrPOHh4eTk5ERERM+ePaNKlSrR2LFjxf1v3rxRVNEYUwjO+S0+vl65IDAwkIYNG0YDBgyQOdf379+nPn36UJ8+fSgrK6uoi8n+RkxMDHXt2pWaN28uszYwk4/87aeHDx+So6MjGRsbk5mZGZmbm9PGjRvp1atXRER05coVEgSBIiIiZL6D6y35e/DgAZmZmdGmTZvEbRKJhB49eiTO5r5y5Urq1q0b2djYUGRkpIJKyn4Exfb1sJKSEtTV1TFkyBD06dMHgYGB8PLyQrNmzcSe0efPn8PZ2RmNGjVCu3btFFzikiN//tyuXbvw6NEjZGRk4Oeff5ZZIzD/2z0iEmdwp/8fRcA92IXn48ePePv2LaKjo9GtWzd06dIFq1evBgCcPn0aO3bsQFBQEAwMDBRcUsYK39/l/M6YMQNZWVlYsmQJzM3Nv5nzK52Vl+us7zdkyBCULVsWwcHB4raMjAxs27YN5ubmiI2Nhbm5OQDAysoKvXv3hrOzM16/fg1LS0tFFZt9pXr16li0aBG8vb15LWw5y19fvXv3DrVq1UJAQAAePHiAd+/eYdSoUWIeNhEhMzMTjRo1Qvny5WW+h+sr+UtPT4eRkRHs7OyQkZGBzZs3Y+/evYiNjYWZmRn279+PFi1awMzMDDY2NnxvsL9VbINsqW7dumHVqlVwdXXF27dv0a9fPzRo0AD37t3Dxo0bUbVqVWzduhUADxGXF2nlP336dOzbtw82NjbQ0tJCixYt8Ouvv2LQoEEFPiOdVEv630x+pL/rp0+fQkNDA6ampmjSpAl0dXXRunVrdO/eHevWrROHyB4/fhxJSUn8AGalBuf8Fh/+/v5iMPDixQtYWFhg3rx5MDY2xvjx47Flyxa4ubnB0NAQAFC1alVUr15dZhg/Kx5q1aqFnTt3Qk1NTdFFKTHyB9g+Pj54+PAhJk2ahGbNmqFBgwYFjs/KysKyZctQqVIlfglVCKTtq/T0dGhoaEBJSQnx8fHw9/fHhQsXUK9ePbRq1Qr16tXDnDlzcPLkSTg7O8Pa2prbuux/U1gf+j+QfzjNgQMHqHPnzqSlpUVlypShNm3akIeHh7j/r4Y3s/8mLCyMTE1NxWUKjh49SoIg0I4dOxRcstJFeg+EhYVRtWrVaNOmTZScnExERF5eXmRkZES+vr4UGxtLT548oZkzZ5K+vj5FR0crstiMFYn89f6NGzeoUaNGdOnSJSL6MmR59OjR1LBhQ9q7d6943NfDknnIpfzkP5fr16+npk2b0smTJ8VtCxcuJEEQaNKkSXTmzBmKiooiBwcHatKkCT/DWakyc+ZMKl++PO3Zs+eby6GlpKTQ6dOnydHRkerWrSsuN8j3ifxI21dHjx4lFxcXunXrFhERbd++ndzc3Mjb25uePn0qHm9rayuzbDBj/4vCg+xvVRj5g+v8/52cnEzv37+nR48eUUpKyt9+B/s+q1atokGDBhER0b59+0hLS4vWrVtHRF+uw1+tO8vk77fffqOyZcvS8uXLKTY2Vmbf5MmTqX79+qSqqko2NjZUq1YtXkOTlXic81v8fD1HR0xMDFlZWZGjoyOdPn1a3L5o0SISBIEEQaChQ4dSr169xPXI+VnOSoPw8HCqVKmSWI/l5eXR+/fv6cqVKxQfH09ERLNmzaLOnTtTz549xftD+v+Z/Bw4cIC0tbVp1qxZ9PDhQ3H71+d69uzZZGZmxm1f9q8odHxc/hy4qKgoKCsrQ0tLS8zXAmSHHuvo6EAQBJm8FPr/PGD23+UfviSVm5uLpKQk7N27FyNHjsTChQvFGXqPHDmC33//HYsXL+YZLgtZSkoKFi1ahClTpmDixInIzMxEfHw8Dh06hNq1a2Pp0qV4//49rl+/jsqVK6NChQowNjZWdLEZKzSc81s8SZ/VPj4+qFmzJvr164fffvsNP//8MxYuXAgAsLe3x7Rp06ClpYVx48bB2toaw4YN46UeWYlGX6Uy5ubmwtTUFCYmJrh//z52796NHTt2QFVVFYaGhggPD0f//v3h4OCAFi1aQElJiVNaCsH9+/fh5uaGpUuXYsSIEeL2V69eQUdHB3p6eti4cSOuXLmCo0ePIjw8HBYWFoorMPvhKOyOlUgk4gN16NChuHv3LrKysvDp0ycsWrQIAwcOLJDv8K38B86J+D75A+yLFy/C3NwclStXRuPGjbFjxw4MHToUvr6+cHV1BfBlUohdu3bB3NwcOjo6iix6qUBEyMvLg7GxMZ49e4b169fj+vXruHnzJqpUqYL+/fvD09MT3bp1U3RRGSsSnPNbfISGhqJ9+/bQ09MDEeHTp084ePAgtmzZAuDLuT948GCBQHvs2LFITk6Gu7s78vLy4OLiwmv+shIpfxsrISEBhoaG0NLSQmxsLIYPH44bN26gW7dumD17NoyMjODu7o67d++iTZs2Mt/BAbb8JScnw9zcHF27dkVqaipCQkKwZ88evHr1Co0bN8bq1atRqVIlKCsr4/z586hZs6aii8x+MArrApZWOkOHDsX169cREhKCq1evwsLCAl5eXoiPj1dU0UqN/KMAZs2ahREjRuDmzZvIyspCq1atYG9vD319faSlpeHOnTu4dOkSevXqhdjYWCxfvlxmvWxWOHR1dWFubg5/f3/Ur18fT548gbOzM16+fAlLS0s8e/ZM0UVkrMjk5eWhUqVKUFNTw4YNG9CvXz+cOnUKAODq6opffvkFPj4+8PX1xdmzZxEdHQ1vb29oaWmhRo0aCi59yXL06FH07t0ba9euRUpKCgRBgKqqKpKTk5GVlQXgy/WSBtrv379HUFAQjh07BuDLJHWLFy/GtGnTsHPnTn6WsBInf4Dt6+uLadOm4d69e2jbti1WrlyJFi1aIDg4GIsWLcKIESPQokULaGtry6wvD4BHaxYSiUSC69evY/78+WjSpAmOHTuGpk2bYurUqbhx4wauXr0KBwcHrFixggNs9t8ocKg6xcfHU9u2benGjRtERPTLL7+QkZERhYeHExFRRkYGEf3v9ZjZ95k7dy4ZGxvT2bNn6dOnTzL7PDw8yMbGhgRBoObNm1Pnzp3FCTh4wiD5kv7O3759S7GxseIamURE+/fvp0OHDlFubq543l1cXGjMmDGUm5vL9wgr8Tjnt/hZtmwZKSkpUUBAAKWmplJmZiZZWlrS3bt3SSKRUF5enlhfPXnyhExMTGjSpEkyz44VK1bQ/fv3FfUnMFboZsyYQRUqVKDt27fTmzdvCuzPzs6mpKQk6tKlC7Vs2ZLbVoVA+vxISEigt2/f0ufPn4noS9tqwIAB5OnpSTExMeLxNjY2tG/fPpnPMvZvFWmQ/fUP9eHDh6SpqUlv3ryhVatWUbly5ej48eNERPThwwdyd3enFy9eFGURS53Y2FiytrYWZ0yMj4+n27dv0+zZsyksLIyIvrzsuHbtGsXGxoqNVJ6AQ76k98ahQ4eoefPmZGZmRvb29uTn51fg2Pfv35OXlxfp6elx45SVOvPnz6fdu3cTEdHTp0+pfv361KlTJ5lAOzg4mARBoGXLlomz8XPDVX7S0tLE/162bBkJgkA+Pj4UHR1NNjY23wwkiIji4uLE68DXg5UGR48eJRMTE3HmaqIvgV5UVBTFxcUREdG8efOoQ4cO1KRJE+7EKAT521fW1tZUu3Ztql69OgUGBtLHjx8LvHydNWsWmZub08uXLxVRXFaCFFmSR/5JTVJSUqCjo4NatWqhW7duGDlyJC5fvoywsDC0bdsWABAbG4tbt27h8ePHMhOhMfnKzc1Feno68vLyEB4ejr179+LevXtISkrCwYMH8fr1a7i5uaFp06biZzg/SP4EQcDRo0fh7OwMX19fNG3aFMeOHYO3tzcyMzPh4+MDADh27BgWLlyId+/e4dy5c7CyslJwyRkrPJzzW/ycPHkSkZGRaNWqFWxtbTFp0iQIgoApU6bg3bt3ePbsGVq2bIlq1apBTU0Nnz59QlZWFpydneHu7g4APMkZKzXS09NRrVo11KpVC9HR0QgNDcW2bdugrKyMunXr4tdff0XTpk0hCAK8vLygoqLCk5zJmSAIOHXqFJydneHj44MhQ4bAx8cH8+bNg5WVFbp27QoA2LRpE86fP48TJ07g+PHjqFy5soJLzn50RXIX53+gjh8/HiYmJhgyZAgqV64MKysrBAUFYdCgQeJEDy9fvsTgwYNhbW2NDh06FEURS4VvzSJubm6OBg0awMvLC3FxcZgwYQL8/f1hb28Pe3t7JCQkFPgezg+Sv9jYWAQFBSEgIAATJkzAhw8f0L9/f7Ro0QLLly+HRCKBr68vunTpgg8fPsDOzg5VqlRRdLEZKzTSnF9/f3+MGzcOOjo6f5vz27NnTwQFBSErKwtdunSBh4cH1NTUMG3aNKipqWH8+PE8UeZ32rJlC7y9vdG9e3fxhTgATJw4EYIgYNKkSWjWrBk6duwIMzMzAMDHjx+hrq6O8ePHi8dzgM1Kom+1scqWLYuIiAgMGjQIly5dQufOncVget68eYiJiYGjoyMcHR0BfKnTOMCWHyKCRCLBzp07MXr0aEyZMgVxcXE4duwYhg0bJjNprKmpKfLy8vD777+jdu3aCiw1KymK5E6WPlB//vlnxMTEIDAwENra2gCAOXPm4N27dzh9+jSsra1hbm6Ox48fo3r16ti2bRuAgssfsH8vf+UfHR0NIoKysjLq1KmDvXv34vfff4e+vj7q168v8zk1NTVFFLfE+tZDGAAMDQ3RunVrODk54d27d7C3t4eTkxMWLFiAKVOmwN/fH+np6Vi6dCmGDBmigJIzVrScnJywdOlSTJkyBQDg5uYGVVVVKCsrQ11dXXwuSAPtAwcOwM7ODidPnkTnzp2hrKyMKVOmQFVVFe3bt+dnyHfavXs33NzcsGXLFjg4OBRYXWLChAlQUlLChAkT0KdPHwwZMgRlypSROYZ76FhJlf/ZHhMTg7S0NFStWhVdunTBsWPHcPr0afTt2xft27dH+fLl8fHjRyxbtgzp6eky38MvoORLEAQoKyvj48eP6Nu3L1JSUtC4cWN07dpVXAZy//79qFy5MhwdHdGuXTuoq6sruNSspBCIimZKz8DAQGzevBnXr1+Hnp4egC9r0WlqasLQ0BBnz57F2bNnoaqqiipVqoiBxF8FJeyfy/+SwtvbG4cOHcL79+9Ro0YNdOrUCd7e3uKxqampePPmDaZOnYrXr1/j9u3b3CiSE+lv+dWrV7h69Sri4uIwevRosULPzs6Gmpoa/P39cfXqVWzZsgUGBgbw8/PDzp07IZFI8Pvvv8PY2JgDBlaipaeno2zZsgCA5cuXw93dHfPnz8fPP/+M4cOHIywsDKampgU+9/79exgaGkJZWZmHJMtRQkIC+vbti969e8v0SKelpeHBgwfIyclBy5YtAQCLFy+Gh4cHpk+fDi8vL/GFOmMlVf421uzZsxEaGorPnz9DVVUV/fr1w+TJk8UlBXNzc5GZmYm+ffsiNTUV58+f5zaunEmvR1JSkpgmNGjQIDx//hxv374VX+CqqqoiIyMDQ4YMQZMmTTB9+nS+Fkyuiix6SklJQatWraCnp4eLFy/i5MmTWLVqFapWrYrOnTvD19cX7du3l/kMB9jyIa38fXx8sG7dOuzevRsWFhYIDAzE3LlzkZmZCT8/PwDAoUOHsHTpUpQrVw63bt2CiooKN1blQPpbjoqKwk8//YRy5crh2bNnCA4Oxu3bt6GhoSGOGoiMjER2djYMDAwAfBluOXz4cIwZM4YbrKzE45zf4ik+Ph4VK1YU/x0cHIyzZ8/iwIEDMDU1hbm5OSIiIjBt2jRkZWUhPDwcWlpaCiwxY0VD2sZatGgRNm7ciB07dqBDhw7o06cPNm7ciJ9++gmGhobIzs5GQEAAzp07h/T0dFy+fBlKSkrc1pUjaYAdHh6OdevWYcSIEejWrRvc3d0xcuRIKCkpYdWqVeLxvr6+uHnzJgIDA/kaMLkrlCA7/1u9/P8dGhqKtLQ0REZGokWLFliyZAnu37+PEydOYOrUqWJQIcU/ePm5ffs2Tp48iT179qBdu3Y4ceIEdu/ejb59+2LlypVQUVHB/PnzMWjQIOjp6cHR0RHKyso8vE8OpA/QyMhI2NraYsqUKZgwYQJSU1PRrl07HDlyBH369BGP79ixI+bPn49x48YhJycHBw4cwLVr1zjAZiUe5/wWXykpKTh69Ch0dHSwZs0axMTEoFWrVjhx4gQ+ffqEmTNnYsGCBZgzZw5mzZoFLy8vCILA6V6sxCMiZGVl4dy5c5g3bx46dOiAY8eO4eTJk1i4cCGaNGmCnJwcqKmpoWnTpsjIyICvry9PclYIBEFAWFgYBgwYAB8fHxgbGwMA6tSpg7Fjx2Lx4sVo1KgRrK2tkZSUhPPnz+P06dOwtLRUcMlZiSTv6crzLzuQf41MIqI5c+bQqFGj6Pjx4+ISH8ePH6dmzZrR27dv5V0Ulk96ejr98ssvlJKSQmfPniUTExNav349paamkpOTEwmCQK6urjKf4SUk5Ofx48ekrq5Os2fPltnesmVLmjVrFg0dOpRCQkLo7du3lJiYSH5+fmRjY0MdO3aku3fvKqjUjBWdXbt2kaamJu3Zs4c+ffr0zWNWrVpFgiBQUFAQZWZmFtjPSwsWntOnT5Ouri5VrVqVGjRoQGfOnKEPHz4QEVFiYiI1bNiQ5s6dK/MZXl+WlVRf/7ZTUlKoadOm9OTJEzp37hxpaWnR2rVriYgoMzOTVq1aRbdv35b5DLex5O/NmzdUv359CgoKKrAvPT2drl+/TsOHDydnZ2fy8vKiR48eKaCUrLSQ6+uz/EP0fHx8cPfuXejq6qJp06ZwdXXF/PnzxbxTiUSCt2/fYvr06WjevDlMTEzkWZRS7cyZM4iKisK7d+/g7e0NbW1taGpqYsqUKVBRUcGePXvQs2dPcWKaGjVq4PPnz3j79q3MsCXuDZIPiUSCzZs3Q1tbW2a0RmBgIK5cuYLKlSvj2bNnCAkJwfjx47F48WJ4eXlh5syZyMzMFHNTGSupEhISsG7dOixcuBB9+/YVt3+d8zt+/HhkZGRgxowZSEhIKJDzyz1Chcfe3h6PHz9GWlraN1c20NbWLpAnzz3YrCTK30569eoVKleuDG1tbZQvXx4//fQTXrx4gZUrV2LYsGEAgMTEROzduxeampqwtrYWv4fbWPKXlJSEjx8/inNEAH+OqNXU1ISNjQ1sbGwUWEJWmsitRUL/P1s1APTs2RMxMTHo1q0bcnJyMHfuXHz48AHe3t5QU1PDmzdvsHLlSpw6dQpVqlTB+vXrxe/gh/L32bhxI2bNmoV69erhwYMHOHz4MKKjo6GqqgoVFRXk5OQgMjIS1apVQ5kyZZCZmYnXr1/DxcUFgwcPBsC58PKmpKQENzc3fP78Gbt374a6ujpSUlKwZMkSHD16FJ07d4YgCJgwYQI2btyISZMmwcLCAsrKyhxgs1KDc36LPyMjIxgZGclsS0hIgIuLC7KzszFixAgFlYyxopG/feTn54eIiAhMnjwZnTt3xsyZMzFx4kTUrl1bDLBTUlIwYsQIEBGvDFKIpPGDRCKBsrIykpKSChxz7tw5pKamonv37jKfYaywyC2Skv5QfX198erVK5w8eRIBAQHQ09NDeno6AgMD4eHhAQCoWLEi9PT00L17dxw6dAjAl4qLf+zfZ926dXB1dcXatWsRFhaGCxcuIC0tDXfv3gX9/yTy0tkuz549i379+qF9+/Z4/PgxnJ2dAXypdDjAlj9TU1N4eHjAxsYGy5Ytg5eXF3bv3g0HBwdkZmYCABwdHVG+fHlxDWDGShNpzu/Zs2fRu3dvBAcHw8jICCdOnMCyZcsQFxeHBQsWAABmzZqFixcvijm/rOh9+PABgYGBcHFxQXx8PC5evCjO6s5YSSVtH3l5eWHZsmUYN24catSoAQBo1KgRRo8ejcTERNSsWROdOnVC586d8e7dO5w5c4bvDznLX/dL44cqVapAS0sLy5cvR0JCgsy+o0ePYt++fcjIyJDZzlhh+U892d96+5ObmwsAyMnJwdixY2FqaoqlS5di+fLl2Lp1K+7fvw8fHx+ULVsW3t7eYsANcM+pPISFhcHV1RWHDh1Ct27dAHx5mVG2bFls2bIFM2bMQK9evdCrVy8MHjwYgiDgzJkzqFevHlatWsVL3hSBChUqYPbs2VBSUkKZMmVw584dtG/fHhoaGgC+zKpsZGSE8uXLK7ikjBUtIyMjbN26Fb169cLZs2ehra2NZcuWoUGDBjAwMEBSUhJ0dHQgkUjEz/CkWooVGxuLS5cuoVq1aggLC+NJnFipcffuXRw8eBA7d+5Ep06dAHxpF2tqamLo0KFo27YtNm/eDHV1dZiammLUqFE8kaycSev+a9eu4erVq8jLy0OjRo3Qtm1b7NmzB23btsXAgQMxcuRIlCtXDkePHsWWLVtw+fJlsc3FWGH713d7/kbNs2fPkJGRgTp16ogVx8yZM5Geno6oqCisW7cOq1evRp8+faCnpwctLS3MnTsXVapUwaBBg8Tv4wD7+2RlZeHEiROoWrUqnj17Jm4fOHAgUlNToaOjg7Jly2LKlCl48+YNAgICMGnSJEyaNEk8liv/omFsbAxPT09IJBLs27cPubm5mDlzJnx9fbFp0yZcunRJXNeRsdKEc35/LA0bNsT27duhq6sLQRCQl5fHzxBWKiQkJODjx4/fnJFaTU0NtWrVwsKFC2W28/0hX4Ig4MCBAxg+fDisrKyQmZmJadOmYdasWViwYAFu3LgBZ2dneHt7IycnB+XLl8f58+dRp04dRRedlSL/6o7PH2D7+Phg7969iI+PF3+8BgYG0NTUhKamJk6fPg1lZWV07doVwJdhyn369MHw4cNlJiTgRtL3K1OmDObMmYMyZcpg9+7dAICIiAg8f/4cly5dEhusQ4YMwebNmzF16lQYGhqKnycirvyLUIUKFTBr1iz4+fnh6NGjCA0NRVRUFCIiIlC/fn1FF48xheGc3x+Lnp4eANk5WRgrqaSjLlVVVaGpqYmEhAQx0Ja2j/ft2wcNDQ306NFD5rN8f8hXTEwMJk6ciKCgIAwfPhy5ubnYs2cPRowYAUEQ4OPjgzNnziApKQnZ2dkoV66cWF8xVlT+cRdy/gB78uTJWLlyJebPn4+dO3dCSUlJplcU+BJIvHnzBmvXrsWdO3cwceJEGBgYiAF2/mF/7PuZmJjAw8MDTZo0wfLly3H27FkcOXIEVapUwefPnwEArVq1grm5eYFzzy86ip400K5WrRoSExNx5coVNG7cWNHFYqzY4JzfHwc/Q1hJ9HVbSTrqslGjRlBSUoKfnx/i4+PFfdnZ2di5cycuX75c5GUtDfLnYCcnJ6Ns2bKwt7eHIAhQU1PD4MGDsX79evj5+eHKlSvQ1NRExYoVUaVKFQ6wmUII9C9njfHw8MD69etx9epVcbKH2bNnIysrCx07doSpqSlMTEygra2NOXPmIDg4GAYGBmjYsCFCQ0MB8Ix+hen9+/fw9/fHpUuX0L9/f0ybNg3Al+HgXbp0gb6+Pnbt2sXnv5hISEiARCKBsbGxoovCWLFy9+5deHt7w9LSEosXL+acX8ZYkck/V9C2bdtw79495ObmokOHDnByckJUVBTs7e1Rt25d9OjRAwYGBti6dSvi4+Nx584drqcKSWhoKLS0tGBiYoIGDRrgxo0baNy4sfhsSE5ORrNmzeDp6SnO8M6YovyrIPvUqVPo3r07RowYgVWrVonbLS0tIZFIkJmZiU+fPmHw4MEICgqCsrIy3r9/j/j4eDRt2hQAT3JWFOLi4uDn54fr16+jT58+mDZtGrp3746nT58iMjISKioq/KKDMVbsJScny+T88pBLxlhRmj59On799Vc0bdoUGRkZOHv2LGbOnImAgAC8fPkSrq6uiI2Nhbq6OqpUqYIdO3ZAVVWV66tCcPv2bdja2mLp0qUYOXIk+vTpg7S0NKxevRq1atUCAGRmZsLW1hbu7u68ZBpTuH8VZL979w6+vr6IiopCnz59MHHiRDRv3hza2tpYtWoVatSogblz5yIwMBDh4eGwt7eX+TwHdkUnLi4O/v7+uHXrFp48eQI9PT3cu3cPqqqq3BvEGPuh8LODMVbUzpw5g4EDB+K3336DjY0NAGDXrl0YMmQI5s6dK47izM7ORk5ODsqVKwdBELiNJQfSOl/6/x8+fIjQ0FDk5ORg3rx5AL6sqrN27VpkZGTAz88PWlpa2LdvHzZu3Ihr167BwsJCoX8DY/+qS9nExATe3t5o2LAhdu7cCWNjY+jr6+PYsWOoXr06BEHAjBkzoK2tjSdPnhT4PDeSik6FChXg5eWFatWqoXHjxhxgM8Z+WPzsYIwVtq9zsJOTk6Gvr4/atWtDIpFAIpFgwIABWL16NQICAhAVFYUyZcpAW1sb+vr6YlDIbaz/TnoNpHW+IAiIj4/HmDFjsHz5cplr9NNPP2HcuHEwMjJC69atMWDAAOzbtw/Hjx/nAJsVC/963LZ0wqbmzZtDQ0MDzZo1g6qqqjgE/MWLFzAwMICZmZncC8v+nQoVKmDZsmU4cuQIB9iMMcYYY39B2o6dPXs2zp8/j3LlyiEmJgavX7+GkpKSGOC1b98e+vr64qRn+fELwf9Omk767NkzLFiwAO7u7ti4cSPKly8PFxcX6Ojo4NChQ3j37p34me7du2P//v2IjIzE4cOHERERAWtrawX+FYz96T9FXNJeUolEgvDwcKipqcHT0xOZmZlwdnZGw4YN0aVLF3mXlf0H0jWXJRIJB9iMMcYYY/nkT0fZt28fgoOD0aFDB1hbW6NTp06YMmUKli5dKub9li1bFpqamviX8wazvyENsCMjI9G5c2fUqVMHDx48gEQiwa1btxAcHAwlJSWsWLECHh4eCAgIgKmpqfi5evXqKfpPYKyAfz27eH7SCbZu376NDh064MCBA6hYsSJOnDgBgCc5Y4wxxhhjxc/Xcz0cP34cx44dg5WVFcaOHQsAOHDgADZs2ICkpCR4eHhAVVUVa9asQUJCAq5evcqTm8mB9Drcu3cPTZs2hYeHB2bOnIn09HQEBgZi06ZN2LdvH9q3b4+VK1diz549qFatGgICAmBiYsKxBiu2vivIBv6cYGv9+vXo1KkTDh8+DIADbMYYY4wxVjzlnwH8/v376N+/P2JjY7Fw4UKMGjVKPC48PBy7du3C3r17YWVlBSMjIzENj2cRl4+EhATY2dlBS0sLN2/eFLc/ffoUTZo0wapVqzBw4EAAwKpVq7B//34YGBhg9erVqFChgqKKzdjf+u4ouEKFCvD09MTGjRs5wGaMMcYYY8XasWPH4Orqij59+mDZsmWoU6cOZs6ciXLlymHTpk2IjY0Vj3V0dMSvv/6KmJgYnDx5EsePHxfnueEAWz6UlJTQsmVLaGhoICAgQMx/T0lJQXZ2NgwNDcVj3dzc4OTkhM+fPxeYrI6x4uS7e7K/xgE2Y4wxxhgrjtavX49p06ahe/fuuHXrFpKSktC7d2+sWrUKmzdvxrp161CrVi0x7/dbvdXc1pUPIkJeXh5UVFTw8eNH+Pr64tKlSxg6dCh69OgBW1tb9OzZE8uXLwcgO/ogKSlJnHeIseJI7kE2Y4wxxhhjxc2WLVswatQoHDlyBA4ODsjIyICHhwd27tyJM2fOoEGDBt/M+/06f5v9d9JzmZycDD09PQBf1iTX1taGpaUl5s+fj8uXL+PBgwcYNmwY1qxZAyICEYmzvPMLDvYj4F8pY4wxxhgr0e7evYspU6bg559/hoODAwBAQ0MDw4cPR15eHj58+AAAmDBhAgYMGIDnz59j7Nix+PjxIwfYciQIAj5+/Ijq1atjx44dOHr0KBwcHJCQkAADAwPMmjULrVq1QsWKFVGpUiXxM9I+QQ6w2Y+C13RijDHGGGMlmomJCfr164d79+7Bz88Ps2bNAgDcuHEDeXl5MDMzE48dP3480tLS8OLFCx6SXAg0NDTg4eGBESNGQBAEhISEwMnJCbm5uTA2NhaXCT58+DBUVFQwY8YMzn9nPxwOshljjDHGWIkkzeM1NjZGYGAg5s+fj8OHD0NfXx8VK1aEu7s7goODUaNGDZlhyTNnzhSHNvMQZfnS1NRE8+bNkZOTAwDIysoCACgrKyMvLw/ly5fHrFmzEBgYiI0bN0JVVRXu7u6KLDJj/xrnZDPGGGOMsRLnyZMnqFatGgBgxYoVaNmyJSpXrowFCxbg3LlzuH//PtavX4+RI0ciNzcXKipf+p7y52BzPrb8SM9lamoqJBIJoqOjce3aNUyfPh1r167F6NGjIZFIQERQVlbGx48fERQUhFGjRqFKlSqKLj5j/wr3ZDPGGGOMsRLl/v37qFevHrZv347IyEhs2rQJV65cgZGREWbNmiVOopWUlAQAUFFREXu98wfVHGDLhzTAPnr0KPbv34/hw4fDzs4O1tbWyMzMxNixY6GsrIwRI0YAALZt2wYrKyv4+/sruOSM/TccZDPGGGOMsRLF0tISQUFBcHFxgaamJiIjI2Fubl4g7zc0NBQSiQQzZ87kvN9CJAgCQkNDMWTIEEyfPh0mJiYAgLJly2LatGmQSCQYNWoUHjx4gKysLGzduhW3bt1ScKkZ++84wYQxxhhjjJUI0ixIdXV1lC9fHrm5uUhJScGFCxcA/NljLc37bdasGdauXYvt27crstgl3oMHDzB58mSsXLkSc+fORbVq1SCRSPDkyRPk5ubC29sba9aswcmTJ/Hw4UNcvHgRNWvWVHSxGfvPuCebMcYYY4z98PJPUBYXF4euXbvi+fPn2L9/P4YOHYqMjAyMHj1aPN7Y2Bhz585F1apV4ezsrKhilwppaWkoX7482rRpg4yMDGzZsgV79+7F69evYWFhgR07dmDs2LHo378/lJWVoa2tregiM/ZduCebMcYYY4z90PIH2D4+PpgxYwYePnwIc3NzjBkzBgsWLMDYsWOxefNmcVi4h4cHXr16hYkTJ4ozW7PCkZWVhffv38PX1xf169fHyZMnYWtrCx8fH7x+/Rpnz54FAOjp6XGAzUoE7slmjDHGGGM/NGmA7eHhgS1btmDlypXijNRaWlqYMWMGJBIJRo4ciatXr+KPP/5AQkICfH19xe/gnGz5kE5ylp6eDiKClpYW7OzssHDhQly4cAH9+vWDi4sLLC0tAQCrVq1CmTJlFFxqxuSLg2zGGGOMMfbDCw8Px44dO3DixAk0bNgQEokE8fHxePHiBWrWrAlvb2+YmZlh586dqF69Os6cOSMzqzj7ftIA+8iRI1i8eDE+fvwIbW1tTJ8+HT///DMGDBggc7y3tzfevn2LJk2aKKjEjBUODrIZY4wxxtgPLzc3F5UqVYKpqSkePHiA3bt3Y/v27VBRUUGFChWwf/9+DBs2DH379oWmpqb4Gen62Oz7CYKA48ePo1evXpg+fTosLCxw5MgReHl54dGjRxgzZgzKlSuHzZs34/Lly/jtt99w/PhxWFhYKLrojMkV52QzxhhjjLEfinQW8fwEQcDr16/h4uKCtm3b4vXr1/Dy8sLChQvx7t07PHz4EADEAJuIOMD+Dt+6Bunp6QgODsb48ePh6+uLkSNHIiwsDD169MC2bdtw+fJlAICOjg4EQcD58+dhbW1d1EVnrNAJ9K07hDHGGGOMsWIo/yRn79+/R1ZWFipXrgwA+O2333Dz5k3UrVsX7dq1g6GhIRISEtCxY0esWLECrVu3VmTRSwzpNUhOTkZiYiIAoGrVqgCANm3awNbWFoGBgcjKyhLzrR0cHJCXl4dTp04BADIzM6Gurq6YP4CxQsav7xhjjDHG2A+BiMQAe+7cuQgLC0N8fDyMjIwwc+ZM9OjRA926dQPwZSh4cnIyhg0bBi0tLbRs2VKRRS8xpAH2vXv34OrqihcvXkBVVRVOTk5YuXIlqlatinPnzgEAypQpg+zsbKipqaFt27Y4duwYcnJyoKqqygE2K9F4uDhjjDHGGPshCIIAAPD398fq1asxY8YMbN++HXXr1kVgYCBWrVqF5ORk5OXlITAwEH379kV8fDzOnTvHy3TJgTTAjoyMhK2tLerXr4+FCxeidevW2L9/P/z9/TFjxgy8ePECAwcOBACoqakBAB49eoRy5cp9c5g5YyUNDxdnjDHGGGM/jA8fPqBbt24YPHgwxo0bJ26fMWMGDh48iM2bN8POzg6hoaGIiorC7NmzoaKiwpOcycmTJ09Qr149TJ8+HT4+PgCAjIwMODk5ITs7G2fOnMGxY8fg6uoKMzMzNGzYEJ8/f8ahQ4dw5coV1KtXT8F/AWOFj3uyGWOMMcZYsRUTE4Nr167h1q1bAABDQ0MkJSWJw8azsrIAAAsXLoShoSFWrlwJAOjZsyfmzZsnLtPFAfb3k0gk2Lx5M7S1tWFoaChu19DQQLt27ZCTkwOJRIJu3brh0qVLqFu3LpKTk6Gqqopr165xgM1KDa5tGGOMMcZYsbRt2zb88ssvePPmDbS1tdG1a1esXbsWNWvWxK5duzB27FiZvF9ra2ukpKQU+B5eB1s+lJSU4Obmhs+fPyMkJARpaWnw8vLChw8fsHDhQsyePRsaGhoAAEtLS2zZsgUAeC1yVupwTzZjjDHGGCt21q1bhzFjxmDSpEk4cOAAfvrpJ4SFhWHZsmWYN28eHj58CGdnZwB/BtGRkZEwMDBQZLFLPFNTU3h4eMDGxgZHjx7FjBkzYG1tDRcXF8ycORPAlwnq8mekSkcdMFZacE42Y4wxxhgrVsLCwtCzZ08cOnRInC08JSUFbdq0gaWlJfbv34+wsDC4urpCX18f5ubmSEpKwqdPnxAVFcVDw4vAu3fv4O/vjwMHDqBixYq4ceMGAHDuO2PgnmzGGGOMMVaMZGVl4cSJE6hatSpevnwpbtfR0UHdunWRm5sLIkL37t1x8+ZNdOnSBbVr10anTp3EADs3N1eBf0HpYGJigtmzZ6N3795QVlbGL7/8AgBQUVGBRCJRcOkYUyzuyWaMMcYYY8XKu3fv8Msvv+DKlSv46aef4OnpifDwcDg5OeHUqVOwt7cHEYlLeuXH+b9FKy4uDn5+frhz5w7s7e0xf/58RReJMYXjIJsxxhhjjBU7+YM3c3Nz/Pbbb1i5ciWGDh0qrtf8V4E2K1pxcXHw9PREbGwsdu/ezXnxrNTjIJsxxhhjjBVL7969Q0BAAPbu3YvmzZsjLCwMAPdWF0fv378HABgbGyu4JIwpHgfZjDHGGGOs2Hr//j38/Pxw48YN/PTTTzIzWHMvNmOsOOIgmzHGGGOMFWtxcXHw9/fHrVu30K5dO/j6+iq6SIwx9pd4dnHGGGOMMVasVahQAV5eXrC0tER8fDy4j4gxVpxxTzZjjDHGGPshJCYmQk9Pjyc9Y4wVaxxkM8YYY4yxH4p0dnHGGCuOOMhmjDHGGGOMMcbkhF8BMsYYY4wxxhhjcsJBNmOMMcYYY4wxJiccZDPGGGOMMcYYY3LCQTZjjDHGGGOMMSYnHGQzxhhjjDHGGGNywkE2Y4wxxhhjjDEmJxxkM8YYY0yufv/9dwiCgOTk5H/8GQsLCyxbtqzQysQYY4wVFQ6yGWOMsVJm2LBhEAQBY8eOLbBv/PjxEAQBw4YNK/qCMcYYYyUAB9mMMcZYKWRmZobdu3cjIyND3JaZmYmQkBBUrlxZgSVjjDHGfmwcZDPGGGOlUKNGjWBmZobQ0FBxW2hoKCpXrgxra2txW1ZWFiZOnIjy5ctDXV0drVq1wo0bN2S+69ixY6hRowY0NDTQrl07vHjxosD/XkREBOzs7KChoQEzMzNMnDgR6enpf1m+V69eoUePHtDS0oKOjg769u2L9+/ff/8fzhhjjBUyDrIZY4yxUmr48OHYsmWL+O/NmzfDxcVF5pgZM2bgwIED2LZtG27fvo1q1aqhc+fOSExMBAC8fv0aPXv2RLdu3XD37l2MHDkSHh4eMt/x9OlTODg4oFevXoiKisKePXsQEREBNze3b5ZLIpGgR48eSExMxPnz53Hq1Ck8e/YM/fr1k/MZYIwxxuSPg2zGGGOslBo0aBAiIiLw8uVLvHz5EpcuXcKgQYPE/enp6QgODsaiRYvg6OgIKysrbNiwARoaGti0aRMAIDg4GJaWlggKCkLNmjUxcODAAvncAQEBGDhwICZPnozq1aujRYsWWLFiBX799VdkZmYWKNeZM2cQHR2NkJAQNG7cGM2aNcOvv/6K8+fPF+hFZ4wxxoobFUUXgDHGGGOKYWRkBCcnJ2zduhVEBCcnJxgaGor7nz59ipycHLRs2VLcpqqqiqZNm+Lhw4cAgIcPH6JZs2Yy32trayvz78jISERFRWHnzp3iNiKCRCLB8+fPUbt2bZnjHz58CDMzM5iZmYnbrKysoKenh4cPH8LGxub7/3jGGGOskHCQzRhjjJViw4cPF4dtr169ulD+N9LS0jBmzBhMnDixwD6eZI0xxlhJw8PFGWOMsVLMwcEB2dnZyMnJQefOnWX2WVpaQk1NDZcuXRK35eTk4MaNG7CysgIA1K5dG9evX5f53NWrV2X+3ahRIzx48ADVqlUr8H9qamoFylS7dm28fv0ar1+/Frc9ePAAycnJ4v8uY4wxVlxxkM0YY4yVYsrKynj48CEePHgAZWVlmX1ly5aFq6srpk+fjuPHj+PBgwcYNWoUPn/+jBEjRgAAxo4di8ePH2P69Ol49OgRQkJCsHXrVpnvmTlzJi5fvgw3NzfcvXsXjx8/xqFDh/5y4rMOHTqgXr16GDhwIG7fvo3r169jyJAhaNOmDZo0aVIo54ExxhiTFw6yGWOMsVJOR0cHOjo639wXGBiIXr16YfDgwWjUqBGePHmCEydOoFy5cgC+DPc+cOAAwsLC0KBBA6xduxb+/v4y31G/fn2cP38eMTExsLOzg7W1NebMmQNTU9Nv/m8KgoBDhw6hXLlyaN26NTp06ICqVatiz5498v3DGWOMsUIgEBEpuhCMMcYYY4wxxlhJwD3ZjDHGGGOMMcaYnHCQzRhjjDHGGGOMyQkH2YwxxhhjjDHGmJxwkM0YY4wxxhhjjMkJB9mMMcYYY4wxxpiccJDNGGOMMcYYY4zJCQfZjDHGGGOMMcaYnHCQzRhjjDHGGGOMyQkH2YwxxhhjjDHGmJxwkM0YY4wxxhhjjMkJB9mMMcYYY4wxxpiccJDNGGOMMcYYY4zJyf8BAK7CCtw2aLwAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_results[[f\"Hits@{k}\" for k in [10, 20, 30, 40, 50]]].plot(kind=\"line\", marker='o', figsize=(10, 6), title=\"Hits@K por modelo\")\n",
        "plt.ylabel(\"Proporción\")\n",
        "plt.xlabel(\"Modelo\")\n",
        "plt.xticks(ticks=range(len(df_results.index)), labels=df_results.index, rotation=45, ha='right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INSfEMJAiFi6"
      },
      "source": [
        "Alas, skip-connections actually seem to be deterimental for ogbl-ddi. Even reducing the number of GNN layers back down to 7 (or even 5) does not significantly improve performance. An in-depth analysis of what went wrong here can be found in the corresponding Medium post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensión del embedding: (768,)\n",
            "Primeros valores: [ 0.6069926  -0.44180122  0.22789395 -0.34092522  0.05965592  0.04752173\n",
            " -0.34843522  0.04692824  0.38924545 -0.32274452]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cargar el modelo BioBERT desde HuggingFace (vía SentenceTransformer o directamente si no está en sbert)\n",
        "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "\n",
        "# Obtener el embedding vectorial del texto de entrada\n",
        "embedding_biobert = model.encode(gpt_response_text)\n",
        "\n",
        "print(\"Dimensión del embedding:\", embedding_biobert.shape)\n",
        "print(\"Primeros valores:\", embedding_biobert[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6758\n",
            "Epoch 1 has loss 10.2484\n",
            "Epoch 2 has loss 9.346\n",
            "Epoch 3 has loss 8.4555\n",
            "Epoch 4 has loss 7.7995\n",
            "Epoch 5 has loss 7.496\n",
            "Epoch 6 has loss 7.6758\n",
            "Epoch 7 has loss 7.4649\n",
            "Epoch 8 has loss 7.2211\n",
            "Epoch 9 has loss 6.9457\n",
            "Epoch 10 has loss 6.8602\n",
            "Epoch 11 has loss 6.5053\n",
            "Epoch 12 has loss 6.2172\n",
            "Epoch 13 has loss 5.9493\n",
            "Epoch 14 has loss 5.8689\n",
            "Epoch 15 has loss 5.7629\n",
            "Epoch 16 has loss 5.7132\n",
            "Epoch 17 has loss 5.6767\n",
            "Epoch 18 has loss 5.5575\n",
            "Epoch 19 has loss 5.5127\n",
            "Epoch 20 has loss 5.8133\n",
            "Epoch 21 has loss 5.5548\n",
            "Epoch 22 has loss 5.489\n",
            "Epoch 23 has loss 5.4425\n",
            "Epoch 24 has loss 5.3206\n",
            "Epoch 25 has loss 5.3\n",
            "Epoch 26 has loss 5.3115\n",
            "Epoch 27 has loss 5.3032\n",
            "Epoch 28 has loss 5.1227\n",
            "Epoch 29 has loss 5.1574\n",
            "Epoch 30 has loss 4.944\n",
            "Epoch 31 has loss 4.8214\n",
            "Epoch 32 has loss 4.8476\n",
            "Epoch 33 has loss 4.8173\n",
            "Epoch 34 has loss 4.6459\n",
            "Epoch 35 has loss 4.7188\n",
            "Epoch 36 has loss 4.561\n",
            "Epoch 37 has loss 4.5498\n",
            "Epoch 38 has loss 4.5124\n",
            "Epoch 39 has loss 4.4583\n",
            "Epoch 40 has loss 4.6147\n",
            "Epoch 41 has loss 4.4859\n",
            "Epoch 42 has loss 4.3918\n",
            "Epoch 43 has loss 4.2804\n",
            "Epoch 44 has loss 4.2647\n",
            "Epoch 45 has loss 4.4828\n",
            "Epoch 46 has loss 4.3115\n",
            "Epoch 47 has loss 4.2607\n",
            "Epoch 48 has loss 4.2122\n",
            "Epoch 49 has loss 4.1341\n",
            "  14978 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  20907 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  25549 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  29757 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  32465 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔬 Resultados SAGE base sin GPT (con BioBERT):\n",
            "Hits@10: 0.1122\n",
            "Hits@20: 0.1566\n",
            "Hits@30: 0.1914\n",
            "Hits@40: 0.2229\n",
            "Hits@50: 0.2432\n",
            "Accuracy: 0.9000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = SAGE(\n",
        "    in_channels=1,  \n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_base_biobert = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔬 Resultados SAGE base sin GPT (con BioBERT):\")\n",
        "for k, v in results_base_biobert.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.1419\n",
            "Epoch 1 has loss 9.9805\n",
            "Epoch 2 has loss 9.2631\n",
            "Epoch 3 has loss 8.5002\n",
            "Epoch 4 has loss 7.9327\n",
            "Epoch 5 has loss 7.6998\n",
            "Epoch 6 has loss 7.5019\n",
            "Epoch 7 has loss 7.3797\n",
            "Epoch 8 has loss 7.3821\n",
            "Epoch 9 has loss 7.1618\n",
            "Epoch 10 has loss 7.0738\n",
            "Epoch 11 has loss 6.682\n",
            "Epoch 12 has loss 6.6539\n",
            "Epoch 13 has loss 6.4763\n",
            "Epoch 14 has loss 6.4001\n",
            "Epoch 15 has loss 6.2973\n",
            "Epoch 16 has loss 6.1589\n",
            "Epoch 17 has loss 6.0592\n",
            "Epoch 18 has loss 5.664\n",
            "Epoch 19 has loss 5.3992\n",
            "Epoch 20 has loss 5.298\n",
            "Epoch 21 has loss 5.1699\n",
            "Epoch 22 has loss 5.1606\n",
            "Epoch 23 has loss 4.9751\n",
            "Epoch 24 has loss 4.8983\n",
            "Epoch 25 has loss 4.8032\n",
            "Epoch 26 has loss 4.8099\n",
            "Epoch 27 has loss 4.6616\n",
            "Epoch 28 has loss 4.4989\n",
            "Epoch 29 has loss 4.4298\n",
            "Epoch 30 has loss 4.4121\n",
            "Epoch 31 has loss 4.362\n",
            "Epoch 32 has loss 4.2281\n",
            "Epoch 33 has loss 4.0974\n",
            "Epoch 34 has loss 4.0473\n",
            "Epoch 35 has loss 4.0645\n",
            "Epoch 36 has loss 3.8751\n",
            "Epoch 37 has loss 3.7712\n",
            "Epoch 38 has loss 3.7895\n",
            "Epoch 39 has loss 3.9225\n",
            "Epoch 40 has loss 3.7288\n",
            "Epoch 41 has loss 3.6439\n",
            "Epoch 42 has loss 3.6303\n",
            "Epoch 43 has loss 3.6831\n",
            "Epoch 44 has loss 3.5685\n",
            "Epoch 45 has loss 3.4632\n",
            "Epoch 46 has loss 3.4994\n",
            "Epoch 47 has loss 3.3435\n",
            "Epoch 48 has loss 3.3745\n",
            "Epoch 49 has loss 3.364\n",
            "  38047 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  49397 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  56569 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  59433 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  64762 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGE (con BioBERT como input):\n",
            "Hits@10: 0.2850\n",
            "Hits@20: 0.3700\n",
            "Hits@30: 0.4238\n",
            "Hits@40: 0.4452\n",
            "Hits@50: 0.4851\n",
            "Accuracy: 0.9281\n"
          ]
        }
      ],
      "source": [
        "\n",
        "embedding_biobert_tensor = torch.tensor(embedding_biobert, device=device)\n",
        "aug_emb_bio = torch.cat([\n",
        "    emb, embedding_biobert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb_bio.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_bio, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_bio_input = test(model, predictor, aug_emb_bio, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGE (con BioBERT como input):\")\n",
        "for k, v in results_bio_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5324\n",
            "Epoch 1 has loss 10.2771\n",
            "Epoch 2 has loss 10.171\n",
            "Epoch 3 has loss 10.102\n",
            "Epoch 4 has loss 9.9966\n",
            "Epoch 5 has loss 9.8709\n",
            "Epoch 6 has loss 9.7571\n",
            "Epoch 7 has loss 9.8073\n",
            "Epoch 8 has loss 9.6425\n",
            "Epoch 9 has loss 9.7626\n",
            "Epoch 10 has loss 9.6055\n",
            "Epoch 11 has loss 9.5406\n",
            "Epoch 12 has loss 9.5159\n",
            "Epoch 13 has loss 9.4801\n",
            "Epoch 14 has loss 9.4788\n",
            "Epoch 15 has loss 9.4953\n",
            "Epoch 16 has loss 9.4763\n",
            "Epoch 17 has loss 9.4783\n",
            "Epoch 18 has loss 9.4606\n",
            "Epoch 19 has loss 9.4503\n",
            "Epoch 20 has loss 9.4606\n",
            "Epoch 21 has loss 9.44\n",
            "Epoch 22 has loss 9.4419\n",
            "Epoch 23 has loss 9.4633\n",
            "Epoch 24 has loss 9.4305\n",
            "Epoch 25 has loss 9.4431\n",
            "Epoch 26 has loss 9.4483\n",
            "Epoch 27 has loss 9.4356\n",
            "Epoch 28 has loss 9.4212\n",
            "Epoch 29 has loss 9.4063\n",
            "Epoch 30 has loss 9.4325\n",
            "Epoch 31 has loss 9.4183\n",
            "Epoch 32 has loss 9.3961\n",
            "Epoch 33 has loss 9.4366\n",
            "Epoch 34 has loss 9.4393\n",
            "Epoch 35 has loss 9.4154\n",
            "Epoch 36 has loss 9.404\n",
            "Epoch 37 has loss 9.4197\n",
            "Epoch 38 has loss 9.4016\n",
            "Epoch 39 has loss 9.3969\n",
            "Epoch 40 has loss 9.4021\n",
            "Epoch 41 has loss 9.4133\n",
            "Epoch 42 has loss 9.4009\n",
            "Epoch 43 has loss 9.3946\n",
            "Epoch 44 has loss 9.3909\n",
            "Epoch 45 has loss 9.3911\n",
            "Epoch 46 has loss 9.4042\n",
            "Epoch 47 has loss 9.3938\n",
            "Epoch 48 has loss 9.3919\n",
            "Epoch 49 has loss 9.4199\n",
            "📉 Epoch 0, Loss: 23.6322\n",
            "📉 Epoch 1, Loss: 15.2663\n",
            "📉 Epoch 2, Loss: 13.6623\n",
            "📉 Epoch 3, Loss: 13.0299\n",
            "📉 Epoch 4, Loss: 12.6245\n",
            "📉 Epoch 5, Loss: 12.3606\n",
            "📉 Epoch 6, Loss: 12.0603\n",
            "📉 Epoch 7, Loss: 11.9169\n",
            "📉 Epoch 8, Loss: 11.8185\n",
            "📉 Epoch 9, Loss: 11.6952\n",
            "📉 Epoch 10, Loss: 11.5809\n",
            "📉 Epoch 11, Loss: 11.5158\n",
            "📉 Epoch 12, Loss: 11.5055\n",
            "📉 Epoch 13, Loss: 11.3967\n",
            "📉 Epoch 14, Loss: 11.5476\n",
            "📉 Epoch 15, Loss: 11.3734\n",
            "📉 Epoch 16, Loss: 11.2938\n",
            "📉 Epoch 17, Loss: 11.2663\n",
            "📉 Epoch 18, Loss: 11.4031\n",
            "📉 Epoch 19, Loss: 11.2442\n",
            "📉 Epoch 20, Loss: 11.2296\n",
            "📉 Epoch 21, Loss: 11.2523\n",
            "📉 Epoch 22, Loss: 11.1534\n",
            "📉 Epoch 23, Loss: 11.1451\n",
            "📉 Epoch 24, Loss: 11.1496\n",
            "📉 Epoch 25, Loss: 11.1290\n",
            "📉 Epoch 26, Loss: 11.1033\n",
            "📉 Epoch 27, Loss: 11.1181\n",
            "📉 Epoch 28, Loss: 11.1289\n",
            "📉 Epoch 29, Loss: 11.0934\n",
            "📉 Epoch 30, Loss: 11.0578\n",
            "📉 Epoch 31, Loss: 11.0504\n",
            "📉 Epoch 32, Loss: 11.0967\n",
            "📉 Epoch 33, Loss: 11.0920\n",
            "📉 Epoch 34, Loss: 11.0231\n",
            "📉 Epoch 35, Loss: 11.1740\n",
            "📉 Epoch 36, Loss: 11.1421\n",
            "📉 Epoch 37, Loss: 11.0192\n",
            "📉 Epoch 38, Loss: 10.9953\n",
            "📉 Epoch 39, Loss: 10.9997\n",
            "📉 Epoch 40, Loss: 10.9846\n",
            "📉 Epoch 41, Loss: 11.0379\n",
            "📉 Epoch 42, Loss: 11.0093\n",
            "📉 Epoch 43, Loss: 10.9578\n",
            "📉 Epoch 44, Loss: 10.9675\n",
            "📉 Epoch 45, Loss: 10.9280\n",
            "📉 Epoch 46, Loss: 10.9424\n",
            "📉 Epoch 47, Loss: 10.9317\n",
            "📉 Epoch 48, Loss: 10.9408\n",
            "📉 Epoch 49, Loss: 10.9255\n",
            "  7270 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  11239 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  13156 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  15555 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  16855 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGE (con BioBERT en la capa final):\n",
            "Hits@10: 0.0545\n",
            "Hits@20: 0.0842\n",
            "Hits@30: 0.0986\n",
            "Hits@40: 0.1165\n",
            "Hits@50: 0.1263\n",
            "Accuracy: 0.8887\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_bio_final = model(emb, adj_t)\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "predictor_bio_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_bio_final.size(1),\n",
        "    hidden_channels=z_bio_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_bio_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_bio_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_bio_final.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    # Positivos\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    # Negativos\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    # Concatenamos\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    # Entrenamiento en batches\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_bio_final(\n",
        "            z_bio_final[edge[0]], z_bio_final[edge[1]],\n",
        "            embedding_biobert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "wrapped_predictor_bio = WrappedPredictor(predictor_bio_final, embedding_biobert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_bio_final = test(dummy_model, wrapped_predictor_bio, z_bio_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGE (con BioBERT en la capa final):\")\n",
        "for k, v in results_bio_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source Prompt Type GPT Injection  \\\n",
              "0          SAGEConv            classic           -             -   \n",
              "1          SAGEConv      classic + GPT     default         input   \n",
              "2          SAGEConv            classic     default         final   \n",
              "3      SkipConnSAGE            classic           -             -   \n",
              "4      SkipConnSAGE      classic + GPT     default         input   \n",
              "5      SkipConnSAGE            classic     default         final   \n",
              "6   PostProcessSAGE            classic           -             -   \n",
              "7   PostProcessSAGE      classic + GPT     default         input   \n",
              "8   PostProcessSAGE            classic     default         final   \n",
              "9          SAGEConv           classic            -             -   \n",
              "10         SAGEConv  classic + BioBERT     default         input   \n",
              "11         SAGEConv            classic     default         final   \n",
              "\n",
              "           Predictor  Epochs   Hits@10   Hits@20   Hits@30   Hits@40  \\\n",
              "0             Neural      50  0.132565  0.171662  0.225854  0.250096   \n",
              "1             Neural      50  0.117703  0.176794  0.197260  0.208849   \n",
              "2       Neural + GPT      50  0.032752  0.044678  0.057555  0.062597   \n",
              "3             Neural     100  0.000225  0.000494  0.000802  0.001041   \n",
              "4             Neural     100  0.000809  0.001049  0.001288  0.001858   \n",
              "5       Neural + GPT     100  0.003274  0.007911  0.012196  0.014923   \n",
              "6             Neural      50  0.005206  0.210482  0.265707  0.293904   \n",
              "7             Neural      50  0.007334  0.007334  0.132925  0.153601   \n",
              "8       Neural + GPT      50  0.038385  0.048438  0.053443  0.061211   \n",
              "9             Neural      50  0.112204  0.156620  0.191394  0.222917   \n",
              "10            Neural      50  0.285020  0.370045  0.423773  0.445228   \n",
              "11  Neural + BioBERT      50  0.054461  0.084194  0.098555  0.116526   \n",
              "\n",
              "     Hits@50  Accuracy  \n",
              "0   0.273229    0.9010  \n",
              "1   0.219119    0.8920  \n",
              "2   0.072718    0.8618  \n",
              "3   0.001326    0.7420  \n",
              "4   0.002277    0.7520  \n",
              "5   0.017664    0.7774  \n",
              "6   0.322446    0.8781  \n",
              "7   0.181296    0.8706  \n",
              "8   0.068358    0.8070  \n",
              "9   0.243204    0.9000  \n",
              "10  0.485149    0.9281  \n",
              "11  0.126265    0.8887  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "\n",
        "# Definimos todos los resultados con su configuración\n",
        "model_results = [\n",
        "    # SAGEConv\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_input},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 50, **results_sage_final},\n",
        "\n",
        "    # SkipConnSAGE\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 100, **results_skipconn},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 100, **results_skipconn_gpt_input},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 100, **results_skipconn_gpt_final},\n",
        "\n",
        "    # PostProcessSAGE\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_base},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + GPT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_gpt_input},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + GPT\", \"Epochs\": 50, **results_post_gpt_final},\n",
        "\n",
        "    # BioBERT (SAGEConv con variantes)\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic \", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_base_biobert},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_bio_input},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_bio_final},\n",
        "]\n",
        "\n",
        "df_summary = pd.DataFrame(model_results)\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "# Mostramos la tabla final\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4048\n",
            "Epoch 1 has loss 10.5245\n",
            "Epoch 2 has loss 10.1552\n",
            "Epoch 3 has loss 10.3725\n",
            "Epoch 4 has loss 10.0459\n",
            "Epoch 5 has loss 9.8668\n",
            "Epoch 6 has loss 10.0747\n",
            "Epoch 7 has loss 9.8839\n",
            "Epoch 8 has loss 9.9537\n",
            "Epoch 9 has loss 9.816\n",
            "Epoch 10 has loss 9.5504\n",
            "Epoch 11 has loss 9.159\n",
            "Epoch 12 has loss 9.9386\n",
            "Epoch 13 has loss 9.6605\n",
            "Epoch 14 has loss 9.6453\n",
            "Epoch 15 has loss 9.5424\n",
            "Epoch 16 has loss 9.2122\n",
            "Epoch 17 has loss 9.5701\n",
            "Epoch 18 has loss 9.0825\n",
            "Epoch 19 has loss 9.0193\n",
            "Epoch 20 has loss 9.085\n",
            "Epoch 21 has loss 8.666\n",
            "Epoch 22 has loss 9.1183\n",
            "Epoch 23 has loss 8.9205\n",
            "Epoch 24 has loss 8.5939\n",
            "Epoch 25 has loss 8.355\n",
            "Epoch 26 has loss 8.6509\n",
            "Epoch 27 has loss 8.6483\n",
            "Epoch 28 has loss 8.3952\n",
            "Epoch 29 has loss 8.9823\n",
            "Epoch 30 has loss 9.255\n",
            "Epoch 31 has loss 8.4263\n",
            "Epoch 32 has loss 8.5328\n",
            "Epoch 33 has loss 8.2601\n",
            "Epoch 34 has loss 8.2693\n",
            "Epoch 35 has loss 8.7092\n",
            "Epoch 36 has loss 8.359\n",
            "Epoch 37 has loss 8.3089\n",
            "Epoch 38 has loss 8.0569\n",
            "Epoch 39 has loss 7.7551\n",
            "Epoch 40 has loss 7.901\n",
            "Epoch 41 has loss 8.1591\n",
            "Epoch 42 has loss 7.9703\n",
            "Epoch 43 has loss 7.9736\n",
            "Epoch 44 has loss 7.7454\n",
            "Epoch 45 has loss 7.8405\n",
            "Epoch 46 has loss 7.6017\n",
            "Epoch 47 has loss 7.6014\n",
            "Epoch 48 has loss 7.716\n",
            "Epoch 49 has loss 7.8487\n",
            "  2392 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4160 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5012 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  6073 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  6776 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE (sin BioBERT):\n",
            "Hits@10: 0.0179\n",
            "Hits@20: 0.0312\n",
            "Hits@30: 0.0375\n",
            "Hits@40: 0.0455\n",
            "Hits@50: 0.0508\n",
            "Accuracy: 0.7755\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE (sin BioBERT):\")\n",
        "for k, v in results_skipconn_bio_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5807\n",
            "Epoch 1 has loss 10.9454\n",
            "Epoch 2 has loss 10.0826\n",
            "Epoch 3 has loss 9.8859\n",
            "Epoch 4 has loss 9.8799\n",
            "Epoch 5 has loss 10.0109\n",
            "Epoch 6 has loss 9.5643\n",
            "Epoch 7 has loss 10.1151\n",
            "Epoch 8 has loss 9.8006\n",
            "Epoch 9 has loss 9.5733\n",
            "Epoch 10 has loss 9.4138\n",
            "Epoch 11 has loss 9.5524\n",
            "Epoch 12 has loss 9.3276\n",
            "Epoch 13 has loss 9.1904\n",
            "Epoch 14 has loss 9.609\n",
            "Epoch 15 has loss 9.316\n",
            "Epoch 16 has loss 9.4486\n",
            "Epoch 17 has loss 9.0489\n",
            "Epoch 18 has loss 9.1405\n",
            "Epoch 19 has loss 8.6873\n",
            "Epoch 20 has loss 9.0539\n",
            "Epoch 21 has loss 9.2702\n",
            "Epoch 22 has loss 8.6042\n",
            "Epoch 23 has loss 8.8779\n",
            "Epoch 24 has loss 8.5081\n",
            "Epoch 25 has loss 8.3514\n",
            "Epoch 26 has loss 8.4835\n",
            "Epoch 27 has loss 8.3952\n",
            "Epoch 28 has loss 8.1781\n",
            "Epoch 29 has loss 8.6251\n",
            "Epoch 30 has loss 8.3853\n",
            "Epoch 31 has loss 8.8523\n",
            "Epoch 32 has loss 8.4804\n",
            "Epoch 33 has loss 8.8343\n",
            "Epoch 34 has loss 8.4833\n",
            "Epoch 35 has loss 8.1281\n",
            "Epoch 36 has loss 8.1545\n",
            "Epoch 37 has loss 8.4221\n",
            "Epoch 38 has loss 7.9987\n",
            "Epoch 39 has loss 8.1216\n",
            "Epoch 40 has loss 7.9778\n",
            "Epoch 41 has loss 7.9926\n",
            "Epoch 42 has loss 8.3312\n",
            "Epoch 43 has loss 7.9505\n",
            "Epoch 44 has loss 8.044\n",
            "Epoch 45 has loss 8.0165\n",
            "Epoch 46 has loss 7.7224\n",
            "Epoch 47 has loss 7.5316\n",
            "Epoch 48 has loss 7.8742\n",
            "Epoch 49 has loss 7.5523\n",
            "  37 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  83 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  144 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  223 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  358 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🪄 Resultados SkipConnSAGE (con BioBERT como input):\n",
            "Hits@10: 0.0003\n",
            "Hits@20: 0.0006\n",
            "Hits@30: 0.0011\n",
            "Hits@40: 0.0017\n",
            "Hits@50: 0.0027\n",
            "Accuracy: 0.7881\n"
          ]
        }
      ],
      "source": [
        "aug_emb_bio = torch.cat([\n",
        "    emb, embedding_biobert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb_bio.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_bio, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_input = test(model, predictor, aug_emb_bio, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🪄 Resultados SkipConnSAGE (con BioBERT como input):\")\n",
        "for k, v in results_skipconn_bio_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.0259\n",
            "Epoch 1 has loss 10.7186\n",
            "Epoch 2 has loss 10.4683\n",
            "Epoch 3 has loss 10.3888\n",
            "Epoch 4 has loss 10.3087\n",
            "Epoch 5 has loss 10.2476\n",
            "Epoch 6 has loss 10.2204\n",
            "Epoch 7 has loss 10.2195\n",
            "Epoch 8 has loss 10.2183\n",
            "Epoch 9 has loss 10.1878\n",
            "Epoch 10 has loss 10.1819\n",
            "Epoch 11 has loss 10.2046\n",
            "Epoch 12 has loss 10.1849\n",
            "Epoch 13 has loss 10.1918\n",
            "Epoch 14 has loss 10.1441\n",
            "Epoch 15 has loss 10.1177\n",
            "Epoch 16 has loss 10.0846\n",
            "Epoch 17 has loss 10.0882\n",
            "Epoch 18 has loss 10.1193\n",
            "Epoch 19 has loss 10.1185\n",
            "Epoch 20 has loss 10.3154\n",
            "Epoch 21 has loss 10.162\n",
            "Epoch 22 has loss 10.1684\n",
            "Epoch 23 has loss 10.1175\n",
            "Epoch 24 has loss 10.1133\n",
            "Epoch 25 has loss 10.0739\n",
            "Epoch 26 has loss 10.0699\n",
            "Epoch 27 has loss 10.0768\n",
            "Epoch 28 has loss 10.0517\n",
            "Epoch 29 has loss 10.0646\n",
            "Epoch 30 has loss 10.0092\n",
            "Epoch 31 has loss 10.027\n",
            "Epoch 32 has loss 10.0295\n",
            "Epoch 33 has loss 9.9887\n",
            "Epoch 34 has loss 10.0124\n",
            "Epoch 35 has loss 9.9892\n",
            "Epoch 36 has loss 9.9656\n",
            "Epoch 37 has loss 9.9646\n",
            "Epoch 38 has loss 9.9844\n",
            "Epoch 39 has loss 10.0117\n",
            "Epoch 40 has loss 9.9906\n",
            "Epoch 41 has loss 9.9919\n",
            "Epoch 42 has loss 9.9702\n",
            "Epoch 43 has loss 9.9471\n",
            "Epoch 44 has loss 9.9353\n",
            "Epoch 45 has loss 9.9131\n",
            "Epoch 46 has loss 9.9202\n",
            "Epoch 47 has loss 9.888\n",
            "Epoch 48 has loss 9.8762\n",
            "Epoch 49 has loss 9.868\n",
            "📉 Epoch 0, Loss: 26.4895\n",
            "📉 Epoch 1, Loss: 21.4669\n",
            "📉 Epoch 2, Loss: 17.5587\n",
            "📉 Epoch 3, Loss: 16.8575\n",
            "📉 Epoch 4, Loss: 16.7214\n",
            "📉 Epoch 5, Loss: 16.6234\n",
            "📉 Epoch 6, Loss: 16.5648\n",
            "📉 Epoch 7, Loss: 16.4990\n",
            "📉 Epoch 8, Loss: 16.4551\n",
            "📉 Epoch 9, Loss: 16.4317\n",
            "📉 Epoch 10, Loss: 16.4030\n",
            "📉 Epoch 11, Loss: 16.3923\n",
            "📉 Epoch 12, Loss: 16.3195\n",
            "📉 Epoch 13, Loss: 16.3017\n",
            "📉 Epoch 14, Loss: 16.2776\n",
            "📉 Epoch 15, Loss: 16.2551\n",
            "📉 Epoch 16, Loss: 16.2282\n",
            "📉 Epoch 17, Loss: 16.2123\n",
            "📉 Epoch 18, Loss: 16.3597\n",
            "📉 Epoch 19, Loss: 16.1649\n",
            "📉 Epoch 20, Loss: 16.1769\n",
            "📉 Epoch 21, Loss: 16.3294\n",
            "📉 Epoch 22, Loss: 16.2970\n",
            "📉 Epoch 23, Loss: 16.3868\n",
            "📉 Epoch 24, Loss: 16.1457\n",
            "📉 Epoch 25, Loss: 16.0954\n",
            "📉 Epoch 26, Loss: 16.2877\n",
            "📉 Epoch 27, Loss: 16.2133\n",
            "📉 Epoch 28, Loss: 16.0534\n",
            "📉 Epoch 29, Loss: 16.0744\n",
            "📉 Epoch 30, Loss: 16.2884\n",
            "📉 Epoch 31, Loss: 16.1039\n",
            "📉 Epoch 32, Loss: 16.0120\n",
            "📉 Epoch 33, Loss: 16.3393\n",
            "📉 Epoch 34, Loss: 15.9931\n",
            "📉 Epoch 35, Loss: 15.9092\n",
            "📉 Epoch 36, Loss: 15.9784\n",
            "📉 Epoch 37, Loss: 16.3286\n",
            "📉 Epoch 38, Loss: 15.8609\n",
            "📉 Epoch 39, Loss: 16.0128\n",
            "📉 Epoch 40, Loss: 15.9342\n",
            "📉 Epoch 41, Loss: 15.8815\n",
            "📉 Epoch 42, Loss: 15.7537\n",
            "📉 Epoch 43, Loss: 15.9196\n",
            "📉 Epoch 44, Loss: 15.9472\n",
            "📉 Epoch 45, Loss: 15.7500\n",
            "📉 Epoch 46, Loss: 15.8365\n",
            "📉 Epoch 47, Loss: 15.7532\n",
            "📉 Epoch 48, Loss: 15.7595\n",
            "📉 Epoch 49, Loss: 15.6432\n",
            "  2104 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2647 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  3779 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  4468 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  5006 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SkipConnSAGE (con BioBERT en la capa final):\n",
            "Hits@10: 0.0158\n",
            "Hits@20: 0.0198\n",
            "Hits@30: 0.0283\n",
            "Hits@40: 0.0335\n",
            "Hits@50: 0.0375\n",
            "Accuracy: 0.8014\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_bio = model(emb, adj_t)\n",
        "\n",
        "predictor_skip_bio_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_bio.size(1),\n",
        "    hidden_channels=z_skip_bio.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_skip_bio_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_skip_bio_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skip_bio_final.train()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skip_bio_final(\n",
        "            z_skip_bio[edge[0]], z_skip_bio[edge[1]],\n",
        "            embedding_biobert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_skip_bio_final, embedding_biobert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skipconn_bio_final = test(dummy_model, wrapped_predictor, z_skip_bio, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SkipConnSAGE (con BioBERT en la capa final):\")\n",
        "for k, v in results_skipconn_bio_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.8695\n",
            "Epoch 1 has loss 10.8265\n",
            "Epoch 2 has loss 9.9749\n",
            "Epoch 3 has loss 11.711\n",
            "Epoch 4 has loss 9.0982\n",
            "Epoch 5 has loss 8.3686\n",
            "Epoch 6 has loss 7.8514\n",
            "Epoch 7 has loss 7.659\n",
            "Epoch 8 has loss 7.4897\n",
            "Epoch 9 has loss 7.6414\n",
            "Epoch 10 has loss 7.4641\n",
            "Epoch 11 has loss 7.341\n",
            "Epoch 12 has loss 7.285\n",
            "Epoch 13 has loss 7.3236\n",
            "Epoch 14 has loss 7.3324\n",
            "Epoch 15 has loss 7.3971\n",
            "Epoch 16 has loss 7.2546\n",
            "Epoch 17 has loss 7.1373\n",
            "Epoch 18 has loss 7.0483\n",
            "Epoch 19 has loss 6.9065\n",
            "Epoch 20 has loss 6.792\n",
            "Epoch 21 has loss 6.9881\n",
            "Epoch 22 has loss 6.6327\n",
            "Epoch 23 has loss 6.6723\n",
            "Epoch 24 has loss 6.4492\n",
            "Epoch 25 has loss 6.7138\n",
            "Epoch 26 has loss 6.2846\n",
            "Epoch 27 has loss 6.2608\n",
            "Epoch 28 has loss 6.2183\n",
            "Epoch 29 has loss 5.9952\n",
            "Epoch 30 has loss 5.8026\n",
            "Epoch 31 has loss 5.7072\n",
            "Epoch 32 has loss 5.7101\n",
            "Epoch 33 has loss 5.6202\n",
            "Epoch 34 has loss 5.5833\n",
            "Epoch 35 has loss 5.7023\n",
            "Epoch 36 has loss 5.3863\n",
            "Epoch 37 has loss 5.4062\n",
            "Epoch 38 has loss 5.3902\n",
            "Epoch 39 has loss 5.4234\n",
            "Epoch 40 has loss 5.2473\n",
            "Epoch 41 has loss 5.1024\n",
            "Epoch 42 has loss 5.151\n",
            "Epoch 43 has loss 5.2115\n",
            "Epoch 44 has loss 5.0446\n",
            "Epoch 45 has loss 5.0469\n",
            "Epoch 46 has loss 4.9577\n",
            "Epoch 47 has loss 4.9799\n",
            "Epoch 48 has loss 4.9353\n",
            "Epoch 49 has loss 5.0017\n",
            "  0 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  27317 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  32612 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📓 Resultados PostProcessSAGE (sin BioBERT):\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0000\n",
            "Hits@40: 0.2046\n",
            "Hits@50: 0.2443\n",
            "Accuracy: 0.8475\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📓 Resultados PostProcessSAGE (sin BioBERT):\")\n",
        "for k, v in results_post_bio_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8342\n",
            "Epoch 1 has loss 13.09\n",
            "Epoch 2 has loss 10.4745\n",
            "Epoch 3 has loss 10.4628\n",
            "Epoch 4 has loss 10.068\n",
            "Epoch 5 has loss 35.4546\n",
            "Epoch 6 has loss 10.0192\n",
            "Epoch 7 has loss 9.8298\n",
            "Epoch 8 has loss 9.1753\n",
            "Epoch 9 has loss 8.3225\n",
            "Epoch 10 has loss 7.8912\n",
            "Epoch 11 has loss 7.821\n",
            "Epoch 12 has loss 7.6369\n",
            "Epoch 13 has loss 7.6232\n",
            "Epoch 14 has loss 7.8565\n",
            "Epoch 15 has loss 7.4041\n",
            "Epoch 16 has loss 7.1558\n",
            "Epoch 17 has loss 7.0044\n",
            "Epoch 18 has loss 7.0641\n",
            "Epoch 19 has loss 6.713\n",
            "Epoch 20 has loss 6.5012\n",
            "Epoch 21 has loss 6.3618\n",
            "Epoch 22 has loss 6.3443\n",
            "Epoch 23 has loss 6.2026\n",
            "Epoch 24 has loss 5.9451\n",
            "Epoch 25 has loss 5.9232\n",
            "Epoch 26 has loss 5.6895\n",
            "Epoch 27 has loss 5.6803\n",
            "Epoch 28 has loss 5.6371\n",
            "Epoch 29 has loss 5.5727\n",
            "Epoch 30 has loss 5.5224\n",
            "Epoch 31 has loss 5.4869\n",
            "Epoch 32 has loss 5.4687\n",
            "Epoch 33 has loss 5.3197\n",
            "Epoch 34 has loss 5.4211\n",
            "Epoch 35 has loss 5.4691\n",
            "Epoch 36 has loss 5.4127\n",
            "Epoch 37 has loss 5.2827\n",
            "Epoch 38 has loss 5.2889\n",
            "Epoch 39 has loss 5.2052\n",
            "Epoch 40 has loss 5.122\n",
            "Epoch 41 has loss 5.1095\n",
            "Epoch 42 has loss 5.1365\n",
            "Epoch 43 has loss 5.0689\n",
            "Epoch 44 has loss 4.9615\n",
            "Epoch 45 has loss 5.0061\n",
            "Epoch 46 has loss 4.9093\n",
            "Epoch 47 has loss 4.8709\n",
            "Epoch 48 has loss 4.8435\n",
            "Epoch 49 has loss 4.9333\n",
            "  1362 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  1362 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  1362 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  1362 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  24097 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📓 Resultados PostProcessSAGE (sin BioBERT, reentrenado):\n",
            "Hits@10: 0.0102\n",
            "Hits@20: 0.0102\n",
            "Hits@30: 0.0102\n",
            "Hits@40: 0.0102\n",
            "Hits@50: 0.1805\n",
            "Accuracy: 0.8596\n"
          ]
        }
      ],
      "source": [
        "# Regenerar el emb por si se sobrescribió\n",
        "emb = torch.ones(num_nodes, 1).to(device)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()  \n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor.reset_parameters()  \n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📓 Resultados PostProcessSAGE (sin BioBERT, reentrenado):\")\n",
        "for k, v in results_post_bio_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8024\n",
            "Epoch 1 has loss 16.6352\n",
            "Epoch 2 has loss 10.2615\n",
            "Epoch 3 has loss 11.345\n",
            "Epoch 4 has loss 9.1903\n",
            "Epoch 5 has loss 8.1748\n",
            "Epoch 6 has loss 7.8259\n",
            "Epoch 7 has loss 7.6049\n",
            "Epoch 8 has loss 7.5049\n",
            "Epoch 9 has loss 7.501\n",
            "Epoch 10 has loss 7.4312\n",
            "Epoch 11 has loss 7.397\n",
            "Epoch 12 has loss 7.2922\n",
            "Epoch 13 has loss 7.3325\n",
            "Epoch 14 has loss 7.2772\n",
            "Epoch 15 has loss 7.3937\n",
            "Epoch 16 has loss 7.0988\n",
            "Epoch 17 has loss 7.5139\n",
            "Epoch 18 has loss 7.0343\n",
            "Epoch 19 has loss 6.8265\n",
            "Epoch 20 has loss 6.4754\n",
            "Epoch 21 has loss 6.3327\n",
            "Epoch 22 has loss 6.1009\n",
            "Epoch 23 has loss 5.9506\n",
            "Epoch 24 has loss 5.8666\n",
            "Epoch 25 has loss 5.9029\n",
            "Epoch 26 has loss 5.8091\n",
            "Epoch 27 has loss 5.8715\n",
            "Epoch 28 has loss 5.6569\n",
            "Epoch 29 has loss 5.6455\n",
            "Epoch 30 has loss 5.8613\n",
            "Epoch 31 has loss 5.7412\n",
            "Epoch 32 has loss 5.6222\n",
            "Epoch 33 has loss 5.4709\n",
            "Epoch 34 has loss 5.492\n",
            "Epoch 35 has loss 5.2643\n",
            "Epoch 36 has loss 5.4934\n",
            "Epoch 37 has loss 5.3652\n",
            "Epoch 38 has loss 5.4021\n",
            "Epoch 39 has loss 5.2715\n",
            "Epoch 40 has loss 5.4031\n",
            "Epoch 41 has loss 5.3118\n",
            "Epoch 42 has loss 5.1914\n",
            "Epoch 43 has loss 5.1354\n",
            "Epoch 44 has loss 5.1678\n",
            "Epoch 45 has loss 5.1026\n",
            "Epoch 46 has loss 5.2517\n",
            "Epoch 47 has loss 5.2047\n",
            "Epoch 48 has loss 5.1622\n",
            "Epoch 49 has loss 5.0344\n",
            "  1636 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2213 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  3409 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3934 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  3934 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE (con BioBERT como input):\n",
            "Hits@10: 0.0123\n",
            "Hits@20: 0.0166\n",
            "Hits@30: 0.0255\n",
            "Hits@40: 0.0295\n",
            "Hits@50: 0.0295\n",
            "Accuracy: 0.8595\n"
          ]
        }
      ],
      "source": [
        "embedding_biobert_tensor = torch.tensor(embedding_biobert, device=device)\n",
        "embedding_biobert_tensor = embedding_biobert_tensor / embedding_biobert_tensor.norm()\n",
        "embedding_biobert_tensor = embedding_biobert_tensor.detach().float().to(device)\n",
        "\n",
        "aug_emb_bio_post = torch.cat([\n",
        "    emb, embedding_biobert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb_bio_post.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(\n",
        "    model, predictor, aug_emb_bio_post, adj_t, split_edge,\n",
        "    torch.nn.BCELoss(), optimizer, 64 * 1024, 50\n",
        ")\n",
        "\n",
        "results_post_bio_input = test(\n",
        "    model, predictor, aug_emb_bio_post, adj_t,\n",
        "    split_edge[\"valid\"], eval, 64 * 1024\n",
        ")\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE (con BioBERT como input):\")\n",
        "for k, v in results_post_bio_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iukxLZcnANXX",
        "outputId": "e67ae70c-e7b0-4679-dd25-0aae4bb0a308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.9695\n",
            "Epoch 1 has loss 11.7339\n",
            "Epoch 2 has loss 11.1585\n",
            "Epoch 3 has loss 10.7572\n",
            "Epoch 4 has loss 12.2233\n",
            "Epoch 5 has loss 10.9947\n",
            "Epoch 6 has loss 10.3464\n",
            "Epoch 7 has loss 11.5665\n",
            "Epoch 8 has loss 10.7815\n",
            "Epoch 9 has loss 9.913\n",
            "Epoch 10 has loss 9.4589\n",
            "Epoch 11 has loss 9.4333\n",
            "Epoch 12 has loss 9.1178\n",
            "Epoch 13 has loss 9.0144\n",
            "Epoch 14 has loss 8.9101\n",
            "Epoch 15 has loss 8.7861\n",
            "Epoch 16 has loss 8.6069\n",
            "Epoch 17 has loss 8.6508\n",
            "Epoch 18 has loss 8.4342\n",
            "Epoch 19 has loss 8.6566\n",
            "Epoch 20 has loss 8.8055\n",
            "Epoch 21 has loss 8.8743\n",
            "Epoch 22 has loss 8.9786\n",
            "Epoch 23 has loss 8.8706\n",
            "Epoch 24 has loss 8.6463\n",
            "Epoch 25 has loss 8.4592\n",
            "Epoch 26 has loss 8.4146\n",
            "Epoch 27 has loss 8.3716\n",
            "Epoch 28 has loss 8.436\n",
            "Epoch 29 has loss 8.5487\n",
            "Epoch 30 has loss 8.3579\n",
            "Epoch 31 has loss 8.324\n",
            "Epoch 32 has loss 8.4986\n",
            "Epoch 33 has loss 8.3513\n",
            "Epoch 34 has loss 8.2976\n",
            "Epoch 35 has loss 8.3119\n",
            "Epoch 36 has loss 8.2402\n",
            "Epoch 37 has loss 8.2374\n",
            "Epoch 38 has loss 8.2252\n",
            "Epoch 39 has loss 8.2202\n",
            "Epoch 40 has loss 8.2434\n",
            "Epoch 41 has loss 8.2716\n",
            "Epoch 42 has loss 8.3404\n",
            "Epoch 43 has loss 8.2114\n",
            "Epoch 44 has loss 8.1632\n",
            "Epoch 45 has loss 8.2471\n",
            "Epoch 46 has loss 8.216\n",
            "Epoch 47 has loss 8.2986\n",
            "Epoch 48 has loss 8.3434\n",
            "Epoch 49 has loss 8.2936\n",
            "📉 Epoch 0, Loss: 25.6320\n",
            "📉 Epoch 1, Loss: 19.5392\n",
            "📉 Epoch 2, Loss: 19.1767\n",
            "📉 Epoch 3, Loss: 19.0200\n",
            "📉 Epoch 4, Loss: 18.7673\n",
            "📉 Epoch 5, Loss: 15.5011\n",
            "📉 Epoch 6, Loss: 13.0428\n",
            "📉 Epoch 7, Loss: 12.8697\n",
            "📉 Epoch 8, Loss: 12.7885\n",
            "📉 Epoch 9, Loss: 12.7557\n",
            "📉 Epoch 10, Loss: 12.7283\n",
            "📉 Epoch 11, Loss: 12.6932\n",
            "📉 Epoch 12, Loss: 12.6610\n",
            "📉 Epoch 13, Loss: 12.6317\n",
            "📉 Epoch 14, Loss: 12.6158\n",
            "📉 Epoch 15, Loss: 12.5780\n",
            "📉 Epoch 16, Loss: 12.5860\n",
            "📉 Epoch 17, Loss: 12.5470\n",
            "📉 Epoch 18, Loss: 12.5498\n",
            "📉 Epoch 19, Loss: 12.5332\n",
            "📉 Epoch 20, Loss: 12.5246\n",
            "📉 Epoch 21, Loss: 12.5017\n",
            "📉 Epoch 22, Loss: 12.5199\n",
            "📉 Epoch 23, Loss: 12.4855\n",
            "📉 Epoch 24, Loss: 12.4672\n",
            "📉 Epoch 25, Loss: 12.4723\n",
            "📉 Epoch 26, Loss: 12.4495\n",
            "📉 Epoch 27, Loss: 12.5119\n",
            "📉 Epoch 28, Loss: 12.4877\n",
            "📉 Epoch 29, Loss: 12.5454\n",
            "📉 Epoch 30, Loss: 12.5234\n",
            "📉 Epoch 31, Loss: 12.5321\n",
            "📉 Epoch 32, Loss: 12.4791\n",
            "📉 Epoch 33, Loss: 12.4731\n",
            "📉 Epoch 34, Loss: 12.4511\n",
            "📉 Epoch 35, Loss: 12.4425\n",
            "📉 Epoch 36, Loss: 12.4670\n",
            "📉 Epoch 37, Loss: 12.4531\n",
            "📉 Epoch 38, Loss: 12.4179\n",
            "📉 Epoch 39, Loss: 12.4027\n",
            "📉 Epoch 40, Loss: 12.4005\n",
            "📉 Epoch 41, Loss: 12.3831\n",
            "📉 Epoch 42, Loss: 12.4025\n",
            "📉 Epoch 43, Loss: 12.4102\n",
            "📉 Epoch 44, Loss: 12.3733\n",
            "📉 Epoch 45, Loss: 12.3739\n",
            "📉 Epoch 46, Loss: 12.3733\n",
            "📉 Epoch 47, Loss: 12.4056\n",
            "📉 Epoch 48, Loss: 12.4078\n",
            "📉 Epoch 49, Loss: 12.3792\n",
            "  9521 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  11204 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  13465 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  15410 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  16662 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📙 Resultados PostProcessSAGE (con BioBERT en la capa final):\n",
            "Hits@10: 0.0713\n",
            "Hits@20: 0.0839\n",
            "Hits@30: 0.1009\n",
            "Hits@40: 0.1154\n",
            "Hits@50: 0.1248\n",
            "Accuracy: 0.8507\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_bio = model(emb, adj_t)\n",
        "\n",
        "predictor_post_bio_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_bio.size(1),\n",
        "    hidden_channels=z_post_bio.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_post_bio_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_bio_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_bio_final.train()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_bio_final(\n",
        "            z_post_bio[edge[0]], z_post_bio[edge[1]],\n",
        "            embedding_biobert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_post_bio_final, embedding_biobert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_bio_final = test(dummy_model, wrapped_predictor, z_post_bio, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📙 Resultados PostProcessSAGE (con BioBERT en la capa final):\")\n",
        "for k, v in results_post_bio_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source Prompt Type GPT Injection  \\\n",
              "0          SAGEConv            classic           -             -   \n",
              "1          SAGEConv      classic + GPT     default         input   \n",
              "2          SAGEConv            classic     default         final   \n",
              "3      SkipConnSAGE            classic           -             -   \n",
              "4      SkipConnSAGE      classic + GPT     default         input   \n",
              "5      SkipConnSAGE            classic     default         final   \n",
              "6   PostProcessSAGE            classic           -             -   \n",
              "7   PostProcessSAGE      classic + GPT     default         input   \n",
              "8   PostProcessSAGE            classic     default         final   \n",
              "9          SAGEConv           classic            -             -   \n",
              "10         SAGEConv  classic + BioBERT     default         input   \n",
              "11         SAGEConv            classic     default         final   \n",
              "12     SkipConnSAGE            classic           -             -   \n",
              "13     SkipConnSAGE  classic + BioBERT     default         input   \n",
              "14     SkipConnSAGE            classic     default         final   \n",
              "15  PostProcessSAGE            classic           -             -   \n",
              "16  PostProcessSAGE  classic + BioBERT     default         input   \n",
              "17  PostProcessSAGE            classic     default         final   \n",
              "\n",
              "           Predictor  Epochs   Hits@10   Hits@20   Hits@30   Hits@40  \\\n",
              "0             Neural      50  0.132565  0.171662  0.225854  0.250096   \n",
              "1             Neural      50  0.117703  0.176794  0.197260  0.208849   \n",
              "2       Neural + GPT      50  0.032752  0.044678  0.057555  0.062597   \n",
              "3             Neural     100  0.000225  0.000494  0.000802  0.001041   \n",
              "4             Neural     100  0.000809  0.001049  0.001288  0.001858   \n",
              "5       Neural + GPT     100  0.003274  0.007911  0.012196  0.014923   \n",
              "6             Neural      50  0.005206  0.210482  0.265707  0.293904   \n",
              "7             Neural      50  0.007334  0.007334  0.132925  0.153601   \n",
              "8       Neural + GPT      50  0.038385  0.048438  0.053443  0.061211   \n",
              "9             Neural      50  0.112204  0.156620  0.191394  0.222917   \n",
              "10            Neural      50  0.285020  0.370045  0.423773  0.445228   \n",
              "11  Neural + BioBERT      50  0.054461  0.084194  0.098555  0.116526   \n",
              "12            Neural      50  0.017919  0.031164  0.037546  0.045494   \n",
              "13            Neural      50  0.000277  0.000622  0.001079  0.001671   \n",
              "14  Neural + BioBERT      50  0.015762  0.019829  0.028309  0.033471   \n",
              "15            Neural      50  0.010203  0.010203  0.010203  0.010203   \n",
              "16            Neural      50  0.003813  0.003813  0.003813  0.003813   \n",
              "17  Neural + BioBERT      50  0.071324  0.083932  0.100870  0.115440   \n",
              "\n",
              "     Hits@50  Accuracy  \n",
              "0   0.273229    0.9010  \n",
              "1   0.219119    0.8920  \n",
              "2   0.072718    0.8618  \n",
              "3   0.001326    0.7420  \n",
              "4   0.002277    0.7520  \n",
              "5   0.017664    0.7774  \n",
              "6   0.322446    0.8781  \n",
              "7   0.181296    0.8706  \n",
              "8   0.068358    0.8070  \n",
              "9   0.243204    0.9000  \n",
              "10  0.485149    0.9281  \n",
              "11  0.126265    0.8887  \n",
              "12  0.050761    0.7755  \n",
              "13  0.002682    0.7881  \n",
              "14  0.037501    0.8014  \n",
              "15  0.180517    0.8596  \n",
              "16  0.003813    0.8574  \n",
              "17  0.124819    0.8507  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "new_rows = [\n",
        "    # SkipConnSAGE con BioBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_base},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_input},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_skipconn_bio_final},\n",
        "\n",
        "    # PostProcessSAGE con BioBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_base},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_input},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_post_bio_final},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "# Mostramos la tabla actualizada\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7138\n",
            "Epoch 1 has loss 10.3959\n",
            "Epoch 2 has loss 9.6222\n",
            "Epoch 3 has loss 9.7832\n",
            "Epoch 4 has loss 8.7808\n",
            "Epoch 5 has loss 8.3726\n",
            "Epoch 6 has loss 7.436\n",
            "Epoch 7 has loss 6.9532\n",
            "Epoch 8 has loss 6.5721\n",
            "Epoch 9 has loss 6.2184\n",
            "Epoch 10 has loss 6.0719\n",
            "Epoch 11 has loss 6.0029\n",
            "Epoch 12 has loss 5.815\n",
            "Epoch 13 has loss 5.9071\n",
            "Epoch 14 has loss 5.8534\n",
            "Epoch 15 has loss 5.6508\n",
            "Epoch 16 has loss 5.6597\n",
            "Epoch 17 has loss 5.4979\n",
            "Epoch 18 has loss 5.5011\n",
            "Epoch 19 has loss 5.4295\n",
            "Epoch 20 has loss 5.3046\n",
            "Epoch 21 has loss 5.1396\n",
            "Epoch 22 has loss 5.1815\n",
            "Epoch 23 has loss 5.0378\n",
            "Epoch 24 has loss 4.9868\n",
            "Epoch 25 has loss 5.0366\n",
            "Epoch 26 has loss 4.732\n",
            "Epoch 27 has loss 4.7664\n",
            "Epoch 28 has loss 4.7142\n",
            "Epoch 29 has loss 4.6265\n",
            "Epoch 30 has loss 4.572\n",
            "Epoch 31 has loss 4.4822\n",
            "Epoch 32 has loss 4.5512\n",
            "Epoch 33 has loss 4.7095\n",
            "Epoch 34 has loss 4.4573\n",
            "Epoch 35 has loss 4.3366\n",
            "Epoch 36 has loss 4.4081\n",
            "Epoch 37 has loss 4.331\n",
            "Epoch 38 has loss 4.2841\n",
            "Epoch 39 has loss 4.2302\n",
            "Epoch 40 has loss 4.3152\n",
            "Epoch 41 has loss 4.2705\n",
            "Epoch 42 has loss 4.2071\n",
            "Epoch 43 has loss 4.1631\n",
            "Epoch 44 has loss 4.1826\n",
            "Epoch 45 has loss 4.0975\n",
            "Epoch 46 has loss 4.1339\n",
            "Epoch 47 has loss 4.119\n",
            "Epoch 48 has loss 4.1031\n",
            "Epoch 49 has loss 4.2628\n",
            "  19275 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  32695 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  38653 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  40950 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  48436 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SAGE (sin SciBERT):\n",
            "Hits@10: 0.1444\n",
            "Hits@20: 0.2449\n",
            "Hits@30: 0.2896\n",
            "Hits@40: 0.3068\n",
            "Hits@50: 0.3628\n",
            "Accuracy: 0.9031\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_sci_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SAGE (sin SciBERT):\")\n",
        "for k, v in results_sage_sci_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Obtener embedding de SciBERT\n",
        "model_sci = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
        "embedding_scibert = model_sci.encode(gpt_response_text)\n",
        "embedding_scibert_tensor = torch.tensor(embedding_scibert, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5369\n",
            "Epoch 1 has loss 9.888\n",
            "Epoch 2 has loss 9.0507\n",
            "Epoch 3 has loss 7.8869\n",
            "Epoch 4 has loss 7.667\n",
            "Epoch 5 has loss 7.5815\n",
            "Epoch 6 has loss 7.3016\n",
            "Epoch 7 has loss 7.1731\n",
            "Epoch 8 has loss 7.1633\n",
            "Epoch 9 has loss 7.2148\n",
            "Epoch 10 has loss 7.1557\n",
            "Epoch 11 has loss 6.8154\n",
            "Epoch 12 has loss 6.6398\n",
            "Epoch 13 has loss 6.4346\n",
            "Epoch 14 has loss 6.7325\n",
            "Epoch 15 has loss 6.3198\n",
            "Epoch 16 has loss 6.2475\n",
            "Epoch 17 has loss 6.2079\n",
            "Epoch 18 has loss 6.0713\n",
            "Epoch 19 has loss 5.9529\n",
            "Epoch 20 has loss 5.6718\n",
            "Epoch 21 has loss 5.4611\n",
            "Epoch 22 has loss 5.3965\n",
            "Epoch 23 has loss 5.1387\n",
            "Epoch 24 has loss 5.1028\n",
            "Epoch 25 has loss 4.9966\n",
            "Epoch 26 has loss 4.9391\n",
            "Epoch 27 has loss 4.9197\n",
            "Epoch 28 has loss 4.9132\n",
            "Epoch 29 has loss 4.9884\n",
            "Epoch 30 has loss 4.7319\n",
            "Epoch 31 has loss 4.6229\n",
            "Epoch 32 has loss 4.6077\n",
            "Epoch 33 has loss 4.5991\n",
            "Epoch 34 has loss 4.5365\n",
            "Epoch 35 has loss 4.4096\n",
            "Epoch 36 has loss 4.3747\n",
            "Epoch 37 has loss 4.3338\n",
            "Epoch 38 has loss 4.5031\n",
            "Epoch 39 has loss 4.4268\n",
            "Epoch 40 has loss 4.3158\n",
            "Epoch 41 has loss 4.2521\n",
            "Epoch 42 has loss 4.3874\n",
            "Epoch 43 has loss 4.2636\n",
            "Epoch 44 has loss 4.2215\n",
            "Epoch 45 has loss 4.2874\n",
            "Epoch 46 has loss 4.2698\n",
            "Epoch 47 has loss 4.0453\n",
            "Epoch 48 has loss 4.036\n",
            "Epoch 49 has loss 4.1014\n",
            "  21234 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  33284 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  37218 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  38980 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  41340 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGE (con SciBERT como input):\n",
            "Hits@10: 0.1591\n",
            "Hits@20: 0.2493\n",
            "Hits@30: 0.2788\n",
            "Hits@40: 0.2920\n",
            "Hits@50: 0.3097\n",
            "Accuracy: 0.8972\n"
          ]
        }
      ],
      "source": [
        "\n",
        "aug_emb_sci = torch.cat([\n",
        "    emb, embedding_scibert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb_sci.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_sci, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_sci_input = test(model, predictor, aug_emb_sci, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGE (con SciBERT como input):\")\n",
        "for k, v in results_sage_sci_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6364\n",
            "Epoch 1 has loss 10.2599\n",
            "Epoch 2 has loss 10.1835\n",
            "Epoch 3 has loss 10.1702\n",
            "Epoch 4 has loss 10.0473\n",
            "Epoch 5 has loss 10.1421\n",
            "Epoch 6 has loss 10.1072\n",
            "Epoch 7 has loss 10.0349\n",
            "Epoch 8 has loss 9.99\n",
            "Epoch 9 has loss 9.9975\n",
            "Epoch 10 has loss 9.9669\n",
            "Epoch 11 has loss 9.7985\n",
            "Epoch 12 has loss 9.7429\n",
            "Epoch 13 has loss 9.6376\n",
            "Epoch 14 has loss 9.5756\n",
            "Epoch 15 has loss 9.5621\n",
            "Epoch 16 has loss 9.5465\n",
            "Epoch 17 has loss 9.5167\n",
            "Epoch 18 has loss 9.51\n",
            "Epoch 19 has loss 9.5057\n",
            "Epoch 20 has loss 9.4874\n",
            "Epoch 21 has loss 9.4764\n",
            "Epoch 22 has loss 9.4566\n",
            "Epoch 23 has loss 9.441\n",
            "Epoch 24 has loss 9.4404\n",
            "Epoch 25 has loss 9.4528\n",
            "Epoch 26 has loss 9.4354\n",
            "Epoch 27 has loss 9.4416\n",
            "Epoch 28 has loss 9.4266\n",
            "Epoch 29 has loss 9.4196\n",
            "Epoch 30 has loss 9.4241\n",
            "Epoch 31 has loss 9.4126\n",
            "Epoch 32 has loss 9.414\n",
            "Epoch 33 has loss 9.4046\n",
            "Epoch 34 has loss 9.4107\n",
            "Epoch 35 has loss 9.4099\n",
            "Epoch 36 has loss 9.4002\n",
            "Epoch 37 has loss 9.4033\n",
            "Epoch 38 has loss 9.4079\n",
            "Epoch 39 has loss 9.416\n",
            "Epoch 40 has loss 9.3953\n",
            "Epoch 41 has loss 9.3999\n",
            "Epoch 42 has loss 9.4005\n",
            "Epoch 43 has loss 9.3903\n",
            "Epoch 44 has loss 9.3862\n",
            "Epoch 45 has loss 9.3663\n",
            "Epoch 46 has loss 9.3879\n",
            "Epoch 47 has loss 9.3829\n",
            "Epoch 48 has loss 9.3897\n",
            "Epoch 49 has loss 9.388\n",
            "📉 Epoch 0, Loss: 21.6425\n",
            "📉 Epoch 1, Loss: 14.5085\n",
            "📉 Epoch 2, Loss: 13.2724\n",
            "📉 Epoch 3, Loss: 12.4960\n",
            "📉 Epoch 4, Loss: 11.9832\n",
            "📉 Epoch 5, Loss: 11.6825\n",
            "📉 Epoch 6, Loss: 11.4639\n",
            "📉 Epoch 7, Loss: 11.3228\n",
            "📉 Epoch 8, Loss: 11.2630\n",
            "📉 Epoch 9, Loss: 11.1079\n",
            "📉 Epoch 10, Loss: 11.0280\n",
            "📉 Epoch 11, Loss: 11.0016\n",
            "📉 Epoch 12, Loss: 10.9639\n",
            "📉 Epoch 13, Loss: 10.9433\n",
            "📉 Epoch 14, Loss: 10.8934\n",
            "📉 Epoch 15, Loss: 10.8582\n",
            "📉 Epoch 16, Loss: 10.8284\n",
            "📉 Epoch 17, Loss: 10.7911\n",
            "📉 Epoch 18, Loss: 10.9874\n",
            "📉 Epoch 19, Loss: 10.9883\n",
            "📉 Epoch 20, Loss: 10.8112\n",
            "📉 Epoch 21, Loss: 10.7560\n",
            "📉 Epoch 22, Loss: 10.7314\n",
            "📉 Epoch 23, Loss: 10.7975\n",
            "📉 Epoch 24, Loss: 10.7461\n",
            "📉 Epoch 25, Loss: 10.7086\n",
            "📉 Epoch 26, Loss: 10.6792\n",
            "📉 Epoch 27, Loss: 10.6376\n",
            "📉 Epoch 28, Loss: 10.6367\n",
            "📉 Epoch 29, Loss: 10.6334\n",
            "📉 Epoch 30, Loss: 10.6168\n",
            "📉 Epoch 31, Loss: 10.6533\n",
            "📉 Epoch 32, Loss: 10.6030\n",
            "📉 Epoch 33, Loss: 10.5836\n",
            "📉 Epoch 34, Loss: 10.5694\n",
            "📉 Epoch 35, Loss: 10.5527\n",
            "📉 Epoch 36, Loss: 10.5854\n",
            "📉 Epoch 37, Loss: 10.5552\n",
            "📉 Epoch 38, Loss: 10.6018\n",
            "📉 Epoch 39, Loss: 10.6460\n",
            "📉 Epoch 40, Loss: 10.5516\n",
            "📉 Epoch 41, Loss: 10.5408\n",
            "📉 Epoch 42, Loss: 10.5256\n",
            "📉 Epoch 43, Loss: 10.5389\n",
            "📉 Epoch 44, Loss: 10.5437\n",
            "📉 Epoch 45, Loss: 10.4988\n",
            "📉 Epoch 46, Loss: 10.4883\n",
            "📉 Epoch 47, Loss: 10.5089\n",
            "📉 Epoch 48, Loss: 10.4873\n",
            "📉 Epoch 49, Loss: 10.4778\n",
            "  10456 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  24344 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26835 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  30102 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  32462 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧠 Resultados SAGE (con SciBERT en capa final):\n",
            "Hits@10: 0.0783\n",
            "Hits@20: 0.1824\n",
            "Hits@30: 0.2010\n",
            "Hits@40: 0.2255\n",
            "Hits@50: 0.2432\n",
            "Accuracy: 0.8985\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_sci_final = model(emb, adj_t)\n",
        "\n",
        "predictor_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_sci_final.size(1),\n",
        "    hidden_channels=z_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_sci_final(\n",
        "            z_sci_final[edge[0]], z_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_sci = WrappedPredictor(predictor_sci_final, embedding_scibert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_sci_final = test(dummy_model, wrapped_predictor_sci, z_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧠 Resultados SAGE (con SciBERT en capa final):\")\n",
        "for k, v in results_sage_sci_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4306\n",
            "Epoch 1 has loss 10.8537\n",
            "Epoch 2 has loss 10.485\n",
            "Epoch 3 has loss 10.4319\n",
            "Epoch 4 has loss 9.9027\n",
            "Epoch 5 has loss 10.1486\n",
            "Epoch 6 has loss 9.9936\n",
            "Epoch 7 has loss 9.5037\n",
            "Epoch 8 has loss 9.3254\n",
            "Epoch 9 has loss 9.6669\n",
            "Epoch 10 has loss 9.9132\n",
            "Epoch 11 has loss 9.822\n",
            "Epoch 12 has loss 9.6943\n",
            "Epoch 13 has loss 9.4046\n",
            "Epoch 14 has loss 9.3556\n",
            "Epoch 15 has loss 9.2739\n",
            "Epoch 16 has loss 9.4139\n",
            "Epoch 17 has loss 9.6909\n",
            "Epoch 18 has loss 9.3212\n",
            "Epoch 19 has loss 9.1816\n",
            "Epoch 20 has loss 8.9714\n",
            "Epoch 21 has loss 9.2362\n",
            "Epoch 22 has loss 8.859\n",
            "Epoch 23 has loss 8.6238\n",
            "Epoch 24 has loss 9.1073\n",
            "Epoch 25 has loss 8.6443\n",
            "Epoch 26 has loss 8.6029\n",
            "Epoch 27 has loss 8.8393\n",
            "Epoch 28 has loss 8.4105\n",
            "Epoch 29 has loss 8.0499\n",
            "Epoch 30 has loss 8.888\n",
            "Epoch 31 has loss 8.4494\n",
            "Epoch 32 has loss 9.0284\n",
            "Epoch 33 has loss 9.2468\n",
            "Epoch 34 has loss 8.4208\n",
            "Epoch 35 has loss 8.4228\n",
            "Epoch 36 has loss 8.1858\n",
            "Epoch 37 has loss 8.1428\n",
            "Epoch 38 has loss 7.8727\n",
            "Epoch 39 has loss 7.6815\n",
            "Epoch 40 has loss 8.1338\n",
            "Epoch 41 has loss 8.0524\n",
            "Epoch 42 has loss 7.9038\n",
            "Epoch 43 has loss 7.7329\n",
            "Epoch 44 has loss 7.7373\n",
            "Epoch 45 has loss 7.7201\n",
            "Epoch 46 has loss 7.6484\n",
            "Epoch 47 has loss 7.5202\n",
            "Epoch 48 has loss 7.6092\n",
            "Epoch 49 has loss 7.5923\n",
            "  1093 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2224 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2581 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3383 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  3901 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔬 Resultados SkipConnSAGE (sin SciBERT):\n",
            "Hits@10: 0.0082\n",
            "Hits@20: 0.0167\n",
            "Hits@30: 0.0193\n",
            "Hits@40: 0.0253\n",
            "Hits@50: 0.0292\n",
            "Accuracy: 0.7761\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_sci_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔬 Resultados SkipConnSAGE (sin SciBERT):\")\n",
        "for k, v in results_skipconn_sci_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4102\n",
            "Epoch 1 has loss 10.2998\n",
            "Epoch 2 has loss 10.199\n",
            "Epoch 3 has loss 9.9035\n",
            "Epoch 4 has loss 9.8973\n",
            "Epoch 5 has loss 10.2335\n",
            "Epoch 6 has loss 10.087\n",
            "Epoch 7 has loss 9.9595\n",
            "Epoch 8 has loss 10.0971\n",
            "Epoch 9 has loss 9.9449\n",
            "Epoch 10 has loss 9.6265\n",
            "Epoch 11 has loss 9.902\n",
            "Epoch 12 has loss 10.0125\n",
            "Epoch 13 has loss 9.9304\n",
            "Epoch 14 has loss 9.7765\n",
            "Epoch 15 has loss 9.5029\n",
            "Epoch 16 has loss 9.4259\n",
            "Epoch 17 has loss 9.2681\n",
            "Epoch 18 has loss 9.5873\n",
            "Epoch 19 has loss 9.0901\n",
            "Epoch 20 has loss 8.8363\n",
            "Epoch 21 has loss 8.6796\n",
            "Epoch 22 has loss 9.0226\n",
            "Epoch 23 has loss 8.691\n",
            "Epoch 24 has loss 9.0473\n",
            "Epoch 25 has loss 8.7563\n",
            "Epoch 26 has loss 8.7643\n",
            "Epoch 27 has loss 8.4215\n",
            "Epoch 28 has loss 8.9582\n",
            "Epoch 29 has loss 8.7223\n",
            "Epoch 30 has loss 8.6969\n",
            "Epoch 31 has loss 8.2491\n",
            "Epoch 32 has loss 8.7004\n",
            "Epoch 33 has loss 8.7292\n",
            "Epoch 34 has loss 8.2548\n",
            "Epoch 35 has loss 8.2295\n",
            "Epoch 36 has loss 8.4708\n",
            "Epoch 37 has loss 8.346\n",
            "Epoch 38 has loss 8.7355\n",
            "Epoch 39 has loss 8.2416\n",
            "Epoch 40 has loss 8.0961\n",
            "Epoch 41 has loss 8.1096\n",
            "Epoch 42 has loss 8.0243\n",
            "Epoch 43 has loss 7.6463\n",
            "Epoch 44 has loss 7.4028\n",
            "Epoch 45 has loss 7.1445\n",
            "Epoch 46 has loss 7.024\n",
            "Epoch 47 has loss 7.411\n",
            "Epoch 48 has loss 7.8213\n",
            "Epoch 49 has loss 7.0572\n",
            "  837 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  1850 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2575 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3390 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4199 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SkipConnSAGE (con SciBERT como input):\n",
            "Hits@10: 0.0063\n",
            "Hits@20: 0.0139\n",
            "Hits@30: 0.0193\n",
            "Hits@40: 0.0254\n",
            "Hits@50: 0.0315\n",
            "Accuracy: 0.7997\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci = torch.cat([\n",
        "    emb, embedding_scibert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb_sci.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_sci, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_sci_input = test(model, predictor, aug_emb_sci, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SkipConnSAGE (con SciBERT como input):\")\n",
        "for k, v in results_skipconn_sci_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.0671\n",
            "Epoch 1 has loss 10.6182\n",
            "Epoch 2 has loss 10.4409\n",
            "Epoch 3 has loss 10.3505\n",
            "Epoch 4 has loss 10.2985\n",
            "Epoch 5 has loss 10.2515\n",
            "Epoch 6 has loss 10.2276\n",
            "Epoch 7 has loss 10.236\n",
            "Epoch 8 has loss 10.2488\n",
            "Epoch 9 has loss 10.2226\n",
            "Epoch 10 has loss 10.1539\n",
            "Epoch 11 has loss 10.1537\n",
            "Epoch 12 has loss 10.1904\n",
            "Epoch 13 has loss 10.1576\n",
            "Epoch 14 has loss 10.1451\n",
            "Epoch 15 has loss 10.072\n",
            "Epoch 16 has loss 10.1023\n",
            "Epoch 17 has loss 10.0729\n",
            "Epoch 18 has loss 10.0984\n",
            "Epoch 19 has loss 10.1713\n",
            "Epoch 20 has loss 10.1152\n",
            "Epoch 21 has loss 10.0864\n",
            "Epoch 22 has loss 10.084\n",
            "Epoch 23 has loss 10.0735\n",
            "Epoch 24 has loss 10.0434\n",
            "Epoch 25 has loss 10.0856\n",
            "Epoch 26 has loss 10.0206\n",
            "Epoch 27 has loss 10.001\n",
            "Epoch 28 has loss 10.0181\n",
            "Epoch 29 has loss 9.9987\n",
            "Epoch 30 has loss 9.9818\n",
            "Epoch 31 has loss 9.9613\n",
            "Epoch 32 has loss 9.9382\n",
            "Epoch 33 has loss 9.9498\n",
            "Epoch 34 has loss 10.206\n",
            "Epoch 35 has loss 10.0414\n",
            "Epoch 36 has loss 10.0042\n",
            "Epoch 37 has loss 9.9622\n",
            "Epoch 38 has loss 9.9334\n",
            "Epoch 39 has loss 9.9422\n",
            "Epoch 40 has loss 9.8984\n",
            "Epoch 41 has loss 9.9167\n",
            "Epoch 42 has loss 9.8845\n",
            "Epoch 43 has loss 9.8522\n",
            "Epoch 44 has loss 9.8496\n",
            "Epoch 45 has loss 9.7737\n",
            "Epoch 46 has loss 9.7429\n",
            "Epoch 47 has loss 9.7278\n",
            "Epoch 48 has loss 9.7305\n",
            "Epoch 49 has loss 9.6566\n",
            "📉 Epoch 0, Loss: 25.6642\n",
            "📉 Epoch 1, Loss: 22.9472\n",
            "📉 Epoch 2, Loss: 21.7156\n",
            "📉 Epoch 3, Loss: 18.6903\n",
            "📉 Epoch 4, Loss: 17.9927\n",
            "📉 Epoch 5, Loss: 17.7678\n",
            "📉 Epoch 6, Loss: 17.6366\n",
            "📉 Epoch 7, Loss: 17.5542\n",
            "📉 Epoch 8, Loss: 17.5424\n",
            "📉 Epoch 9, Loss: 17.4213\n",
            "📉 Epoch 10, Loss: 17.4387\n",
            "📉 Epoch 11, Loss: 17.3692\n",
            "📉 Epoch 12, Loss: 17.3414\n",
            "📉 Epoch 13, Loss: 17.3155\n",
            "📉 Epoch 14, Loss: 17.2653\n",
            "📉 Epoch 15, Loss: 17.2479\n",
            "📉 Epoch 16, Loss: 17.2944\n",
            "📉 Epoch 17, Loss: 17.2091\n",
            "📉 Epoch 18, Loss: 17.1981\n",
            "📉 Epoch 19, Loss: 17.1949\n",
            "📉 Epoch 20, Loss: 17.1846\n",
            "📉 Epoch 21, Loss: 17.1518\n",
            "📉 Epoch 22, Loss: 17.1587\n",
            "📉 Epoch 23, Loss: 17.1619\n",
            "📉 Epoch 24, Loss: 17.0993\n",
            "📉 Epoch 25, Loss: 17.1607\n",
            "📉 Epoch 26, Loss: 17.1079\n",
            "📉 Epoch 27, Loss: 17.1258\n",
            "📉 Epoch 28, Loss: 17.1122\n",
            "📉 Epoch 29, Loss: 17.1077\n",
            "📉 Epoch 30, Loss: 17.1416\n",
            "📉 Epoch 31, Loss: 17.0685\n",
            "📉 Epoch 32, Loss: 17.2775\n",
            "📉 Epoch 33, Loss: 17.1006\n",
            "📉 Epoch 34, Loss: 17.1015\n",
            "📉 Epoch 35, Loss: 17.0041\n",
            "📉 Epoch 36, Loss: 17.2145\n",
            "📉 Epoch 37, Loss: 17.1282\n",
            "📉 Epoch 38, Loss: 16.9363\n",
            "📉 Epoch 39, Loss: 16.9322\n",
            "📉 Epoch 40, Loss: 16.9469\n",
            "📉 Epoch 41, Loss: 17.0919\n",
            "📉 Epoch 42, Loss: 16.9281\n",
            "📉 Epoch 43, Loss: 16.9278\n",
            "📉 Epoch 44, Loss: 17.0386\n",
            "📉 Epoch 45, Loss: 17.0171\n",
            "📉 Epoch 46, Loss: 17.0616\n",
            "📉 Epoch 47, Loss: 16.8376\n",
            "📉 Epoch 48, Loss: 16.8274\n",
            "📉 Epoch 49, Loss: 16.9041\n",
            "  3513 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4457 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5574 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  6541 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  7702 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE (con SciBERT en la capa final):\n",
            "Hits@10: 0.0263\n",
            "Hits@20: 0.0334\n",
            "Hits@30: 0.0418\n",
            "Hits@40: 0.0490\n",
            "Hits@50: 0.0577\n",
            "Accuracy: 0.7884\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_sci = model(emb, adj_t)\n",
        "\n",
        "predictor_skip_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_sci.size(1),\n",
        "    hidden_channels=z_skip_sci.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_skip_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_skip_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skip_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skip_sci_final(\n",
        "            z_skip_sci[edge[0]], z_skip_sci[edge[1]],\n",
        "            embedding_scibert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_skip_sci_final, embedding_scibert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skipconn_sci_final = test(dummy_model, wrapped_predictor, z_skip_sci, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE (con SciBERT en la capa final):\")\n",
        "for k, v in results_skipconn_sci_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8951\n",
            "Epoch 1 has loss 12.3414\n",
            "Epoch 2 has loss 22.7907\n",
            "Epoch 3 has loss 9.9834\n",
            "Epoch 4 has loss 9.1353\n",
            "Epoch 5 has loss 8.3355\n",
            "Epoch 6 has loss 7.9041\n",
            "Epoch 7 has loss 7.7195\n",
            "Epoch 8 has loss 7.6973\n",
            "Epoch 9 has loss 7.6634\n",
            "Epoch 10 has loss 7.5237\n",
            "Epoch 11 has loss 7.5477\n",
            "Epoch 12 has loss 7.4671\n",
            "Epoch 13 has loss 7.4684\n",
            "Epoch 14 has loss 7.352\n",
            "Epoch 15 has loss 7.3804\n",
            "Epoch 16 has loss 7.3271\n",
            "Epoch 17 has loss 7.1546\n",
            "Epoch 18 has loss 7.0525\n",
            "Epoch 19 has loss 7.0571\n",
            "Epoch 20 has loss 6.775\n",
            "Epoch 21 has loss 6.5071\n",
            "Epoch 22 has loss 6.4289\n",
            "Epoch 23 has loss 6.5213\n",
            "Epoch 24 has loss 6.2117\n",
            "Epoch 25 has loss 6.2258\n",
            "Epoch 26 has loss 6.1548\n",
            "Epoch 27 has loss 5.9279\n",
            "Epoch 28 has loss 5.7877\n",
            "Epoch 29 has loss 5.9827\n",
            "Epoch 30 has loss 5.6616\n",
            "Epoch 31 has loss 5.5867\n",
            "Epoch 32 has loss 5.4092\n",
            "Epoch 33 has loss 5.6029\n",
            "Epoch 34 has loss 5.9068\n",
            "Epoch 35 has loss 5.6695\n",
            "Epoch 36 has loss 5.3378\n",
            "Epoch 37 has loss 5.3013\n",
            "Epoch 38 has loss 5.2051\n",
            "Epoch 39 has loss 4.9972\n",
            "Epoch 40 has loss 4.9906\n",
            "Epoch 41 has loss 4.958\n",
            "Epoch 42 has loss 4.9856\n",
            "Epoch 43 has loss 4.9121\n",
            "Epoch 44 has loss 4.8958\n",
            "Epoch 45 has loss 4.9671\n",
            "Epoch 46 has loss 4.9767\n",
            "Epoch 47 has loss 4.8619\n",
            "Epoch 48 has loss 4.7674\n",
            "Epoch 49 has loss 4.7969\n",
            "  3833 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  34892 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  39119 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  40549 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  41846 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE (sin SciBERT):\n",
            "Hits@10: 0.0287\n",
            "Hits@20: 0.2614\n",
            "Hits@30: 0.2931\n",
            "Hits@40: 0.3038\n",
            "Hits@50: 0.3135\n",
            "Accuracy: 0.8717\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_base = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE (sin SciBERT):\")\n",
        "for k, v in results_post_sci_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1555\n",
            "Epoch 1 has loss 11.7869\n",
            "Epoch 2 has loss 11.7469\n",
            "Epoch 3 has loss 44.2268\n",
            "Epoch 4 has loss 27.4437\n",
            "Epoch 5 has loss 9.8128\n",
            "Epoch 6 has loss 9.3026\n",
            "Epoch 7 has loss 8.6172\n",
            "Epoch 8 has loss 8.1866\n",
            "Epoch 9 has loss 7.8958\n",
            "Epoch 10 has loss 7.9107\n",
            "Epoch 11 has loss 7.7402\n",
            "Epoch 12 has loss 7.4985\n",
            "Epoch 13 has loss 7.4581\n",
            "Epoch 14 has loss 7.3765\n",
            "Epoch 15 has loss 7.3648\n",
            "Epoch 16 has loss 7.1716\n",
            "Epoch 17 has loss 6.8144\n",
            "Epoch 18 has loss 6.8067\n",
            "Epoch 19 has loss 6.6152\n",
            "Epoch 20 has loss 6.6694\n",
            "Epoch 21 has loss 6.4492\n",
            "Epoch 22 has loss 6.3133\n",
            "Epoch 23 has loss 6.3504\n",
            "Epoch 24 has loss 6.4047\n",
            "Epoch 25 has loss 6.239\n",
            "Epoch 26 has loss 6.1826\n",
            "Epoch 27 has loss 5.9732\n",
            "Epoch 28 has loss 5.7308\n",
            "Epoch 29 has loss 5.5871\n",
            "Epoch 30 has loss 5.5359\n",
            "Epoch 31 has loss 5.4789\n",
            "Epoch 32 has loss 5.4377\n",
            "Epoch 33 has loss 5.3242\n",
            "Epoch 34 has loss 5.2419\n",
            "Epoch 35 has loss 5.2721\n",
            "Epoch 36 has loss 5.3706\n",
            "Epoch 37 has loss 5.1828\n",
            "Epoch 38 has loss 5.137\n",
            "Epoch 39 has loss 5.1253\n",
            "Epoch 40 has loss 5.0475\n",
            "Epoch 41 has loss 5.128\n",
            "Epoch 42 has loss 4.9842\n",
            "Epoch 43 has loss 4.943\n",
            "Epoch 44 has loss 4.8954\n",
            "Epoch 45 has loss 4.8803\n",
            "Epoch 46 has loss 4.8317\n",
            "Epoch 47 has loss 4.7319\n",
            "Epoch 48 has loss 4.6007\n",
            "Epoch 49 has loss 4.5458\n",
            "  13332 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  17611 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  19311 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  20650 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  22202 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📗 Resultados PostProcessSAGE (con SciBERT como input):\n",
            "Hits@10: 0.0999\n",
            "Hits@20: 0.1319\n",
            "Hits@30: 0.1447\n",
            "Hits@40: 0.1547\n",
            "Hits@50: 0.1663\n",
            "Accuracy: 0.8644\n"
          ]
        }
      ],
      "source": [
        "aug_emb_post_sci = torch.cat([\n",
        "    emb, embedding_scibert_tensor.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb_post_sci.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_post_sci, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_input = test(model, predictor, aug_emb_post_sci, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📗 Resultados PostProcessSAGE (con SciBERT como input):\")\n",
        "for k, v in results_post_sci_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.9687\n",
            "Epoch 1 has loss 11.6965\n",
            "Epoch 2 has loss 11.1833\n",
            "Epoch 3 has loss 11.064\n",
            "Epoch 4 has loss 11.0778\n",
            "Epoch 5 has loss 11.4885\n",
            "Epoch 6 has loss 11.4075\n",
            "Epoch 7 has loss 10.8116\n",
            "Epoch 8 has loss 10.4377\n",
            "Epoch 9 has loss 10.3486\n",
            "Epoch 10 has loss 10.0102\n",
            "Epoch 11 has loss 9.6015\n",
            "Epoch 12 has loss 9.2683\n",
            "Epoch 13 has loss 9.0183\n",
            "Epoch 14 has loss 8.9251\n",
            "Epoch 15 has loss 8.8673\n",
            "Epoch 16 has loss 9.0487\n",
            "Epoch 17 has loss 9.4705\n",
            "Epoch 18 has loss 9.19\n",
            "Epoch 19 has loss 9.0139\n",
            "Epoch 20 has loss 8.9917\n",
            "Epoch 21 has loss 8.8013\n",
            "Epoch 22 has loss 8.7987\n",
            "Epoch 23 has loss 8.71\n",
            "Epoch 24 has loss 8.7508\n",
            "Epoch 25 has loss 8.9065\n",
            "Epoch 26 has loss 8.6347\n",
            "Epoch 27 has loss 8.53\n",
            "Epoch 28 has loss 8.6409\n",
            "Epoch 29 has loss 8.6552\n",
            "Epoch 30 has loss 8.5222\n",
            "Epoch 31 has loss 8.5203\n",
            "Epoch 32 has loss 8.5647\n",
            "Epoch 33 has loss 8.9236\n",
            "Epoch 34 has loss 8.6124\n",
            "Epoch 35 has loss 8.4505\n",
            "Epoch 36 has loss 8.3272\n",
            "Epoch 37 has loss 8.4575\n",
            "Epoch 38 has loss 8.2564\n",
            "Epoch 39 has loss 8.3179\n",
            "Epoch 40 has loss 8.598\n",
            "Epoch 41 has loss 8.3595\n",
            "Epoch 42 has loss 8.3759\n",
            "Epoch 43 has loss 8.319\n",
            "Epoch 44 has loss 8.3498\n",
            "Epoch 45 has loss 8.3581\n",
            "Epoch 46 has loss 8.4291\n",
            "Epoch 47 has loss 8.2463\n",
            "Epoch 48 has loss 8.2312\n",
            "Epoch 49 has loss 8.3332\n",
            "📉 Epoch 0, Loss: 24.0054\n",
            "📉 Epoch 1, Loss: 20.3656\n",
            "📉 Epoch 2, Loss: 19.7369\n",
            "📉 Epoch 3, Loss: 15.5675\n",
            "📉 Epoch 4, Loss: 14.1578\n",
            "📉 Epoch 5, Loss: 13.7639\n",
            "📉 Epoch 6, Loss: 13.4224\n",
            "📉 Epoch 7, Loss: 13.2275\n",
            "📉 Epoch 8, Loss: 13.1520\n",
            "📉 Epoch 9, Loss: 13.0942\n",
            "📉 Epoch 10, Loss: 12.9913\n",
            "📉 Epoch 11, Loss: 13.0007\n",
            "📉 Epoch 12, Loss: 12.9328\n",
            "📉 Epoch 13, Loss: 12.8978\n",
            "📉 Epoch 14, Loss: 12.9040\n",
            "📉 Epoch 15, Loss: 12.8178\n",
            "📉 Epoch 16, Loss: 12.8378\n",
            "📉 Epoch 17, Loss: 12.7814\n",
            "📉 Epoch 18, Loss: 12.8207\n",
            "📉 Epoch 19, Loss: 12.7693\n",
            "📉 Epoch 20, Loss: 12.8059\n",
            "📉 Epoch 21, Loss: 12.7515\n",
            "📉 Epoch 22, Loss: 12.7009\n",
            "📉 Epoch 23, Loss: 12.6740\n",
            "📉 Epoch 24, Loss: 13.3738\n",
            "📉 Epoch 25, Loss: 12.8883\n",
            "📉 Epoch 26, Loss: 12.6751\n",
            "📉 Epoch 27, Loss: 12.6576\n",
            "📉 Epoch 28, Loss: 12.6916\n",
            "📉 Epoch 29, Loss: 12.6318\n",
            "📉 Epoch 30, Loss: 12.6064\n",
            "📉 Epoch 31, Loss: 12.6318\n",
            "📉 Epoch 32, Loss: 12.6124\n",
            "📉 Epoch 33, Loss: 12.6069\n",
            "📉 Epoch 34, Loss: 12.5739\n",
            "📉 Epoch 35, Loss: 12.5714\n",
            "📉 Epoch 36, Loss: 12.5518\n",
            "📉 Epoch 37, Loss: 12.5779\n",
            "📉 Epoch 38, Loss: 12.5354\n",
            "📉 Epoch 39, Loss: 12.5525\n",
            "📉 Epoch 40, Loss: 12.6357\n",
            "📉 Epoch 41, Loss: 12.5228\n",
            "📉 Epoch 42, Loss: 12.5239\n",
            "📉 Epoch 43, Loss: 12.5240\n",
            "📉 Epoch 44, Loss: 12.5378\n",
            "📉 Epoch 45, Loss: 12.9808\n",
            "📉 Epoch 46, Loss: 12.7852\n",
            "📉 Epoch 47, Loss: 12.6330\n",
            "📉 Epoch 48, Loss: 12.5500\n",
            "📉 Epoch 49, Loss: 12.5166\n",
            "  8823 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  10791 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  12102 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  13537 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  14809 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📙 Resultados PostProcessSAGE (con SciBERT en la capa final):\n",
            "Hits@10: 0.0661\n",
            "Hits@20: 0.0808\n",
            "Hits@30: 0.0907\n",
            "Hits@40: 0.1014\n",
            "Hits@50: 0.1109\n",
            "Accuracy: 0.8477\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_sci = model(emb, adj_t)\n",
        "\n",
        "predictor_post_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_sci.size(1),\n",
        "    hidden_channels=z_post_sci.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor.shape[0]\n",
        ").to(device)\n",
        "predictor_post_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_sci_final(\n",
        "            z_post_sci[edge[0]], z_post_sci[edge[1]],\n",
        "            embedding_scibert_tensor.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_post_sci = WrappedPredictor(predictor_post_sci_final, embedding_scibert_tensor)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_sci_final = test(dummy_model, wrapped_predictor_post_sci, z_post_sci, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📙 Resultados PostProcessSAGE (con SciBERT en la capa final):\")\n",
        "for k, v in results_post_sci_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.144394</td>\n",
              "      <td>0.244927</td>\n",
              "      <td>0.289559</td>\n",
              "      <td>0.306767</td>\n",
              "      <td>0.362846</td>\n",
              "      <td>0.9031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.159069</td>\n",
              "      <td>0.249339</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.292009</td>\n",
              "      <td>0.309688</td>\n",
              "      <td>0.8972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.182367</td>\n",
              "      <td>0.201028</td>\n",
              "      <td>0.225502</td>\n",
              "      <td>0.243181</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.029223</td>\n",
              "      <td>0.7761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.013859</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>0.031456</td>\n",
              "      <td>0.7997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.026317</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>0.041756</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.7884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.8339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099873</td>\n",
              "      <td>0.131928</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.154694</td>\n",
              "      <td>0.166321</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.066095</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.101409</td>\n",
              "      <td>0.110938</td>\n",
              "      <td>0.8477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source Prompt Type GPT Injection  \\\n",
              "0          SAGEConv            classic           -             -   \n",
              "1          SAGEConv      classic + GPT     default         input   \n",
              "2          SAGEConv            classic     default         final   \n",
              "3      SkipConnSAGE            classic           -             -   \n",
              "4      SkipConnSAGE      classic + GPT     default         input   \n",
              "5      SkipConnSAGE            classic     default         final   \n",
              "6   PostProcessSAGE            classic           -             -   \n",
              "7   PostProcessSAGE      classic + GPT     default         input   \n",
              "8   PostProcessSAGE            classic     default         final   \n",
              "9          SAGEConv           classic            -             -   \n",
              "10         SAGEConv  classic + BioBERT     default         input   \n",
              "11         SAGEConv            classic     default         final   \n",
              "12     SkipConnSAGE            classic           -             -   \n",
              "13     SkipConnSAGE  classic + BioBERT     default         input   \n",
              "14     SkipConnSAGE            classic     default         final   \n",
              "15  PostProcessSAGE            classic           -             -   \n",
              "16  PostProcessSAGE  classic + BioBERT     default         input   \n",
              "17  PostProcessSAGE            classic     default         final   \n",
              "18         SAGEConv            classic           -             -   \n",
              "19         SAGEConv  classic + SciBERT     default         input   \n",
              "20         SAGEConv            classic     default         final   \n",
              "21     SkipConnSAGE            classic           -             -   \n",
              "22     SkipConnSAGE  classic + SciBERT     default         input   \n",
              "23     SkipConnSAGE            classic     default         final   \n",
              "24  PostProcessSAGE            classic           -             -   \n",
              "25  PostProcessSAGE  classic + SciBERT     default         input   \n",
              "26  PostProcessSAGE            classic     default         final   \n",
              "\n",
              "           Predictor  Epochs   Hits@10   Hits@20   Hits@30   Hits@40  \\\n",
              "0             Neural      50  0.132565  0.171662  0.225854  0.250096   \n",
              "1             Neural      50  0.117703  0.176794  0.197260  0.208849   \n",
              "2       Neural + GPT      50  0.032752  0.044678  0.057555  0.062597   \n",
              "3             Neural     100  0.000225  0.000494  0.000802  0.001041   \n",
              "4             Neural     100  0.000809  0.001049  0.001288  0.001858   \n",
              "5       Neural + GPT     100  0.003274  0.007911  0.012196  0.014923   \n",
              "6             Neural      50  0.005206  0.210482  0.265707  0.293904   \n",
              "7             Neural      50  0.007334  0.007334  0.132925  0.153601   \n",
              "8       Neural + GPT      50  0.038385  0.048438  0.053443  0.061211   \n",
              "9             Neural      50  0.112204  0.156620  0.191394  0.222917   \n",
              "10            Neural      50  0.285020  0.370045  0.423773  0.445228   \n",
              "11  Neural + BioBERT      50  0.054461  0.084194  0.098555  0.116526   \n",
              "12            Neural      50  0.017919  0.031164  0.037546  0.045494   \n",
              "13            Neural      50  0.000277  0.000622  0.001079  0.001671   \n",
              "14  Neural + BioBERT      50  0.015762  0.019829  0.028309  0.033471   \n",
              "15            Neural      50  0.010203  0.010203  0.010203  0.010203   \n",
              "16            Neural      50  0.003813  0.003813  0.003813  0.003813   \n",
              "17  Neural + BioBERT      50  0.071324  0.083932  0.100870  0.115440   \n",
              "18            Neural      50  0.144394  0.244927  0.289559  0.306767   \n",
              "19            Neural      50  0.159069  0.249339  0.278809  0.292009   \n",
              "20  Neural + SciBERT      50  0.078329  0.182367  0.201028  0.225502   \n",
              "21            Neural      50  0.008188  0.016661  0.019335  0.025343   \n",
              "22            Neural      50  0.006270  0.013859  0.019290  0.025395   \n",
              "23  Neural + SciBERT      50  0.026317  0.033389  0.041756  0.049000   \n",
              "24            Neural      50  0.000562  0.000562  0.000562  0.000562   \n",
              "25            Neural      50  0.099873  0.131928  0.144664  0.154694   \n",
              "26  Neural + SciBERT      50  0.066095  0.080838  0.090659  0.101409   \n",
              "\n",
              "     Hits@50  Accuracy  \n",
              "0   0.273229    0.9010  \n",
              "1   0.219119    0.8920  \n",
              "2   0.072718    0.8618  \n",
              "3   0.001326    0.7420  \n",
              "4   0.002277    0.7520  \n",
              "5   0.017664    0.7774  \n",
              "6   0.322446    0.8781  \n",
              "7   0.181296    0.8706  \n",
              "8   0.068358    0.8070  \n",
              "9   0.243204    0.9000  \n",
              "10  0.485149    0.9281  \n",
              "11  0.126265    0.8887  \n",
              "12  0.050761    0.7755  \n",
              "13  0.002682    0.7881  \n",
              "14  0.037501    0.8014  \n",
              "15  0.180517    0.8596  \n",
              "16  0.003813    0.8574  \n",
              "17  0.124819    0.8507  \n",
              "18  0.362846    0.9031  \n",
              "19  0.309688    0.8972  \n",
              "20  0.243181    0.8985  \n",
              "21  0.029223    0.7761  \n",
              "22  0.031456    0.7997  \n",
              "23  0.057698    0.7884  \n",
              "24  0.000562    0.8339  \n",
              "25  0.166321    0.8644  \n",
              "26  0.110938    0.8477  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Nuevas filas con resultados de SciBERT\n",
        "new_rows_scibert = [\n",
        "    # SAGEConv con SciBERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_sci_base},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_sci_input},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_sage_sci_final},\n",
        "\n",
        "    # SkipConnSAGE con SciBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_sci_base},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_sci_input},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_skipconn_sci_final},\n",
        "\n",
        "    # PostProcessSAGE con SciBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_base},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": \"default\", \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_input},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"default\", \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_post_sci_final},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows_scibert)], ignore_index=True)\n",
        "\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "# Mostrar la tabla final actualizada\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GPT Response v2 ===\n",
            "\n",
            "1. In a drug–drug interaction network, certain graph neighborhoods and motifs can indicate a higher likelihood of a link between two drugs. For example:\n",
            "   - **Common neighbors**: If two drugs share many common neighbors, it suggests a potential interaction between them, as drugs that interact often do so with similar sets of other drugs.\n",
            "   - **Structural holes**: Nodes that bridge different communities in the network may indicate a potential interaction between drugs from these separate communities.\n",
            "   - **Network motifs**: Specific patterns of interactions, such as feed-forward loops or bipartite structures, can also provide insights into potential interactions.\n",
            "\n",
            "2. Some failure cases, ambiguities, and limitations in predicting links from a topology-only graph include:\n",
            "   - **Sparsity**: If the network is sparse or lacks certain connections, it may be challenging to infer new interactions accurately.\n",
            "   - **Missing information**: Without additional features or context, it can be difficult to differentiate between different types of interactions or predict interactions with drugs that have not been well-studied.\n",
            "   - **Network noise**: Noisy or irrelevant connections in the network can introduce biases or obscure true interactions.\n",
            "\n",
            "3. To score pairs of nodes and combine topological evidence with latent semantic relationships, you could consider the following approach:\n",
            "   - **Graph-based similarity**: Calculate topological similarity measures such as Jaccard coefficient, Adamic-Adar index, or preferential attachment to quantify the likelihood of a link between two nodes.\n",
            "   - **Latent semantic relationships**: Incorporate latent semantic relationships by leveraging techniques like node embeddings or graph neural networks to capture higher-order structural information and potential hidden relationships between drugs.\n",
            "   - **Combination**: Combine the topological evidence and latent semantic relationships through a weighted sum, concatenation, or other fusion methods to generate a comprehensive score for predicting links between pairs of nodes.\n",
            "\n",
            "By integrating topological features with semantic relationships, you can enhance the link prediction capabilities of the graph neural network and improve the accuracy of inferring drug interactions based on the network structure.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=\"sk-...\")\n",
        "\n",
        "# Prompting enfocado en razonamiento sobre enlaces\n",
        "response_v2 = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant specialized in graph-based machine learning.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"\n",
        "You are an AI system specialized in biomedical knowledge graphs.\n",
        "\n",
        "You are analyzing a drug–drug interaction network in which:\n",
        "- Nodes represent drugs.\n",
        "- Edges represent known interactions between drugs.\n",
        "- The network has no node features; only structure is available.\n",
        "- The average node is connected to around 500 others.\n",
        "\n",
        "Your goal is to infer whether a new interaction (edge) should exist between two given drugs, based only on the network's topology and global interaction patterns.\n",
        "\n",
        "You are not told the identities of the drugs but only the graph structure.\n",
        "\n",
        "Please simulate your reasoning process by answering:\n",
        "\n",
        "1. What kind of graph neighborhoods, motifs, or topological similarities would lead you to believe a link is likely?\n",
        "2. What are some failure cases, ambiguities, or limitations in predicting links from such a topology-only graph?\n",
        "3. If you had to score pairs of nodes, how would you combine topological evidence with hypothetical latent semantic relationships?\n",
        "\n",
        "This output will be transformed into an embedding and used as additional semantic input to enhance the performance of a graph neural network for link prediction.\n",
        "\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"=== GPT Response v2 ===\\n\")\n",
        "print(response_v2.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Respuesta v2 guardada en 'gpt_ddi_reasoning_run2.txt'\n"
          ]
        }
      ],
      "source": [
        "gpt_response_v2 = response_v2.choices[0].message.content\n",
        "\n",
        "with open(\"gpt_ddi_reasoning_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(gpt_response_v2)\n",
        "\n",
        "print(\"✅ Respuesta v2 guardada en 'gpt_ddi_reasoning_run2.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensión del nuevo embedding: (384,)\n",
            "Primeros valores: [ 0.02924465 -0.07621226 -0.05022812  0.00279903  0.06210268 -0.01504837\n",
            " -0.04575742  0.04615958  0.04283014 -0.09850551]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Leer el nuevo archivo\n",
        "with open(\"gpt_ddi_reasoning_run2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt_response_text_v2 = f.read()\n",
        "\n",
        "# Usamos el mismo modelo SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedding_v2 = model.encode(gpt_response_text_v2)\n",
        "\n",
        "print(\"Dimensión del nuevo embedding:\", embedding_v2.shape)\n",
        "print(\"Primeros valores:\", embedding_v2[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding v2 guardado en 'gpt_ddi_embedding_v2_run2.npy'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\"gpt_ddi_embedding_v2_run2.npy\", embedding_v2)\n",
        "print(\"✅ Embedding v2 guardado en 'gpt_ddi_embedding_v2_run2.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_global_v2 = np.load(\"gpt_ddi_embedding_v2_run2.npy\")\n",
        "embedding_global_v2 = torch.tensor(embedding_global_v2, dtype=torch.float32).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "aug_emb_v2 = torch.cat([emb, embedding_global_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6336\n",
            "Epoch 1 has loss 10.1175\n",
            "Epoch 2 has loss 9.3894\n",
            "Epoch 3 has loss 8.7749\n",
            "Epoch 4 has loss 7.9447\n",
            "Epoch 5 has loss 7.6697\n",
            "Epoch 6 has loss 7.5028\n",
            "Epoch 7 has loss 7.4545\n",
            "Epoch 8 has loss 7.2946\n",
            "Epoch 9 has loss 7.143\n",
            "Epoch 10 has loss 6.9733\n",
            "Epoch 11 has loss 6.7042\n",
            "Epoch 12 has loss 6.584\n",
            "Epoch 13 has loss 6.5814\n",
            "Epoch 14 has loss 6.4333\n",
            "Epoch 15 has loss 6.1419\n",
            "Epoch 16 has loss 6.0217\n",
            "Epoch 17 has loss 5.9121\n",
            "Epoch 18 has loss 5.5398\n",
            "Epoch 19 has loss 5.3858\n",
            "Epoch 20 has loss 5.3622\n",
            "Epoch 21 has loss 5.2169\n",
            "Epoch 22 has loss 5.1416\n",
            "Epoch 23 has loss 5.1575\n",
            "Epoch 24 has loss 5.0103\n",
            "Epoch 25 has loss 5.0047\n",
            "Epoch 26 has loss 5.0013\n",
            "Epoch 27 has loss 4.9645\n",
            "Epoch 28 has loss 4.9223\n",
            "Epoch 29 has loss 4.9786\n",
            "Epoch 30 has loss 4.9529\n",
            "Epoch 31 has loss 4.8061\n",
            "Epoch 32 has loss 4.8476\n",
            "Epoch 33 has loss 4.7178\n",
            "Epoch 34 has loss 4.612\n",
            "Epoch 35 has loss 4.5719\n",
            "Epoch 36 has loss 4.5146\n",
            "Epoch 37 has loss 4.4711\n",
            "Epoch 38 has loss 4.4948\n",
            "Epoch 39 has loss 4.4608\n",
            "Epoch 40 has loss 4.3712\n",
            "Epoch 41 has loss 4.3747\n",
            "Epoch 42 has loss 4.337\n",
            "Epoch 43 has loss 4.3282\n",
            "Epoch 44 has loss 4.2249\n",
            "Epoch 45 has loss 4.1842\n",
            "Epoch 46 has loss 4.1406\n",
            "Epoch 47 has loss 4.1304\n",
            "Epoch 48 has loss 4.1733\n",
            "Epoch 49 has loss 4.3075\n",
            "  23749 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  33666 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  42447 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  46344 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  48660 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SAGEConv (base, v2):\n",
            "Hits@10: 0.1779\n",
            "Hits@20: 0.2522\n",
            "Hits@30: 0.3180\n",
            "Hits@40: 0.3472\n",
            "Hits@50: 0.3645\n",
            "Accuracy: 0.9012\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SAGEConv (base, v2):\")\n",
        "for k, v in results_sage_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7213\n",
            "Epoch 1 has loss 9.7553\n",
            "Epoch 2 has loss 8.8173\n",
            "Epoch 3 has loss 7.9969\n",
            "Epoch 4 has loss 7.7476\n",
            "Epoch 5 has loss 7.5818\n",
            "Epoch 6 has loss 7.4166\n",
            "Epoch 7 has loss 7.3278\n",
            "Epoch 8 has loss 7.3408\n",
            "Epoch 9 has loss 6.831\n",
            "Epoch 10 has loss 6.4625\n",
            "Epoch 11 has loss 6.2449\n",
            "Epoch 12 has loss 6.0891\n",
            "Epoch 13 has loss 6.0409\n",
            "Epoch 14 has loss 5.7773\n",
            "Epoch 15 has loss 5.7082\n",
            "Epoch 16 has loss 5.616\n",
            "Epoch 17 has loss 5.5298\n",
            "Epoch 18 has loss 5.4495\n",
            "Epoch 19 has loss 5.1922\n",
            "Epoch 20 has loss 5.3252\n",
            "Epoch 21 has loss 5.3129\n",
            "Epoch 22 has loss 5.0506\n",
            "Epoch 23 has loss 5.0408\n",
            "Epoch 24 has loss 4.8776\n",
            "Epoch 25 has loss 4.9305\n",
            "Epoch 26 has loss 4.8255\n",
            "Epoch 27 has loss 4.8616\n",
            "Epoch 28 has loss 4.8658\n",
            "Epoch 29 has loss 4.7207\n",
            "Epoch 30 has loss 4.5884\n",
            "Epoch 31 has loss 4.6147\n",
            "Epoch 32 has loss 4.6806\n",
            "Epoch 33 has loss 4.5816\n",
            "Epoch 34 has loss 4.6563\n",
            "Epoch 35 has loss 4.4691\n",
            "Epoch 36 has loss 4.4783\n",
            "Epoch 37 has loss 4.7238\n",
            "Epoch 38 has loss 4.6363\n",
            "Epoch 39 has loss 4.4722\n",
            "Epoch 40 has loss 4.486\n",
            "Epoch 41 has loss 4.3064\n",
            "Epoch 42 has loss 4.3141\n",
            "Epoch 43 has loss 4.3366\n",
            "Epoch 44 has loss 4.2555\n",
            "Epoch 45 has loss 4.2533\n",
            "Epoch 46 has loss 4.3322\n",
            "Epoch 47 has loss 4.2343\n",
            "Epoch 48 has loss 4.1733\n",
            "Epoch 49 has loss 4.1645\n",
            "  16259 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  21409 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26661 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  30481 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  33717 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔗 Resultados SAGEConv (con GPT v2 en input):\n",
            "Hits@10: 0.1218\n",
            "Hits@20: 0.1604\n",
            "Hits@30: 0.1997\n",
            "Hits@40: 0.2283\n",
            "Hits@50: 0.2526\n",
            "Accuracy: 0.8987\n"
          ]
        }
      ],
      "source": [
        "aug_emb_input_v2 = torch.cat([emb, embedding_global_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb_input_v2.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
        "\n",
        "train(model, predictor, aug_emb_input_v2, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_input_v2 = test(model, predictor, aug_emb_input_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔗 Resultados SAGEConv (con GPT v2 en input):\")\n",
        "for k, v in results_sage_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.866\n",
            "Epoch 1 has loss 10.3597\n",
            "Epoch 2 has loss 10.2031\n",
            "Epoch 3 has loss 10.1873\n",
            "Epoch 4 has loss 10.1106\n",
            "Epoch 5 has loss 10.0101\n",
            "Epoch 6 has loss 9.8927\n",
            "Epoch 7 has loss 9.8168\n",
            "Epoch 8 has loss 9.6948\n",
            "Epoch 9 has loss 9.7546\n",
            "Epoch 10 has loss 9.6262\n",
            "Epoch 11 has loss 9.5414\n",
            "Epoch 12 has loss 9.5098\n",
            "Epoch 13 has loss 9.4852\n",
            "Epoch 14 has loss 9.4888\n",
            "Epoch 15 has loss 9.4902\n",
            "Epoch 16 has loss 9.4898\n",
            "Epoch 17 has loss 9.459\n",
            "Epoch 18 has loss 9.4434\n",
            "Epoch 19 has loss 9.4772\n",
            "Epoch 20 has loss 9.4776\n",
            "Epoch 21 has loss 9.443\n",
            "Epoch 22 has loss 9.4271\n",
            "Epoch 23 has loss 9.4537\n",
            "Epoch 24 has loss 9.4307\n",
            "Epoch 25 has loss 9.4176\n",
            "Epoch 26 has loss 9.4214\n",
            "Epoch 27 has loss 9.4274\n",
            "Epoch 28 has loss 9.4105\n",
            "Epoch 29 has loss 9.398\n",
            "Epoch 30 has loss 9.4108\n",
            "Epoch 31 has loss 9.4188\n",
            "Epoch 32 has loss 9.4088\n",
            "Epoch 33 has loss 9.4283\n",
            "Epoch 34 has loss 9.424\n",
            "Epoch 35 has loss 9.4079\n",
            "Epoch 36 has loss 9.4075\n",
            "Epoch 37 has loss 9.4139\n",
            "Epoch 38 has loss 9.418\n",
            "Epoch 39 has loss 9.4096\n",
            "Epoch 40 has loss 9.3999\n",
            "Epoch 41 has loss 9.3916\n",
            "Epoch 42 has loss 9.4005\n",
            "Epoch 43 has loss 9.3848\n",
            "Epoch 44 has loss 9.3806\n",
            "Epoch 45 has loss 9.3874\n",
            "Epoch 46 has loss 9.3811\n",
            "Epoch 47 has loss 9.3752\n",
            "Epoch 48 has loss 9.3885\n",
            "Epoch 49 has loss 9.3755\n",
            "📉 Epoch 0, Loss: 16.4531\n",
            "📉 Epoch 1, Loss: 13.8365\n",
            "📉 Epoch 2, Loss: 12.8918\n",
            "📉 Epoch 3, Loss: 12.2115\n",
            "📉 Epoch 4, Loss: 11.7570\n",
            "📉 Epoch 5, Loss: 11.5697\n",
            "📉 Epoch 6, Loss: 11.3495\n",
            "📉 Epoch 7, Loss: 11.2288\n",
            "📉 Epoch 8, Loss: 11.1826\n",
            "📉 Epoch 9, Loss: 11.0710\n",
            "📉 Epoch 10, Loss: 11.0701\n",
            "📉 Epoch 11, Loss: 11.1484\n",
            "📉 Epoch 12, Loss: 10.9851\n",
            "📉 Epoch 13, Loss: 10.9122\n",
            "📉 Epoch 14, Loss: 10.9854\n",
            "📉 Epoch 15, Loss: 10.8530\n",
            "📉 Epoch 16, Loss: 10.7895\n",
            "📉 Epoch 17, Loss: 10.9406\n",
            "📉 Epoch 18, Loss: 10.8236\n",
            "📉 Epoch 19, Loss: 10.7866\n",
            "📉 Epoch 20, Loss: 10.7843\n",
            "📉 Epoch 21, Loss: 10.7197\n",
            "📉 Epoch 22, Loss: 10.7816\n",
            "📉 Epoch 23, Loss: 10.7499\n",
            "📉 Epoch 24, Loss: 10.6625\n",
            "📉 Epoch 25, Loss: 10.6824\n",
            "📉 Epoch 26, Loss: 10.6801\n",
            "📉 Epoch 27, Loss: 10.6211\n",
            "📉 Epoch 28, Loss: 10.6463\n",
            "📉 Epoch 29, Loss: 10.6443\n",
            "📉 Epoch 30, Loss: 10.6198\n",
            "📉 Epoch 31, Loss: 10.5816\n",
            "📉 Epoch 32, Loss: 10.5788\n",
            "📉 Epoch 33, Loss: 10.6086\n",
            "📉 Epoch 34, Loss: 10.5235\n",
            "📉 Epoch 35, Loss: 10.5633\n",
            "📉 Epoch 36, Loss: 10.5156\n",
            "📉 Epoch 37, Loss: 10.5509\n",
            "📉 Epoch 38, Loss: 10.5603\n",
            "📉 Epoch 39, Loss: 10.5266\n",
            "📉 Epoch 40, Loss: 10.4829\n",
            "📉 Epoch 41, Loss: 10.5701\n",
            "📉 Epoch 42, Loss: 10.5007\n",
            "📉 Epoch 43, Loss: 10.4934\n",
            "📉 Epoch 44, Loss: 10.5179\n",
            "📉 Epoch 45, Loss: 10.4587\n",
            "📉 Epoch 46, Loss: 10.4548\n",
            "📉 Epoch 47, Loss: 10.4525\n",
            "📉 Epoch 48, Loss: 10.5053\n",
            "📉 Epoch 49, Loss: 10.4431\n",
            "  23815 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  27096 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  31388 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  33713 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  35719 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv (con GPT v2 en la capa final):\n",
            "Hits@10: 0.1784\n",
            "Hits@20: 0.2030\n",
            "Hits@30: 0.2351\n",
            "Hits@40: 0.2526\n",
            "Hits@50: 0.2676\n",
            "Accuracy: 0.8981\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor_dummy = DotProductLinkPredictor().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, predictor_dummy, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_v2 = model(emb, adj_t)\n",
        "\n",
        "predictor_final_v2 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_v2.size(1),\n",
        "    hidden_channels=z_v2.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_final_v2.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_final_v2.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_final_v2.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_final_v2(\n",
        "            z_v2[edge[0]], z_v2[edge[1]],\n",
        "            embedding_global_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_final_v2 = WrappedPredictor(predictor_final_v2, embedding_global_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_final_v2 = test(dummy_model, wrapped_predictor_final_v2, z_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv (con GPT v2 en la capa final):\")\n",
        "for k, v in results_sage_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7471\n",
            "Epoch 1 has loss 10.3789\n",
            "Epoch 2 has loss 10.217\n",
            "Epoch 3 has loss 10.0863\n",
            "Epoch 4 has loss 10.0258\n",
            "Epoch 5 has loss 10.1268\n",
            "Epoch 6 has loss 9.7704\n",
            "Epoch 7 has loss 9.4464\n",
            "Epoch 8 has loss 9.7165\n",
            "Epoch 9 has loss 9.5457\n",
            "Epoch 10 has loss 9.443\n",
            "Epoch 11 has loss 9.4625\n",
            "Epoch 12 has loss 9.3834\n",
            "Epoch 13 has loss 9.698\n",
            "Epoch 14 has loss 9.4472\n",
            "Epoch 15 has loss 9.4708\n",
            "Epoch 16 has loss 9.638\n",
            "Epoch 17 has loss 9.5099\n",
            "Epoch 18 has loss 9.7604\n",
            "Epoch 19 has loss 9.4529\n",
            "Epoch 20 has loss 9.2179\n",
            "Epoch 21 has loss 9.5094\n",
            "Epoch 22 has loss 9.4981\n",
            "Epoch 23 has loss 9.2492\n",
            "Epoch 24 has loss 9.1454\n",
            "Epoch 25 has loss 8.9891\n",
            "Epoch 26 has loss 9.2478\n",
            "Epoch 27 has loss 9.5669\n",
            "Epoch 28 has loss 8.8337\n",
            "Epoch 29 has loss 8.773\n",
            "Epoch 30 has loss 8.8425\n",
            "Epoch 31 has loss 8.8886\n",
            "Epoch 32 has loss 9.0055\n",
            "Epoch 33 has loss 8.9295\n",
            "Epoch 34 has loss 8.9394\n",
            "Epoch 35 has loss 8.932\n",
            "Epoch 36 has loss 8.8463\n",
            "Epoch 37 has loss 8.9888\n",
            "Epoch 38 has loss 8.6921\n",
            "Epoch 39 has loss 8.6052\n",
            "Epoch 40 has loss 8.8026\n",
            "Epoch 41 has loss 8.6034\n",
            "Epoch 42 has loss 8.7289\n",
            "Epoch 43 has loss 8.871\n",
            "Epoch 44 has loss 8.5321\n",
            "Epoch 45 has loss 8.6349\n",
            "Epoch 46 has loss 8.5012\n",
            "Epoch 47 has loss 8.9637\n",
            "Epoch 48 has loss 8.4317\n",
            "Epoch 49 has loss 8.7619\n",
            "  2 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  440 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  1938 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  2226 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  2700 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE (sin embedding v2):\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0033\n",
            "Hits@30: 0.0145\n",
            "Hits@40: 0.0167\n",
            "Hits@50: 0.0202\n",
            "Accuracy: 0.7771\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE (sin embedding v2):\")\n",
        "for k, v in results_skip_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.1868\n",
            "Epoch 1 has loss 10.6427\n",
            "Epoch 2 has loss 10.1428\n",
            "Epoch 3 has loss 10.1154\n",
            "Epoch 4 has loss 10.2889\n",
            "Epoch 5 has loss 10.0238\n",
            "Epoch 6 has loss 9.9572\n",
            "Epoch 7 has loss 9.964\n",
            "Epoch 8 has loss 10.1003\n",
            "Epoch 9 has loss 10.4297\n",
            "Epoch 10 has loss 10.2296\n",
            "Epoch 11 has loss 9.843\n",
            "Epoch 12 has loss 9.9156\n",
            "Epoch 13 has loss 9.8306\n",
            "Epoch 14 has loss 9.4848\n",
            "Epoch 15 has loss 9.8151\n",
            "Epoch 16 has loss 9.7194\n",
            "Epoch 17 has loss 9.5064\n",
            "Epoch 18 has loss 9.4477\n",
            "Epoch 19 has loss 9.3628\n",
            "Epoch 20 has loss 9.4527\n",
            "Epoch 21 has loss 9.064\n",
            "Epoch 22 has loss 9.2273\n",
            "Epoch 23 has loss 8.9772\n",
            "Epoch 24 has loss 8.8025\n",
            "Epoch 25 has loss 9.0225\n",
            "Epoch 26 has loss 8.9456\n",
            "Epoch 27 has loss 8.645\n",
            "Epoch 28 has loss 8.4809\n",
            "Epoch 29 has loss 8.329\n",
            "Epoch 30 has loss 8.7405\n",
            "Epoch 31 has loss 8.2756\n",
            "Epoch 32 has loss 8.4249\n",
            "Epoch 33 has loss 8.2638\n",
            "Epoch 34 has loss 8.2175\n",
            "Epoch 35 has loss 8.1315\n",
            "Epoch 36 has loss 7.9278\n",
            "Epoch 37 has loss 8.1996\n",
            "Epoch 38 has loss 8.6249\n",
            "Epoch 39 has loss 8.1006\n",
            "Epoch 40 has loss 7.8541\n",
            "Epoch 41 has loss 7.7633\n",
            "Epoch 42 has loss 7.9879\n",
            "Epoch 43 has loss 7.9842\n",
            "Epoch 44 has loss 7.7311\n",
            "Epoch 45 has loss 7.7679\n",
            "Epoch 46 has loss 7.7263\n",
            "Epoch 47 has loss 7.534\n",
            "Epoch 48 has loss 7.3159\n",
            "Epoch 49 has loss 7.0661\n",
            "  5014 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  10373 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  12104 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  12668 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  13324 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📙 Resultados SkipConnSAGE (embedding v2 como input):\n",
            "Hits@10: 0.0376\n",
            "Hits@20: 0.0777\n",
            "Hits@30: 0.0907\n",
            "Hits@40: 0.0949\n",
            "Hits@50: 0.0998\n",
            "Accuracy: 0.7046\n"
          ]
        }
      ],
      "source": [
        "aug_emb_v2 = torch.cat([\n",
        "    emb,\n",
        "    embedding_global_v2.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb_v2.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_v2, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_input_v2 = test(model, predictor, aug_emb_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📙 Resultados SkipConnSAGE (embedding v2 como input):\")\n",
        "for k, v in results_skip_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.026\n",
            "Epoch 1 has loss 10.6569\n",
            "Epoch 2 has loss 10.4236\n",
            "Epoch 3 has loss 10.3586\n",
            "Epoch 4 has loss 10.2819\n",
            "Epoch 5 has loss 10.2171\n",
            "Epoch 6 has loss 10.2136\n",
            "Epoch 7 has loss 10.2339\n",
            "Epoch 8 has loss 10.2276\n",
            "Epoch 9 has loss 10.1989\n",
            "Epoch 10 has loss 10.1884\n",
            "Epoch 11 has loss 10.1785\n",
            "Epoch 12 has loss 10.1285\n",
            "Epoch 13 has loss 10.1371\n",
            "Epoch 14 has loss 10.1612\n",
            "Epoch 15 has loss 10.129\n",
            "Epoch 16 has loss 10.0334\n",
            "Epoch 17 has loss 10.1101\n",
            "Epoch 18 has loss 10.143\n",
            "Epoch 19 has loss 10.0771\n",
            "Epoch 20 has loss 10.0228\n",
            "Epoch 21 has loss 10.0461\n",
            "Epoch 22 has loss 10.0375\n",
            "Epoch 23 has loss 10.0144\n",
            "Epoch 24 has loss 10.0029\n",
            "Epoch 25 has loss 9.9939\n",
            "Epoch 26 has loss 9.9593\n",
            "Epoch 27 has loss 9.9769\n",
            "Epoch 28 has loss 9.9993\n",
            "Epoch 29 has loss 9.942\n",
            "Epoch 30 has loss 9.9094\n",
            "Epoch 31 has loss 9.9257\n",
            "Epoch 32 has loss 9.9673\n",
            "Epoch 33 has loss 9.8911\n",
            "Epoch 34 has loss 9.856\n",
            "Epoch 35 has loss 9.8617\n",
            "Epoch 36 has loss 9.8326\n",
            "Epoch 37 has loss 9.7592\n",
            "Epoch 38 has loss 9.7847\n",
            "Epoch 39 has loss 9.7225\n",
            "Epoch 40 has loss 9.6716\n",
            "Epoch 41 has loss 9.8468\n",
            "Epoch 42 has loss 9.7238\n",
            "Epoch 43 has loss 9.6611\n",
            "Epoch 44 has loss 9.6159\n",
            "Epoch 45 has loss 9.622\n",
            "Epoch 46 has loss 9.5875\n",
            "Epoch 47 has loss 9.5519\n",
            "Epoch 48 has loss 9.5367\n",
            "Epoch 49 has loss 9.5231\n",
            "📉 Epoch 0, Loss: 18.6380\n",
            "📉 Epoch 1, Loss: 16.5639\n",
            "📉 Epoch 2, Loss: 15.9322\n",
            "📉 Epoch 3, Loss: 16.8186\n",
            "📉 Epoch 4, Loss: 16.2214\n",
            "📉 Epoch 5, Loss: 15.4464\n",
            "📉 Epoch 6, Loss: 14.9455\n",
            "📉 Epoch 7, Loss: 15.8007\n",
            "📉 Epoch 8, Loss: 14.9107\n",
            "📉 Epoch 9, Loss: 15.2984\n",
            "📉 Epoch 10, Loss: 14.7170\n",
            "📉 Epoch 11, Loss: 14.7752\n",
            "📉 Epoch 12, Loss: 14.7411\n",
            "📉 Epoch 13, Loss: 14.4600\n",
            "📉 Epoch 14, Loss: 15.5786\n",
            "📉 Epoch 15, Loss: 14.6082\n",
            "📉 Epoch 16, Loss: 14.4829\n",
            "📉 Epoch 17, Loss: 14.3473\n",
            "📉 Epoch 18, Loss: 14.7175\n",
            "📉 Epoch 19, Loss: 16.8335\n",
            "📉 Epoch 20, Loss: 14.6388\n",
            "📉 Epoch 21, Loss: 14.3851\n",
            "📉 Epoch 22, Loss: 14.4635\n",
            "📉 Epoch 23, Loss: 14.5422\n",
            "📉 Epoch 24, Loss: 14.4687\n",
            "📉 Epoch 25, Loss: 14.2736\n",
            "📉 Epoch 26, Loss: 14.8011\n",
            "📉 Epoch 27, Loss: 14.2875\n",
            "📉 Epoch 28, Loss: 15.5771\n",
            "📉 Epoch 29, Loss: 14.8954\n",
            "📉 Epoch 30, Loss: 14.5094\n",
            "📉 Epoch 31, Loss: 14.5842\n",
            "📉 Epoch 32, Loss: 14.2302\n",
            "📉 Epoch 33, Loss: 14.4627\n",
            "📉 Epoch 34, Loss: 14.3492\n",
            "📉 Epoch 35, Loss: 14.2677\n",
            "📉 Epoch 36, Loss: 14.3530\n",
            "📉 Epoch 37, Loss: 14.4638\n",
            "📉 Epoch 38, Loss: 14.2709\n",
            "📉 Epoch 39, Loss: 14.1961\n",
            "📉 Epoch 40, Loss: 14.0591\n",
            "📉 Epoch 41, Loss: 14.0822\n",
            "📉 Epoch 42, Loss: 14.1699\n",
            "📉 Epoch 43, Loss: 14.3806\n",
            "📉 Epoch 44, Loss: 14.0900\n",
            "📉 Epoch 45, Loss: 14.3101\n",
            "📉 Epoch 46, Loss: 14.0923\n",
            "📉 Epoch 47, Loss: 14.0482\n",
            "📉 Epoch 48, Loss: 14.1017\n",
            "📉 Epoch 49, Loss: 14.4087\n",
            "  22 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  70 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  122 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  216 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  349 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📗 Resultados SkipConnSAGE (embedding v2 como capa final):\n",
            "Hits@10: 0.0002\n",
            "Hits@20: 0.0005\n",
            "Hits@30: 0.0009\n",
            "Hits@40: 0.0016\n",
            "Hits@50: 0.0026\n",
            "Accuracy: 0.8083\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model,DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_v2 = model(emb, adj_t)\n",
        "\n",
        "predictor = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_v2.size(1),\n",
        "    hidden_channels=z_skip_v2.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v2.shape[0]\n",
        ").to(device)\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "predictor.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor(\n",
        "            z_skip_v2[edge[0]], z_skip_v2[edge[1]],\n",
        "            embedding_global_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor, embedding_global_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skip_final_v2 = test(dummy_model, wrapped_predictor, z_skip_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📗 Resultados SkipConnSAGE (embedding v2 como capa final):\")\n",
        "for k, v in results_skip_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8196\n",
            "Epoch 1 has loss 11.7278\n",
            "Epoch 2 has loss 11.7133\n",
            "Epoch 3 has loss 10.7738\n",
            "Epoch 4 has loss 10.3815\n",
            "Epoch 5 has loss 8.9698\n",
            "Epoch 6 has loss 8.5448\n",
            "Epoch 7 has loss 7.8887\n",
            "Epoch 8 has loss 7.6167\n",
            "Epoch 9 has loss 7.4938\n",
            "Epoch 10 has loss 7.4952\n",
            "Epoch 11 has loss 7.4258\n",
            "Epoch 12 has loss 7.3971\n",
            "Epoch 13 has loss 7.3543\n",
            "Epoch 14 has loss 7.3679\n",
            "Epoch 15 has loss 7.3188\n",
            "Epoch 16 has loss 7.2889\n",
            "Epoch 17 has loss 7.2445\n",
            "Epoch 18 has loss 7.2897\n",
            "Epoch 19 has loss 7.3038\n",
            "Epoch 20 has loss 7.237\n",
            "Epoch 21 has loss 7.187\n",
            "Epoch 22 has loss 7.1254\n",
            "Epoch 23 has loss 6.9556\n",
            "Epoch 24 has loss 6.7286\n",
            "Epoch 25 has loss 6.5605\n",
            "Epoch 26 has loss 6.6083\n",
            "Epoch 27 has loss 6.3836\n",
            "Epoch 28 has loss 6.2\n",
            "Epoch 29 has loss 5.9315\n",
            "Epoch 30 has loss 5.9108\n",
            "Epoch 31 has loss 5.7199\n",
            "Epoch 32 has loss 5.9641\n",
            "Epoch 33 has loss 5.6457\n",
            "Epoch 34 has loss 5.6189\n",
            "Epoch 35 has loss 5.4964\n",
            "Epoch 36 has loss 5.3952\n",
            "Epoch 37 has loss 5.5014\n",
            "Epoch 38 has loss 5.3967\n",
            "Epoch 39 has loss 5.2969\n",
            "Epoch 40 has loss 5.1846\n",
            "Epoch 41 has loss 5.4652\n",
            "Epoch 42 has loss 5.1999\n",
            "Epoch 43 has loss 5.0417\n",
            "Epoch 44 has loss 5.0821\n",
            "Epoch 45 has loss 5.0276\n",
            "Epoch 46 has loss 4.8996\n",
            "Epoch 47 has loss 4.8679\n",
            "Epoch 48 has loss 4.7899\n",
            "Epoch 49 has loss 4.8373\n",
            "  13269 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  20570 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  24253 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  26532 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  27976 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE (Prompt V2, sin BERT):\n",
            "Hits@10: 0.0994\n",
            "Hits@20: 0.1541\n",
            "Hits@30: 0.1817\n",
            "Hits@40: 0.1988\n",
            "Hits@50: 0.2096\n",
            "Accuracy: 0.8709\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE (Prompt V2, sin BERT):\")\n",
        "for k, v in results_post_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8002\n",
            "Epoch 1 has loss 13.3926\n",
            "Epoch 2 has loss 10.9869\n",
            "Epoch 3 has loss 11.7762\n",
            "Epoch 4 has loss 9.7303\n",
            "Epoch 5 has loss 8.8128\n",
            "Epoch 6 has loss 8.3216\n",
            "Epoch 7 has loss 7.7494\n",
            "Epoch 8 has loss 7.5954\n",
            "Epoch 9 has loss 7.5253\n",
            "Epoch 10 has loss 7.4813\n",
            "Epoch 11 has loss 7.4765\n",
            "Epoch 12 has loss 7.4565\n",
            "Epoch 13 has loss 7.4334\n",
            "Epoch 14 has loss 7.4181\n",
            "Epoch 15 has loss 7.375\n",
            "Epoch 16 has loss 7.4111\n",
            "Epoch 17 has loss 7.351\n",
            "Epoch 18 has loss 7.424\n",
            "Epoch 19 has loss 7.4106\n",
            "Epoch 20 has loss 7.373\n",
            "Epoch 21 has loss 7.3709\n",
            "Epoch 22 has loss 7.3817\n",
            "Epoch 23 has loss 7.3778\n",
            "Epoch 24 has loss 7.5136\n",
            "Epoch 25 has loss 8.3397\n",
            "Epoch 26 has loss 9.4346\n",
            "Epoch 27 has loss 8.1903\n",
            "Epoch 28 has loss 7.7632\n",
            "Epoch 29 has loss 7.3803\n",
            "Epoch 30 has loss 7.3259\n",
            "Epoch 31 has loss 7.3389\n",
            "Epoch 32 has loss 7.3713\n",
            "Epoch 33 has loss 7.2789\n",
            "Epoch 34 has loss 7.4492\n",
            "Epoch 35 has loss 7.338\n",
            "Epoch 36 has loss 7.2411\n",
            "Epoch 37 has loss 7.2148\n",
            "Epoch 38 has loss 7.0059\n",
            "Epoch 39 has loss 6.5193\n",
            "Epoch 40 has loss 6.3274\n",
            "Epoch 41 has loss 6.366\n",
            "Epoch 42 has loss 6.0115\n",
            "Epoch 43 has loss 5.9773\n",
            "Epoch 44 has loss 5.9555\n",
            "Epoch 45 has loss 5.7712\n",
            "Epoch 46 has loss 5.7988\n",
            "Epoch 47 has loss 5.68\n",
            "Epoch 48 has loss 5.4664\n",
            "Epoch 49 has loss 5.4167\n",
            "  635 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  19060 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  21830 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  23937 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  25943 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧩 Resultados PostProcessSAGE (Prompt V2, con BERT como input):\n",
            "Hits@10: 0.0048\n",
            "Hits@20: 0.1428\n",
            "Hits@30: 0.1635\n",
            "Hits@40: 0.1793\n",
            "Hits@50: 0.1943\n",
            "Accuracy: 0.8496\n"
          ]
        }
      ],
      "source": [
        "aug_emb_v2 = torch.cat([emb, embedding_global_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb_v2.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_v2, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bert_input_v2 = test(model, predictor, aug_emb_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧩 Resultados PostProcessSAGE (Prompt V2, con BERT como input):\")\n",
        "for k, v in results_post_bert_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8569\n",
            "Epoch 1 has loss 11.7657\n",
            "Epoch 2 has loss 11.715\n",
            "Epoch 3 has loss 11.6204\n",
            "Epoch 4 has loss 11.0264\n",
            "Epoch 5 has loss 10.959\n",
            "Epoch 6 has loss 10.1599\n",
            "Epoch 7 has loss 9.7465\n",
            "Epoch 8 has loss 9.4715\n",
            "Epoch 9 has loss 9.3588\n",
            "Epoch 10 has loss 9.2665\n",
            "Epoch 11 has loss 9.2716\n",
            "Epoch 12 has loss 9.2461\n",
            "Epoch 13 has loss 9.1843\n",
            "Epoch 14 has loss 9.1537\n",
            "Epoch 15 has loss 9.1329\n",
            "Epoch 16 has loss 9.1097\n",
            "Epoch 17 has loss 9.1997\n",
            "Epoch 18 has loss 9.0744\n",
            "Epoch 19 has loss 8.9933\n",
            "Epoch 20 has loss 8.7845\n",
            "Epoch 21 has loss 8.9791\n",
            "Epoch 22 has loss 8.7174\n",
            "Epoch 23 has loss 8.5419\n",
            "Epoch 24 has loss 8.4908\n",
            "Epoch 25 has loss 8.3668\n",
            "Epoch 26 has loss 8.644\n",
            "Epoch 27 has loss 8.3433\n",
            "Epoch 28 has loss 8.3089\n",
            "Epoch 29 has loss 8.7658\n",
            "Epoch 30 has loss 8.8199\n",
            "Epoch 31 has loss 9.0078\n",
            "Epoch 32 has loss 8.915\n",
            "Epoch 33 has loss 8.5449\n",
            "Epoch 34 has loss 8.6202\n",
            "Epoch 35 has loss 8.627\n",
            "Epoch 36 has loss 8.8176\n",
            "Epoch 37 has loss 8.6883\n",
            "Epoch 38 has loss 8.558\n",
            "Epoch 39 has loss 8.5348\n",
            "Epoch 40 has loss 8.4478\n",
            "Epoch 41 has loss 8.5064\n",
            "Epoch 42 has loss 8.7726\n",
            "Epoch 43 has loss 8.5346\n",
            "Epoch 44 has loss 8.4857\n",
            "Epoch 45 has loss 8.4103\n",
            "Epoch 46 has loss 8.4015\n",
            "Epoch 47 has loss 8.3878\n",
            "Epoch 48 has loss 8.4377\n",
            "Epoch 49 has loss 8.5\n",
            "📉 Epoch 0, Loss: 19.4412\n",
            "📉 Epoch 1, Loss: 13.7741\n",
            "📉 Epoch 2, Loss: 12.9622\n",
            "📉 Epoch 3, Loss: 12.7641\n",
            "📉 Epoch 4, Loss: 12.6982\n",
            "📉 Epoch 5, Loss: 12.6736\n",
            "📉 Epoch 6, Loss: 12.6422\n",
            "📉 Epoch 7, Loss: 12.6660\n",
            "📉 Epoch 8, Loss: 12.6225\n",
            "📉 Epoch 9, Loss: 12.6109\n",
            "📉 Epoch 10, Loss: 12.5952\n",
            "📉 Epoch 11, Loss: 12.5822\n",
            "📉 Epoch 12, Loss: 12.5685\n",
            "📉 Epoch 13, Loss: 12.5713\n",
            "📉 Epoch 14, Loss: 12.5514\n",
            "📉 Epoch 15, Loss: 12.5413\n",
            "📉 Epoch 16, Loss: 12.5318\n",
            "📉 Epoch 17, Loss: 12.5299\n",
            "📉 Epoch 18, Loss: 12.5127\n",
            "📉 Epoch 19, Loss: 12.5052\n",
            "📉 Epoch 20, Loss: 12.5012\n",
            "📉 Epoch 21, Loss: 12.4934\n",
            "📉 Epoch 22, Loss: 12.4898\n",
            "📉 Epoch 23, Loss: 12.4839\n",
            "📉 Epoch 24, Loss: 12.4801\n",
            "📉 Epoch 25, Loss: 12.4815\n",
            "📉 Epoch 26, Loss: 12.4916\n",
            "📉 Epoch 27, Loss: 12.4878\n",
            "📉 Epoch 28, Loss: 12.4729\n",
            "📉 Epoch 29, Loss: 12.4712\n",
            "📉 Epoch 30, Loss: 12.4576\n",
            "📉 Epoch 31, Loss: 12.4500\n",
            "📉 Epoch 32, Loss: 12.4663\n",
            "📉 Epoch 33, Loss: 12.4487\n",
            "📉 Epoch 34, Loss: 12.4761\n",
            "📉 Epoch 35, Loss: 12.4574\n",
            "📉 Epoch 36, Loss: 12.4473\n",
            "📉 Epoch 37, Loss: 12.4475\n",
            "📉 Epoch 38, Loss: 12.4500\n",
            "📉 Epoch 39, Loss: 12.4547\n",
            "📉 Epoch 40, Loss: 12.4503\n",
            "📉 Epoch 41, Loss: 12.4473\n",
            "📉 Epoch 42, Loss: 12.4528\n",
            "📉 Epoch 43, Loss: 12.4462\n",
            "📉 Epoch 44, Loss: 12.4353\n",
            "📉 Epoch 45, Loss: 12.4283\n",
            "📉 Epoch 46, Loss: 12.4216\n",
            "📉 Epoch 47, Loss: 12.4249\n",
            "📉 Epoch 48, Loss: 12.4268\n",
            "📉 Epoch 49, Loss: 12.4227\n",
            "  5289 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  7901 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  13677 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  14966 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  15920 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados PostProcessSAGE (Prompt V2, con BERT como señal final):\n",
            "Hits@10: 0.0396\n",
            "Hits@20: 0.0592\n",
            "Hits@30: 0.1025\n",
            "Hits@40: 0.1121\n",
            "Hits@50: 0.1193\n",
            "Accuracy: 0.8508\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_bert_final = model(emb, adj_t)\n",
        "\n",
        "predictor_post_bert_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_bert_final.size(1),\n",
        "    hidden_channels=z_post_bert_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_post_bert_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_bert_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_bert_final.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in torch.utils.data.DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_bert_final(\n",
        "            z_post_bert_final[edge[0]], z_post_bert_final[edge[1]],\n",
        "            embedding_global_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_post_bert_final, embedding_global_v2)\n",
        "dummy_model = DummyModel()\n",
        "results_post_bert_final_v2 = test(dummy_model, wrapped_predictor, z_post_bert_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados PostProcessSAGE (Prompt V2, con BERT como señal final):\")\n",
        "for k, v in results_post_bert_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.144394</td>\n",
              "      <td>0.244927</td>\n",
              "      <td>0.289559</td>\n",
              "      <td>0.306767</td>\n",
              "      <td>0.362846</td>\n",
              "      <td>0.9031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.159069</td>\n",
              "      <td>0.249339</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.292009</td>\n",
              "      <td>0.309688</td>\n",
              "      <td>0.8972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.182367</td>\n",
              "      <td>0.201028</td>\n",
              "      <td>0.225502</td>\n",
              "      <td>0.243181</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.029223</td>\n",
              "      <td>0.7761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.013859</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>0.031456</td>\n",
              "      <td>0.7997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.026317</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>0.041756</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.7884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.8339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>default</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099873</td>\n",
              "      <td>0.131928</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.154694</td>\n",
              "      <td>0.166321</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>default</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.066095</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.101409</td>\n",
              "      <td>0.110938</td>\n",
              "      <td>0.8477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.177910</td>\n",
              "      <td>0.252201</td>\n",
              "      <td>0.317981</td>\n",
              "      <td>0.347175</td>\n",
              "      <td>0.364524</td>\n",
              "      <td>0.9012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.160380</td>\n",
              "      <td>0.199724</td>\n",
              "      <td>0.228341</td>\n",
              "      <td>0.252583</td>\n",
              "      <td>0.8987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178404</td>\n",
              "      <td>0.202983</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.252553</td>\n",
              "      <td>0.267580</td>\n",
              "      <td>0.8981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003296</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>0.020226</td>\n",
              "      <td>0.7771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.037561</td>\n",
              "      <td>0.077707</td>\n",
              "      <td>0.090674</td>\n",
              "      <td>0.094899</td>\n",
              "      <td>0.099813</td>\n",
              "      <td>0.7046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.8083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.8475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.8310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.039621</td>\n",
              "      <td>0.059188</td>\n",
              "      <td>0.102458</td>\n",
              "      <td>0.112114</td>\n",
              "      <td>0.119261</td>\n",
              "      <td>0.8508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source                Prompt Type  \\\n",
              "0          SAGEConv            classic                          -   \n",
              "1          SAGEConv      classic + GPT                    default   \n",
              "2          SAGEConv            classic                    default   \n",
              "3      SkipConnSAGE            classic                          -   \n",
              "4      SkipConnSAGE      classic + GPT                    default   \n",
              "5      SkipConnSAGE            classic                    default   \n",
              "6   PostProcessSAGE            classic                          -   \n",
              "7   PostProcessSAGE      classic + GPT                    default   \n",
              "8   PostProcessSAGE            classic                    default   \n",
              "9          SAGEConv           classic                           -   \n",
              "10         SAGEConv  classic + BioBERT                    default   \n",
              "11         SAGEConv            classic                    default   \n",
              "12     SkipConnSAGE            classic                          -   \n",
              "13     SkipConnSAGE  classic + BioBERT                    default   \n",
              "14     SkipConnSAGE            classic                    default   \n",
              "15  PostProcessSAGE            classic                          -   \n",
              "16  PostProcessSAGE  classic + BioBERT                    default   \n",
              "17  PostProcessSAGE            classic                    default   \n",
              "18         SAGEConv            classic                          -   \n",
              "19         SAGEConv  classic + SciBERT                    default   \n",
              "20         SAGEConv            classic                    default   \n",
              "21     SkipConnSAGE            classic                          -   \n",
              "22     SkipConnSAGE  classic + SciBERT                    default   \n",
              "23     SkipConnSAGE            classic                    default   \n",
              "24  PostProcessSAGE            classic                          -   \n",
              "25  PostProcessSAGE  classic + SciBERT                    default   \n",
              "26  PostProcessSAGE            classic                    default   \n",
              "27         SAGEConv            classic                          -   \n",
              "28         SAGEConv     classic + BERT  explicit structural focus   \n",
              "29         SAGEConv            classic  explicit structural focus   \n",
              "30     SkipConnSAGE            classic                          -   \n",
              "31     SkipConnSAGE     classic + BERT  explicit structural focus   \n",
              "32     SkipConnSAGE            classic  explicit structural focus   \n",
              "33  PostProcessSAGE            classic                          -   \n",
              "34  PostProcessSAGE     classic + BERT  explicit structural focus   \n",
              "35  PostProcessSAGE            classic  explicit structural focus   \n",
              "\n",
              "   GPT Injection         Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -            Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input            Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final      Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -            Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input            Neural     100  0.000809  0.001049  0.001288   \n",
              "5          final      Neural + GPT     100  0.003274  0.007911  0.012196   \n",
              "6              -            Neural      50  0.005206  0.210482  0.265707   \n",
              "7          input            Neural      50  0.007334  0.007334  0.132925   \n",
              "8          final      Neural + GPT      50  0.038385  0.048438  0.053443   \n",
              "9              -            Neural      50  0.112204  0.156620  0.191394   \n",
              "10         input            Neural      50  0.285020  0.370045  0.423773   \n",
              "11         final  Neural + BioBERT      50  0.054461  0.084194  0.098555   \n",
              "12             -            Neural      50  0.017919  0.031164  0.037546   \n",
              "13         input            Neural      50  0.000277  0.000622  0.001079   \n",
              "14         final  Neural + BioBERT      50  0.015762  0.019829  0.028309   \n",
              "15             -            Neural      50  0.010203  0.010203  0.010203   \n",
              "16         input            Neural      50  0.003813  0.003813  0.003813   \n",
              "17         final  Neural + BioBERT      50  0.071324  0.083932  0.100870   \n",
              "18             -            Neural      50  0.144394  0.244927  0.289559   \n",
              "19         input            Neural      50  0.159069  0.249339  0.278809   \n",
              "20         final  Neural + SciBERT      50  0.078329  0.182367  0.201028   \n",
              "21             -            Neural      50  0.008188  0.016661  0.019335   \n",
              "22         input            Neural      50  0.006270  0.013859  0.019290   \n",
              "23         final  Neural + SciBERT      50  0.026317  0.033389  0.041756   \n",
              "24             -            Neural      50  0.000562  0.000562  0.000562   \n",
              "25         input            Neural      50  0.099873  0.131928  0.144664   \n",
              "26         final  Neural + SciBERT      50  0.066095  0.080838  0.090659   \n",
              "27             -            Neural      50  0.177910  0.252201  0.317981   \n",
              "28         input            Neural      50  0.121800  0.160380  0.199724   \n",
              "29         final     Neural + BERT      50  0.178404  0.202983  0.235135   \n",
              "30             -            Neural      50  0.000015  0.003296  0.014518   \n",
              "31         input            Neural      50  0.037561  0.077707  0.090674   \n",
              "32         final     Neural + BERT      50  0.000165  0.000524  0.000914   \n",
              "33             -            Neural      50  0.000367  0.000367  0.000367   \n",
              "34         input            Neural      50  0.005439  0.005439  0.005439   \n",
              "35         final     Neural + BERT      50  0.039621  0.059188  0.102458   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "5   0.014923  0.017664    0.7774  \n",
              "6   0.293904  0.322446    0.8781  \n",
              "7   0.153601  0.181296    0.8706  \n",
              "8   0.061211  0.068358    0.8070  \n",
              "9   0.222917  0.243204    0.9000  \n",
              "10  0.445228  0.485149    0.9281  \n",
              "11  0.116526  0.126265    0.8887  \n",
              "12  0.045494  0.050761    0.7755  \n",
              "13  0.001671  0.002682    0.7881  \n",
              "14  0.033471  0.037501    0.8014  \n",
              "15  0.010203  0.180517    0.8596  \n",
              "16  0.003813  0.003813    0.8574  \n",
              "17  0.115440  0.124819    0.8507  \n",
              "18  0.306767  0.362846    0.9031  \n",
              "19  0.292009  0.309688    0.8972  \n",
              "20  0.225502  0.243181    0.8985  \n",
              "21  0.025343  0.029223    0.7761  \n",
              "22  0.025395  0.031456    0.7997  \n",
              "23  0.049000  0.057698    0.7884  \n",
              "24  0.000562  0.000562    0.8339  \n",
              "25  0.154694  0.166321    0.8644  \n",
              "26  0.101409  0.110938    0.8477  \n",
              "27  0.347175  0.364524    0.9012  \n",
              "28  0.228341  0.252583    0.8987  \n",
              "29  0.252553  0.267580    0.8981  \n",
              "30  0.016676  0.020226    0.7771  \n",
              "31  0.094899  0.099813    0.7046  \n",
              "32  0.001618  0.002614    0.8083  \n",
              "33  0.000367  0.000367    0.8475  \n",
              "34  0.005439  0.005439    0.8310  \n",
              "35  0.112114  0.119261    0.8508  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "new_prompt_type = \"explicit structural focus\"  \n",
        "new_rows = [\n",
        "    # SAGEConv + BERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_base_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_input_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_sage_final_v2},\n",
        "\n",
        "    # SkipConnSAGE + BERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_base_v2},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_input_v2},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_skip_final_v2},\n",
        "\n",
        "    # PostProcessSAGE + BERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_base_v2},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bert_input_v2},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_post_bert_final_v2},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.144394</td>\n",
              "      <td>0.244927</td>\n",
              "      <td>0.289559</td>\n",
              "      <td>0.306767</td>\n",
              "      <td>0.362846</td>\n",
              "      <td>0.9031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.159069</td>\n",
              "      <td>0.249339</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.292009</td>\n",
              "      <td>0.309688</td>\n",
              "      <td>0.8972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.182367</td>\n",
              "      <td>0.201028</td>\n",
              "      <td>0.225502</td>\n",
              "      <td>0.243181</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.029223</td>\n",
              "      <td>0.7761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.013859</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>0.031456</td>\n",
              "      <td>0.7997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.026317</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>0.041756</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.7884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.8339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099873</td>\n",
              "      <td>0.131928</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.154694</td>\n",
              "      <td>0.166321</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.066095</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.101409</td>\n",
              "      <td>0.110938</td>\n",
              "      <td>0.8477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.177910</td>\n",
              "      <td>0.252201</td>\n",
              "      <td>0.317981</td>\n",
              "      <td>0.347175</td>\n",
              "      <td>0.364524</td>\n",
              "      <td>0.9012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.160380</td>\n",
              "      <td>0.199724</td>\n",
              "      <td>0.228341</td>\n",
              "      <td>0.252583</td>\n",
              "      <td>0.8987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178404</td>\n",
              "      <td>0.202983</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.252553</td>\n",
              "      <td>0.267580</td>\n",
              "      <td>0.8981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003296</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>0.020226</td>\n",
              "      <td>0.7771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.037561</td>\n",
              "      <td>0.077707</td>\n",
              "      <td>0.090674</td>\n",
              "      <td>0.094899</td>\n",
              "      <td>0.099813</td>\n",
              "      <td>0.7046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.8083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.8475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.8310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.039621</td>\n",
              "      <td>0.059188</td>\n",
              "      <td>0.102458</td>\n",
              "      <td>0.112114</td>\n",
              "      <td>0.119261</td>\n",
              "      <td>0.8508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source                Prompt Type  \\\n",
              "0          SAGEConv            classic                          -   \n",
              "1          SAGEConv      classic + GPT    reasoning-based summary   \n",
              "2          SAGEConv            classic    reasoning-based summary   \n",
              "3      SkipConnSAGE            classic                          -   \n",
              "4      SkipConnSAGE      classic + GPT    reasoning-based summary   \n",
              "5      SkipConnSAGE            classic    reasoning-based summary   \n",
              "6   PostProcessSAGE            classic                          -   \n",
              "7   PostProcessSAGE      classic + GPT    reasoning-based summary   \n",
              "8   PostProcessSAGE            classic    reasoning-based summary   \n",
              "9          SAGEConv           classic                           -   \n",
              "10         SAGEConv  classic + BioBERT    reasoning-based summary   \n",
              "11         SAGEConv            classic    reasoning-based summary   \n",
              "12     SkipConnSAGE            classic                          -   \n",
              "13     SkipConnSAGE  classic + BioBERT    reasoning-based summary   \n",
              "14     SkipConnSAGE            classic    reasoning-based summary   \n",
              "15  PostProcessSAGE            classic                          -   \n",
              "16  PostProcessSAGE  classic + BioBERT    reasoning-based summary   \n",
              "17  PostProcessSAGE            classic    reasoning-based summary   \n",
              "18         SAGEConv            classic                          -   \n",
              "19         SAGEConv  classic + SciBERT    reasoning-based summary   \n",
              "20         SAGEConv            classic    reasoning-based summary   \n",
              "21     SkipConnSAGE            classic                          -   \n",
              "22     SkipConnSAGE  classic + SciBERT    reasoning-based summary   \n",
              "23     SkipConnSAGE            classic    reasoning-based summary   \n",
              "24  PostProcessSAGE            classic                          -   \n",
              "25  PostProcessSAGE  classic + SciBERT    reasoning-based summary   \n",
              "26  PostProcessSAGE            classic    reasoning-based summary   \n",
              "27         SAGEConv            classic                          -   \n",
              "28         SAGEConv     classic + BERT  explicit structural focus   \n",
              "29         SAGEConv            classic  explicit structural focus   \n",
              "30     SkipConnSAGE            classic                          -   \n",
              "31     SkipConnSAGE     classic + BERT  explicit structural focus   \n",
              "32     SkipConnSAGE            classic  explicit structural focus   \n",
              "33  PostProcessSAGE            classic                          -   \n",
              "34  PostProcessSAGE     classic + BERT  explicit structural focus   \n",
              "35  PostProcessSAGE            classic  explicit structural focus   \n",
              "\n",
              "   GPT Injection         Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -            Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input            Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final      Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -            Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input            Neural     100  0.000809  0.001049  0.001288   \n",
              "5          final      Neural + GPT     100  0.003274  0.007911  0.012196   \n",
              "6              -            Neural      50  0.005206  0.210482  0.265707   \n",
              "7          input            Neural      50  0.007334  0.007334  0.132925   \n",
              "8          final      Neural + GPT      50  0.038385  0.048438  0.053443   \n",
              "9              -            Neural      50  0.112204  0.156620  0.191394   \n",
              "10         input            Neural      50  0.285020  0.370045  0.423773   \n",
              "11         final  Neural + BioBERT      50  0.054461  0.084194  0.098555   \n",
              "12             -            Neural      50  0.017919  0.031164  0.037546   \n",
              "13         input            Neural      50  0.000277  0.000622  0.001079   \n",
              "14         final  Neural + BioBERT      50  0.015762  0.019829  0.028309   \n",
              "15             -            Neural      50  0.010203  0.010203  0.010203   \n",
              "16         input            Neural      50  0.003813  0.003813  0.003813   \n",
              "17         final  Neural + BioBERT      50  0.071324  0.083932  0.100870   \n",
              "18             -            Neural      50  0.144394  0.244927  0.289559   \n",
              "19         input            Neural      50  0.159069  0.249339  0.278809   \n",
              "20         final  Neural + SciBERT      50  0.078329  0.182367  0.201028   \n",
              "21             -            Neural      50  0.008188  0.016661  0.019335   \n",
              "22         input            Neural      50  0.006270  0.013859  0.019290   \n",
              "23         final  Neural + SciBERT      50  0.026317  0.033389  0.041756   \n",
              "24             -            Neural      50  0.000562  0.000562  0.000562   \n",
              "25         input            Neural      50  0.099873  0.131928  0.144664   \n",
              "26         final  Neural + SciBERT      50  0.066095  0.080838  0.090659   \n",
              "27             -            Neural      50  0.177910  0.252201  0.317981   \n",
              "28         input            Neural      50  0.121800  0.160380  0.199724   \n",
              "29         final     Neural + BERT      50  0.178404  0.202983  0.235135   \n",
              "30             -            Neural      50  0.000015  0.003296  0.014518   \n",
              "31         input            Neural      50  0.037561  0.077707  0.090674   \n",
              "32         final     Neural + BERT      50  0.000165  0.000524  0.000914   \n",
              "33             -            Neural      50  0.000367  0.000367  0.000367   \n",
              "34         input            Neural      50  0.005439  0.005439  0.005439   \n",
              "35         final     Neural + BERT      50  0.039621  0.059188  0.102458   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "5   0.014923  0.017664    0.7774  \n",
              "6   0.293904  0.322446    0.8781  \n",
              "7   0.153601  0.181296    0.8706  \n",
              "8   0.061211  0.068358    0.8070  \n",
              "9   0.222917  0.243204    0.9000  \n",
              "10  0.445228  0.485149    0.9281  \n",
              "11  0.116526  0.126265    0.8887  \n",
              "12  0.045494  0.050761    0.7755  \n",
              "13  0.001671  0.002682    0.7881  \n",
              "14  0.033471  0.037501    0.8014  \n",
              "15  0.010203  0.180517    0.8596  \n",
              "16  0.003813  0.003813    0.8574  \n",
              "17  0.115440  0.124819    0.8507  \n",
              "18  0.306767  0.362846    0.9031  \n",
              "19  0.292009  0.309688    0.8972  \n",
              "20  0.225502  0.243181    0.8985  \n",
              "21  0.025343  0.029223    0.7761  \n",
              "22  0.025395  0.031456    0.7997  \n",
              "23  0.049000  0.057698    0.7884  \n",
              "24  0.000562  0.000562    0.8339  \n",
              "25  0.154694  0.166321    0.8644  \n",
              "26  0.101409  0.110938    0.8477  \n",
              "27  0.347175  0.364524    0.9012  \n",
              "28  0.228341  0.252583    0.8987  \n",
              "29  0.252553  0.267580    0.8981  \n",
              "30  0.016676  0.020226    0.7771  \n",
              "31  0.094899  0.099813    0.7046  \n",
              "32  0.001618  0.002614    0.8083  \n",
              "33  0.000367  0.000367    0.8475  \n",
              "34  0.005439  0.005439    0.8310  \n",
              "35  0.112114  0.119261    0.8508  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reemplazar \"default\" por \"reasoning-based summary\"\n",
        "df_summary[\"Prompt Type\"] = df_summary[\"Prompt Type\"].replace(\"default\", \"reasoning-based summary\")\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "embedding_biobert_v2 = model.encode(gpt_response_text_v2)\n",
        "embedding_biobert_tensor_v2 = torch.tensor(embedding_biobert_v2, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.3602\n",
            "Epoch 1 has loss 10.2152\n",
            "Epoch 2 has loss 9.3006\n",
            "Epoch 3 has loss 8.6122\n",
            "Epoch 4 has loss 7.833\n",
            "Epoch 5 has loss 7.472\n",
            "Epoch 6 has loss 7.3401\n",
            "Epoch 7 has loss 6.8975\n",
            "Epoch 8 has loss 6.604\n",
            "Epoch 9 has loss 6.5535\n",
            "Epoch 10 has loss 6.3442\n",
            "Epoch 11 has loss 6.1545\n",
            "Epoch 12 has loss 5.9042\n",
            "Epoch 13 has loss 5.7567\n",
            "Epoch 14 has loss 5.8152\n",
            "Epoch 15 has loss 5.618\n",
            "Epoch 16 has loss 5.5368\n",
            "Epoch 17 has loss 5.4513\n",
            "Epoch 18 has loss 5.4516\n",
            "Epoch 19 has loss 5.2735\n",
            "Epoch 20 has loss 5.3038\n",
            "Epoch 21 has loss 5.2258\n",
            "Epoch 22 has loss 5.0937\n",
            "Epoch 23 has loss 5.1145\n",
            "Epoch 24 has loss 4.8114\n",
            "Epoch 25 has loss 4.8206\n",
            "Epoch 26 has loss 4.6721\n",
            "Epoch 27 has loss 4.7441\n",
            "Epoch 28 has loss 4.6076\n",
            "Epoch 29 has loss 4.6443\n",
            "Epoch 30 has loss 4.6886\n",
            "Epoch 31 has loss 4.6485\n",
            "Epoch 32 has loss 4.6251\n",
            "Epoch 33 has loss 4.518\n",
            "Epoch 34 has loss 4.409\n",
            "Epoch 35 has loss 4.3833\n",
            "Epoch 36 has loss 4.3701\n",
            "Epoch 37 has loss 4.5753\n",
            "Epoch 38 has loss 4.5274\n",
            "Epoch 39 has loss 4.32\n",
            "Epoch 40 has loss 4.4485\n",
            "Epoch 41 has loss 4.38\n",
            "Epoch 42 has loss 4.3162\n",
            "Epoch 43 has loss 4.2543\n",
            "Epoch 44 has loss 4.2414\n",
            "Epoch 45 has loss 4.186\n",
            "Epoch 46 has loss 4.1356\n",
            "Epoch 47 has loss 4.2159\n",
            "Epoch 48 has loss 4.2861\n",
            "Epoch 49 has loss 4.1189\n",
            "  31435 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  39328 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  44830 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  46950 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  49946 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + BioBERT (sin usar el embedding):\n",
            "Hits@10: 0.2355\n",
            "Hits@20: 0.2946\n",
            "Hits@30: 0.3358\n",
            "Hits@40: 0.3517\n",
            "Hits@50: 0.3742\n",
            "Accuracy: 0.9025\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_bio_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + BioBERT (sin usar el embedding):\")\n",
        "for k, v in results_sage_bio_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5807\n",
            "Epoch 1 has loss 9.4797\n",
            "Epoch 2 has loss 8.3278\n",
            "Epoch 3 has loss 7.8243\n",
            "Epoch 4 has loss 7.5599\n",
            "Epoch 5 has loss 7.4725\n",
            "Epoch 6 has loss 7.4116\n",
            "Epoch 7 has loss 7.3509\n",
            "Epoch 8 has loss 7.3309\n",
            "Epoch 9 has loss 7.101\n",
            "Epoch 10 has loss 7.0613\n",
            "Epoch 11 has loss 7.1333\n",
            "Epoch 12 has loss 6.966\n",
            "Epoch 13 has loss 6.9025\n",
            "Epoch 14 has loss 6.5779\n",
            "Epoch 15 has loss 6.3644\n",
            "Epoch 16 has loss 6.7048\n",
            "Epoch 17 has loss 6.1828\n",
            "Epoch 18 has loss 6.257\n",
            "Epoch 19 has loss 5.9155\n",
            "Epoch 20 has loss 5.6635\n",
            "Epoch 21 has loss 5.5502\n",
            "Epoch 22 has loss 5.5252\n",
            "Epoch 23 has loss 5.2419\n",
            "Epoch 24 has loss 5.2537\n",
            "Epoch 25 has loss 5.3084\n",
            "Epoch 26 has loss 5.1734\n",
            "Epoch 27 has loss 5.0416\n",
            "Epoch 28 has loss 4.9995\n",
            "Epoch 29 has loss 4.8679\n",
            "Epoch 30 has loss 4.92\n",
            "Epoch 31 has loss 4.7152\n",
            "Epoch 32 has loss 4.7327\n",
            "Epoch 33 has loss 4.7202\n",
            "Epoch 34 has loss 4.5247\n",
            "Epoch 35 has loss 4.5424\n",
            "Epoch 36 has loss 4.5374\n",
            "Epoch 37 has loss 4.5472\n",
            "Epoch 38 has loss 4.4999\n",
            "Epoch 39 has loss 4.4646\n",
            "Epoch 40 has loss 4.4\n",
            "Epoch 41 has loss 4.4043\n",
            "Epoch 42 has loss 4.2923\n",
            "Epoch 43 has loss 4.2234\n",
            "Epoch 44 has loss 4.2084\n",
            "Epoch 45 has loss 4.1746\n",
            "Epoch 46 has loss 4.0858\n",
            "Epoch 47 has loss 4.2268\n",
            "Epoch 48 has loss 4.2704\n",
            "Epoch 49 has loss 4.1353\n",
            "  23848 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  28410 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  33635 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  38773 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  40698 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGEConv + BioBERT (como input):\n",
            "Hits@10: 0.1787\n",
            "Hits@20: 0.2128\n",
            "Hits@30: 0.2520\n",
            "Hits@40: 0.2905\n",
            "Hits@50: 0.3049\n",
            "Accuracy: 0.9051\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_bio_input_v2 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGEConv + BioBERT (como input):\")\n",
        "for k, v in results_sage_bio_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6676\n",
            "Epoch 1 has loss 10.2833\n",
            "Epoch 2 has loss 10.2035\n",
            "Epoch 3 has loss 10.1277\n",
            "Epoch 4 has loss 10.0444\n",
            "Epoch 5 has loss 10.0566\n",
            "Epoch 6 has loss 9.9866\n",
            "Epoch 7 has loss 10.0045\n",
            "Epoch 8 has loss 9.9936\n",
            "Epoch 9 has loss 9.9102\n",
            "Epoch 10 has loss 9.8015\n",
            "Epoch 11 has loss 9.7172\n",
            "Epoch 12 has loss 9.6379\n",
            "Epoch 13 has loss 9.7298\n",
            "Epoch 14 has loss 9.656\n",
            "Epoch 15 has loss 9.5586\n",
            "Epoch 16 has loss 9.5471\n",
            "Epoch 17 has loss 9.5184\n",
            "Epoch 18 has loss 9.5105\n",
            "Epoch 19 has loss 9.4992\n",
            "Epoch 20 has loss 9.4914\n",
            "Epoch 21 has loss 9.4866\n",
            "Epoch 22 has loss 9.4896\n",
            "Epoch 23 has loss 9.4904\n",
            "Epoch 24 has loss 9.4933\n",
            "Epoch 25 has loss 9.4883\n",
            "Epoch 26 has loss 9.4876\n",
            "Epoch 27 has loss 9.4796\n",
            "Epoch 28 has loss 9.4792\n",
            "Epoch 29 has loss 9.4762\n",
            "Epoch 30 has loss 9.4696\n",
            "Epoch 31 has loss 9.4675\n",
            "Epoch 32 has loss 9.463\n",
            "Epoch 33 has loss 9.468\n",
            "Epoch 34 has loss 9.4551\n",
            "Epoch 35 has loss 9.4551\n",
            "Epoch 36 has loss 9.4571\n",
            "Epoch 37 has loss 9.4579\n",
            "Epoch 38 has loss 9.4568\n",
            "Epoch 39 has loss 9.4662\n",
            "Epoch 40 has loss 9.4593\n",
            "Epoch 41 has loss 9.454\n",
            "Epoch 42 has loss 9.4567\n",
            "Epoch 43 has loss 9.4831\n",
            "Epoch 44 has loss 9.4686\n",
            "Epoch 45 has loss 9.4569\n",
            "Epoch 46 has loss 9.448\n",
            "Epoch 47 has loss 9.4502\n",
            "Epoch 48 has loss 9.4576\n",
            "Epoch 49 has loss 9.4502\n",
            "📉 Epoch 0, Loss: 23.3071\n",
            "📉 Epoch 1, Loss: 15.6765\n",
            "📉 Epoch 2, Loss: 14.5886\n",
            "📉 Epoch 3, Loss: 13.6854\n",
            "📉 Epoch 4, Loss: 13.3911\n",
            "📉 Epoch 5, Loss: 13.3080\n",
            "📉 Epoch 6, Loss: 13.1093\n",
            "📉 Epoch 7, Loss: 13.0671\n",
            "📉 Epoch 8, Loss: 12.9761\n",
            "📉 Epoch 9, Loss: 12.9551\n",
            "📉 Epoch 10, Loss: 12.8698\n",
            "📉 Epoch 11, Loss: 12.8261\n",
            "📉 Epoch 12, Loss: 12.9766\n",
            "📉 Epoch 13, Loss: 12.7956\n",
            "📉 Epoch 14, Loss: 12.8196\n",
            "📉 Epoch 15, Loss: 12.7292\n",
            "📉 Epoch 16, Loss: 12.6786\n",
            "📉 Epoch 17, Loss: 12.6676\n",
            "📉 Epoch 18, Loss: 12.7321\n",
            "📉 Epoch 19, Loss: 12.6223\n",
            "📉 Epoch 20, Loss: 12.5910\n",
            "📉 Epoch 21, Loss: 12.7083\n",
            "📉 Epoch 22, Loss: 12.7650\n",
            "📉 Epoch 23, Loss: 12.7517\n",
            "📉 Epoch 24, Loss: 12.5837\n",
            "📉 Epoch 25, Loss: 12.5459\n",
            "📉 Epoch 26, Loss: 12.5496\n",
            "📉 Epoch 27, Loss: 12.6815\n",
            "📉 Epoch 28, Loss: 12.7632\n",
            "📉 Epoch 29, Loss: 12.6051\n",
            "📉 Epoch 30, Loss: 12.5173\n",
            "📉 Epoch 31, Loss: 12.5352\n",
            "📉 Epoch 32, Loss: 12.5202\n",
            "📉 Epoch 33, Loss: 12.6395\n",
            "📉 Epoch 34, Loss: 12.5830\n",
            "📉 Epoch 35, Loss: 12.4568\n",
            "📉 Epoch 36, Loss: 12.4742\n",
            "📉 Epoch 37, Loss: 12.4681\n",
            "📉 Epoch 38, Loss: 12.5316\n",
            "📉 Epoch 39, Loss: 12.7677\n",
            "📉 Epoch 40, Loss: 12.5295\n",
            "📉 Epoch 41, Loss: 12.4890\n",
            "📉 Epoch 42, Loss: 12.4255\n",
            "📉 Epoch 43, Loss: 12.3992\n",
            "📉 Epoch 44, Loss: 12.5126\n",
            "📉 Epoch 45, Loss: 12.4541\n",
            "📉 Epoch 46, Loss: 12.3841\n",
            "📉 Epoch 47, Loss: 12.4984\n",
            "📉 Epoch 48, Loss: 12.3724\n",
            "📉 Epoch 49, Loss: 12.3367\n",
            "  4859 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5579 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  6127 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  7054 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  7662 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔬 Resultados SAGEConv + BioBERT (en predictor final):\n",
            "Hits@10: 0.0364\n",
            "Hits@20: 0.0418\n",
            "Hits@30: 0.0459\n",
            "Hits@40: 0.0528\n",
            "Hits@50: 0.0574\n",
            "Accuracy: 0.8582\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_bio_final = model(emb, adj_t)\n",
        "\n",
        "predictor_bio_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_bio_final.size(1),\n",
        "    hidden_channels=z_bio_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_bio_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_bio_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_bio_final.train()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_bio_final(\n",
        "            z_bio_final[edge[0]], z_bio_final[edge[1]],\n",
        "            embedding_biobert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_bio = WrappedPredictor(predictor_bio_final, embedding_biobert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_bio_final_v2 = test(dummy_model, wrapped_predictor_bio, z_bio_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔬 Resultados SAGEConv + BioBERT (en predictor final):\")\n",
        "for k, v in results_sage_bio_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.3468\n",
            "Epoch 1 has loss 10.3554\n",
            "Epoch 2 has loss 10.477\n",
            "Epoch 3 has loss 10.3148\n",
            "Epoch 4 has loss 9.8027\n",
            "Epoch 5 has loss 9.7982\n",
            "Epoch 6 has loss 9.5361\n",
            "Epoch 7 has loss 9.4751\n",
            "Epoch 8 has loss 9.7395\n",
            "Epoch 9 has loss 9.8663\n",
            "Epoch 10 has loss 9.5899\n",
            "Epoch 11 has loss 10.0494\n",
            "Epoch 12 has loss 9.9707\n",
            "Epoch 13 has loss 9.6028\n",
            "Epoch 14 has loss 9.5158\n",
            "Epoch 15 has loss 9.3354\n",
            "Epoch 16 has loss 9.0473\n",
            "Epoch 17 has loss 8.7915\n",
            "Epoch 18 has loss 8.9424\n",
            "Epoch 19 has loss 8.7509\n",
            "Epoch 20 has loss 8.7322\n",
            "Epoch 21 has loss 8.4511\n",
            "Epoch 22 has loss 8.6897\n",
            "Epoch 23 has loss 8.3117\n",
            "Epoch 24 has loss 8.7723\n",
            "Epoch 25 has loss 8.2927\n",
            "Epoch 26 has loss 8.0909\n",
            "Epoch 27 has loss 8.1039\n",
            "Epoch 28 has loss 7.8625\n",
            "Epoch 29 has loss 8.2114\n",
            "Epoch 30 has loss 8.1853\n",
            "Epoch 31 has loss 7.9279\n",
            "Epoch 32 has loss 7.8679\n",
            "Epoch 33 has loss 7.7797\n",
            "Epoch 34 has loss 7.8713\n",
            "Epoch 35 has loss 8.4732\n",
            "Epoch 36 has loss 7.9562\n",
            "Epoch 37 has loss 7.6644\n",
            "Epoch 38 has loss 7.9799\n",
            "Epoch 39 has loss 7.7267\n",
            "Epoch 40 has loss 7.7173\n",
            "Epoch 41 has loss 7.3984\n",
            "Epoch 42 has loss 7.2028\n",
            "Epoch 43 has loss 7.0165\n",
            "Epoch 44 has loss 6.7876\n",
            "Epoch 45 has loss 6.8734\n",
            "Epoch 46 has loss 6.764\n",
            "Epoch 47 has loss 6.6352\n",
            "Epoch 48 has loss 6.7565\n",
            "Epoch 49 has loss 6.4084\n",
            "  2 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  9 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  14 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  17 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE + BioBERT (sin GPT):\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0001\n",
            "Hits@40: 0.0001\n",
            "Hits@50: 0.0001\n",
            "Accuracy: 0.7365\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE + BioBERT (sin GPT):\")\n",
        "for k, v in results_skipconn_bio_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.6055\n",
            "Epoch 1 has loss 10.5495\n",
            "Epoch 2 has loss 10.1899\n",
            "Epoch 3 has loss 10.8724\n",
            "Epoch 4 has loss 10.1854\n",
            "Epoch 5 has loss 9.9562\n",
            "Epoch 6 has loss 9.7775\n",
            "Epoch 7 has loss 9.679\n",
            "Epoch 8 has loss 9.5908\n",
            "Epoch 9 has loss 9.3264\n",
            "Epoch 10 has loss 9.3449\n",
            "Epoch 11 has loss 9.1661\n",
            "Epoch 12 has loss 9.3366\n",
            "Epoch 13 has loss 9.3057\n",
            "Epoch 14 has loss 9.118\n",
            "Epoch 15 has loss 8.974\n",
            "Epoch 16 has loss 8.9894\n",
            "Epoch 17 has loss 8.7126\n",
            "Epoch 18 has loss 8.7507\n",
            "Epoch 19 has loss 8.5267\n",
            "Epoch 20 has loss 8.4192\n",
            "Epoch 21 has loss 8.6055\n",
            "Epoch 22 has loss 8.2399\n",
            "Epoch 23 has loss 8.289\n",
            "Epoch 24 has loss 8.0567\n",
            "Epoch 25 has loss 8.4096\n",
            "Epoch 26 has loss 8.1654\n",
            "Epoch 27 has loss 8.117\n",
            "Epoch 28 has loss 8.2671\n",
            "Epoch 29 has loss 7.9805\n",
            "Epoch 30 has loss 7.9338\n",
            "Epoch 31 has loss 7.7648\n",
            "Epoch 32 has loss 8.2225\n",
            "Epoch 33 has loss 8.1999\n",
            "Epoch 34 has loss 7.6327\n",
            "Epoch 35 has loss 7.1542\n",
            "Epoch 36 has loss 7.0833\n",
            "Epoch 37 has loss 6.9841\n",
            "Epoch 38 has loss 7.0336\n",
            "Epoch 39 has loss 6.6504\n",
            "Epoch 40 has loss 6.746\n",
            "Epoch 41 has loss 6.5244\n",
            "Epoch 42 has loss 6.0991\n",
            "Epoch 43 has loss 6.8044\n",
            "Epoch 44 has loss 6.1929\n",
            "Epoch 45 has loss 6.0835\n",
            "Epoch 46 has loss 5.864\n",
            "Epoch 47 has loss 5.6805\n",
            "Epoch 48 has loss 5.5821\n",
            "Epoch 49 has loss 5.6166\n",
            "  1118 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  1464 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  1698 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  2092 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  2575 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE + BioBERT (como input):\n",
            "Hits@10: 0.0084\n",
            "Hits@20: 0.0110\n",
            "Hits@30: 0.0127\n",
            "Hits@40: 0.0157\n",
            "Hits@50: 0.0193\n",
            "Accuracy: 0.7638\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_input_v2 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE + BioBERT (como input):\")\n",
        "for k, v in results_skipconn_bio_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1904\n",
            "Epoch 1 has loss 10.7059\n",
            "Epoch 2 has loss 10.5329\n",
            "Epoch 3 has loss 10.4003\n",
            "Epoch 4 has loss 10.3569\n",
            "Epoch 5 has loss 10.255\n",
            "Epoch 6 has loss 10.2559\n",
            "Epoch 7 has loss 10.2394\n",
            "Epoch 8 has loss 10.1865\n",
            "Epoch 9 has loss 10.1703\n",
            "Epoch 10 has loss 10.2028\n",
            "Epoch 11 has loss 10.1321\n",
            "Epoch 12 has loss 10.1578\n",
            "Epoch 13 has loss 10.1555\n",
            "Epoch 14 has loss 10.1048\n",
            "Epoch 15 has loss 10.1065\n",
            "Epoch 16 has loss 10.0907\n",
            "Epoch 17 has loss 10.0936\n",
            "Epoch 18 has loss 10.1351\n",
            "Epoch 19 has loss 10.1386\n",
            "Epoch 20 has loss 10.0894\n",
            "Epoch 21 has loss 10.0906\n",
            "Epoch 22 has loss 10.053\n",
            "Epoch 23 has loss 10.0175\n",
            "Epoch 24 has loss 10.3263\n",
            "Epoch 25 has loss 10.1932\n",
            "Epoch 26 has loss 10.148\n",
            "Epoch 27 has loss 10.0999\n",
            "Epoch 28 has loss 10.0803\n",
            "Epoch 29 has loss 10.0748\n",
            "Epoch 30 has loss 10.0968\n",
            "Epoch 31 has loss 10.0693\n",
            "Epoch 32 has loss 10.0246\n",
            "Epoch 33 has loss 10.0178\n",
            "Epoch 34 has loss 10.0531\n",
            "Epoch 35 has loss 10.0105\n",
            "Epoch 36 has loss 9.9853\n",
            "Epoch 37 has loss 9.9988\n",
            "Epoch 38 has loss 10.0323\n",
            "Epoch 39 has loss 9.9896\n",
            "Epoch 40 has loss 9.9859\n",
            "Epoch 41 has loss 9.9707\n",
            "Epoch 42 has loss 9.964\n",
            "Epoch 43 has loss 9.9191\n",
            "Epoch 44 has loss 9.9474\n",
            "Epoch 45 has loss 9.9593\n",
            "Epoch 46 has loss 9.9805\n",
            "Epoch 47 has loss 9.8941\n",
            "Epoch 48 has loss 9.8718\n",
            "Epoch 49 has loss 9.9768\n",
            "📉 Epoch 0, Loss: 25.2717\n",
            "📉 Epoch 1, Loss: 22.9346\n",
            "📉 Epoch 2, Loss: 21.6304\n",
            "📉 Epoch 3, Loss: 18.6181\n",
            "📉 Epoch 4, Loss: 18.0543\n",
            "📉 Epoch 5, Loss: 17.8586\n",
            "📉 Epoch 6, Loss: 17.8057\n",
            "📉 Epoch 7, Loss: 18.2235\n",
            "📉 Epoch 8, Loss: 17.8969\n",
            "📉 Epoch 9, Loss: 17.7703\n",
            "📉 Epoch 10, Loss: 17.6891\n",
            "📉 Epoch 11, Loss: 17.6591\n",
            "📉 Epoch 12, Loss: 17.3774\n",
            "📉 Epoch 13, Loss: 17.6108\n",
            "📉 Epoch 14, Loss: 17.9659\n",
            "📉 Epoch 15, Loss: 17.9022\n",
            "📉 Epoch 16, Loss: 18.2193\n",
            "📉 Epoch 17, Loss: 18.0802\n",
            "📉 Epoch 18, Loss: 17.8237\n",
            "📉 Epoch 19, Loss: 18.0249\n",
            "📉 Epoch 20, Loss: 17.6062\n",
            "📉 Epoch 21, Loss: 17.4316\n",
            "📉 Epoch 22, Loss: 17.3996\n",
            "📉 Epoch 23, Loss: 17.2853\n",
            "📉 Epoch 24, Loss: 17.6568\n",
            "📉 Epoch 25, Loss: 18.2252\n",
            "📉 Epoch 26, Loss: 17.3628\n",
            "📉 Epoch 27, Loss: 17.3359\n",
            "📉 Epoch 28, Loss: 17.4279\n",
            "📉 Epoch 29, Loss: 17.1994\n",
            "📉 Epoch 30, Loss: 18.0235\n",
            "📉 Epoch 31, Loss: 17.6813\n",
            "📉 Epoch 32, Loss: 17.3386\n",
            "📉 Epoch 33, Loss: 17.0020\n",
            "📉 Epoch 34, Loss: 17.3377\n",
            "📉 Epoch 35, Loss: 17.0031\n",
            "📉 Epoch 36, Loss: 17.5194\n",
            "📉 Epoch 37, Loss: 17.2948\n",
            "📉 Epoch 38, Loss: 16.6681\n",
            "📉 Epoch 39, Loss: 16.9492\n",
            "📉 Epoch 40, Loss: 16.9157\n",
            "📉 Epoch 41, Loss: 17.3109\n",
            "📉 Epoch 42, Loss: 17.2470\n",
            "📉 Epoch 43, Loss: 16.6973\n",
            "📉 Epoch 44, Loss: 16.6766\n",
            "📉 Epoch 45, Loss: 17.1108\n",
            "📉 Epoch 46, Loss: 16.9689\n",
            "📉 Epoch 47, Loss: 16.7360\n",
            "📉 Epoch 48, Loss: 16.9120\n",
            "📉 Epoch 49, Loss: 16.6923\n",
            "  317 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  673 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  795 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  1097 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  1434 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SkipConnSAGE + BioBERT (GPT final):\n",
            "Hits@10: 0.0024\n",
            "Hits@20: 0.0050\n",
            "Hits@30: 0.0060\n",
            "Hits@40: 0.0082\n",
            "Hits@50: 0.0107\n",
            "Accuracy: 0.7927\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skipconn_bio_v2 = model(emb, adj_t)\n",
        "\n",
        "predictor_skipconn_bio_final_v2 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skipconn_bio_v2.size(1),\n",
        "    hidden_channels=z_skipconn_bio_v2.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_skipconn_bio_final_v2.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_skipconn_bio_final_v2.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skipconn_bio_final_v2.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skipconn_bio_final_v2(\n",
        "            z_skipconn_bio_v2[edge[0]], z_skipconn_bio_v2[edge[1]],\n",
        "            embedding_biobert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_skipconn_bio_final_v2, embedding_biobert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skipconn_bio_final_v2 = test(dummy_model, wrapped_predictor, z_skipconn_bio_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SkipConnSAGE + BioBERT (GPT final):\")\n",
        "for k, v in results_skipconn_bio_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7971\n",
            "Epoch 1 has loss 12.0706\n",
            "Epoch 2 has loss 11.8188\n",
            "Epoch 3 has loss 11.7487\n",
            "Epoch 4 has loss 10.9309\n",
            "Epoch 5 has loss 9.9667\n",
            "Epoch 6 has loss 10.7981\n",
            "Epoch 7 has loss 9.1702\n",
            "Epoch 8 has loss 8.2028\n",
            "Epoch 9 has loss 8.1363\n",
            "Epoch 10 has loss 7.8316\n",
            "Epoch 11 has loss 7.735\n",
            "Epoch 12 has loss 7.5862\n",
            "Epoch 13 has loss 7.5562\n",
            "Epoch 14 has loss 7.5125\n",
            "Epoch 15 has loss 7.4267\n",
            "Epoch 16 has loss 7.3775\n",
            "Epoch 17 has loss 7.3673\n",
            "Epoch 18 has loss 7.3839\n",
            "Epoch 19 has loss 7.3301\n",
            "Epoch 20 has loss 7.3883\n",
            "Epoch 21 has loss 7.3257\n",
            "Epoch 22 has loss 7.3467\n",
            "Epoch 23 has loss 7.3299\n",
            "Epoch 24 has loss 7.3572\n",
            "Epoch 25 has loss 7.3611\n",
            "Epoch 26 has loss 7.3139\n",
            "Epoch 27 has loss 7.3491\n",
            "Epoch 28 has loss 7.2805\n",
            "Epoch 29 has loss 7.3503\n",
            "Epoch 30 has loss 7.2998\n",
            "Epoch 31 has loss 7.2715\n",
            "Epoch 32 has loss 7.2832\n",
            "Epoch 33 has loss 7.3162\n",
            "Epoch 34 has loss 7.2782\n",
            "Epoch 35 has loss 7.2407\n",
            "Epoch 36 has loss 7.2414\n",
            "Epoch 37 has loss 7.212\n",
            "Epoch 38 has loss 7.4345\n",
            "Epoch 39 has loss 7.3006\n",
            "Epoch 40 has loss 7.1908\n",
            "Epoch 41 has loss 7.3491\n",
            "Epoch 42 has loss 7.059\n",
            "Epoch 43 has loss 6.8703\n",
            "Epoch 44 has loss 6.626\n",
            "Epoch 45 has loss 6.6161\n",
            "Epoch 46 has loss 6.1868\n",
            "Epoch 47 has loss 6.1968\n",
            "Epoch 48 has loss 6.0432\n",
            "Epoch 49 has loss 5.8661\n",
            "  6919 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  7984 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  9577 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  10852 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  12186 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (sin GPT):\n",
            "Hits@10: 0.0518\n",
            "Hits@20: 0.0598\n",
            "Hits@30: 0.0717\n",
            "Hits@40: 0.0813\n",
            "Hits@50: 0.0913\n",
            "Accuracy: 0.8109\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_base_v2 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (sin GPT):\")\n",
        "for k, v in results_post_bio_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.9866\n",
            "Epoch 1 has loss 11.784\n",
            "Epoch 2 has loss 11.5214\n",
            "Epoch 3 has loss 10.3969\n",
            "Epoch 4 has loss 10.0786\n",
            "Epoch 5 has loss 9.7017\n",
            "Epoch 6 has loss 9.0015\n",
            "Epoch 7 has loss 8.6268\n",
            "Epoch 8 has loss 8.0943\n",
            "Epoch 9 has loss 7.8493\n",
            "Epoch 10 has loss 7.6586\n",
            "Epoch 11 has loss 7.5579\n",
            "Epoch 12 has loss 7.5006\n",
            "Epoch 13 has loss 7.5016\n",
            "Epoch 14 has loss 7.4748\n",
            "Epoch 15 has loss 7.5635\n",
            "Epoch 16 has loss 7.4521\n",
            "Epoch 17 has loss 7.4652\n",
            "Epoch 18 has loss 6.78\n",
            "Epoch 19 has loss 6.482\n",
            "Epoch 20 has loss 6.3545\n",
            "Epoch 21 has loss 6.2893\n",
            "Epoch 22 has loss 6.1995\n",
            "Epoch 23 has loss 6.198\n",
            "Epoch 24 has loss 5.9243\n",
            "Epoch 25 has loss 5.9845\n",
            "Epoch 26 has loss 5.8734\n",
            "Epoch 27 has loss 5.7931\n",
            "Epoch 28 has loss 5.7846\n",
            "Epoch 29 has loss 5.6862\n",
            "Epoch 30 has loss 5.5398\n",
            "Epoch 31 has loss 5.6704\n",
            "Epoch 32 has loss 5.4601\n",
            "Epoch 33 has loss 5.3799\n",
            "Epoch 34 has loss 5.4489\n",
            "Epoch 35 has loss 5.2819\n",
            "Epoch 36 has loss 5.1536\n",
            "Epoch 37 has loss 5.2463\n",
            "Epoch 38 has loss 5.1227\n",
            "Epoch 39 has loss 5.0656\n",
            "Epoch 40 has loss 5.0432\n",
            "Epoch 41 has loss 4.9739\n",
            "Epoch 42 has loss 4.9947\n",
            "Epoch 43 has loss 4.8966\n",
            "Epoch 44 has loss 4.8693\n",
            "Epoch 45 has loss 4.7422\n",
            "Epoch 46 has loss 4.7452\n",
            "Epoch 47 has loss 4.6779\n",
            "Epoch 48 has loss 4.7444\n",
            "Epoch 49 has loss 4.661\n",
            "  3386 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  5525 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  6629 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  7746 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  9010 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (como input):\n",
            "Hits@10: 0.0254\n",
            "Hits@20: 0.0414\n",
            "Hits@30: 0.0497\n",
            "Hits@40: 0.0580\n",
            "Hits@50: 0.0675\n",
            "Accuracy: 0.8676\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_input_v2 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (como input):\")\n",
        "for k, v in results_post_bio_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.0402\n",
            "Epoch 1 has loss 11.5205\n",
            "Epoch 2 has loss 11.0726\n",
            "Epoch 3 has loss 11.0522\n",
            "Epoch 4 has loss 10.9624\n",
            "Epoch 5 has loss 10.4867\n",
            "Epoch 6 has loss 10.4451\n",
            "Epoch 7 has loss 10.3507\n",
            "Epoch 8 has loss 9.8846\n",
            "Epoch 9 has loss 9.5555\n",
            "Epoch 10 has loss 9.3898\n",
            "Epoch 11 has loss 9.3009\n",
            "Epoch 12 has loss 9.3106\n",
            "Epoch 13 has loss 9.6554\n",
            "Epoch 14 has loss 9.7798\n",
            "Epoch 15 has loss 9.1474\n",
            "Epoch 16 has loss 8.942\n",
            "Epoch 17 has loss 8.8527\n",
            "Epoch 18 has loss 8.9345\n",
            "Epoch 19 has loss 8.9887\n",
            "Epoch 20 has loss 8.9573\n",
            "Epoch 21 has loss 9.0573\n",
            "Epoch 22 has loss 8.8763\n",
            "Epoch 23 has loss 9.0014\n",
            "Epoch 24 has loss 8.7949\n",
            "Epoch 25 has loss 8.8018\n",
            "Epoch 26 has loss 8.7468\n",
            "Epoch 27 has loss 8.6901\n",
            "Epoch 28 has loss 8.5199\n",
            "Epoch 29 has loss 8.573\n",
            "Epoch 30 has loss 8.7126\n",
            "Epoch 31 has loss 8.6155\n",
            "Epoch 32 has loss 8.5363\n",
            "Epoch 33 has loss 8.5174\n",
            "Epoch 34 has loss 8.4486\n",
            "Epoch 35 has loss 8.4813\n",
            "Epoch 36 has loss 8.2753\n",
            "Epoch 37 has loss 8.516\n",
            "Epoch 38 has loss 8.8815\n",
            "Epoch 39 has loss 8.5621\n",
            "Epoch 40 has loss 8.6088\n",
            "Epoch 41 has loss 8.5175\n",
            "Epoch 42 has loss 8.3242\n",
            "Epoch 43 has loss 8.5347\n",
            "Epoch 44 has loss 8.7421\n",
            "Epoch 45 has loss 8.7113\n",
            "Epoch 46 has loss 8.4363\n",
            "Epoch 47 has loss 8.3916\n",
            "Epoch 48 has loss 8.782\n",
            "Epoch 49 has loss 8.515\n",
            "📉 Epoch 0, Loss: 23.9976\n",
            "📉 Epoch 1, Loss: 14.5289\n",
            "📉 Epoch 2, Loss: 13.8206\n",
            "📉 Epoch 3, Loss: 13.2726\n",
            "📉 Epoch 4, Loss: 13.0022\n",
            "📉 Epoch 5, Loss: 12.8682\n",
            "📉 Epoch 6, Loss: 12.8077\n",
            "📉 Epoch 7, Loss: 12.7486\n",
            "📉 Epoch 8, Loss: 12.7071\n",
            "📉 Epoch 9, Loss: 12.6793\n",
            "📉 Epoch 10, Loss: 12.6400\n",
            "📉 Epoch 11, Loss: 12.6164\n",
            "📉 Epoch 12, Loss: 12.6017\n",
            "📉 Epoch 13, Loss: 12.6084\n",
            "📉 Epoch 14, Loss: 12.5931\n",
            "📉 Epoch 15, Loss: 12.5622\n",
            "📉 Epoch 16, Loss: 12.5528\n",
            "📉 Epoch 17, Loss: 12.5481\n",
            "📉 Epoch 18, Loss: 12.5334\n",
            "📉 Epoch 19, Loss: 12.5083\n",
            "📉 Epoch 20, Loss: 12.5090\n",
            "📉 Epoch 21, Loss: 12.4978\n",
            "📉 Epoch 22, Loss: 12.4986\n",
            "📉 Epoch 23, Loss: 12.5071\n",
            "📉 Epoch 24, Loss: 12.5053\n",
            "📉 Epoch 25, Loss: 12.5037\n",
            "📉 Epoch 26, Loss: 12.4730\n",
            "📉 Epoch 27, Loss: 12.4662\n",
            "📉 Epoch 28, Loss: 12.4804\n",
            "📉 Epoch 29, Loss: 12.4563\n",
            "📉 Epoch 30, Loss: 12.4506\n",
            "📉 Epoch 31, Loss: 12.4600\n",
            "📉 Epoch 32, Loss: 12.4517\n",
            "📉 Epoch 33, Loss: 12.4369\n",
            "📉 Epoch 34, Loss: 12.4302\n",
            "📉 Epoch 35, Loss: 12.4506\n",
            "📉 Epoch 36, Loss: 12.4127\n",
            "📉 Epoch 37, Loss: 12.4046\n",
            "📉 Epoch 38, Loss: 12.4123\n",
            "📉 Epoch 39, Loss: 12.4360\n",
            "📉 Epoch 40, Loss: 12.4245\n",
            "📉 Epoch 41, Loss: 12.4166\n",
            "📉 Epoch 42, Loss: 12.4031\n",
            "📉 Epoch 43, Loss: 12.3903\n",
            "📉 Epoch 44, Loss: 12.3834\n",
            "📉 Epoch 45, Loss: 12.3885\n",
            "📉 Epoch 46, Loss: 12.3918\n",
            "📉 Epoch 47, Loss: 12.3939\n",
            "📉 Epoch 48, Loss: 12.3740\n",
            "📉 Epoch 49, Loss: 12.3799\n",
            "  12995 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  17727 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  19938 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  22169 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  23864 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (GPT final):\n",
            "Hits@10: 0.0973\n",
            "Hits@20: 0.1328\n",
            "Hits@30: 0.1494\n",
            "Hits@40: 0.1661\n",
            "Hits@50: 0.1788\n",
            "Accuracy: 0.8496\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_bio_v2 = model(emb, adj_t)\n",
        "\n",
        "predictor_post_bio_final_v2 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_bio_v2.size(1),\n",
        "    hidden_channels=z_post_bio_v2.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_post_bio_final_v2.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_bio_final_v2.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_bio_final_v2.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_bio_final_v2(\n",
        "            z_post_bio_v2[edge[0]], z_post_bio_v2[edge[1]],\n",
        "            embedding_biobert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_post_bio_final_v2, embedding_biobert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_bio_final_v2 = test(dummy_model, wrapped_predictor, z_post_bio_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (GPT final):\")\n",
        "for k, v in results_post_bio_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.144394</td>\n",
              "      <td>0.244927</td>\n",
              "      <td>0.289559</td>\n",
              "      <td>0.306767</td>\n",
              "      <td>0.362846</td>\n",
              "      <td>0.9031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.159069</td>\n",
              "      <td>0.249339</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.292009</td>\n",
              "      <td>0.309688</td>\n",
              "      <td>0.8972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.182367</td>\n",
              "      <td>0.201028</td>\n",
              "      <td>0.225502</td>\n",
              "      <td>0.243181</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.029223</td>\n",
              "      <td>0.7761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.013859</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>0.031456</td>\n",
              "      <td>0.7997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.026317</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>0.041756</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.7884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.8339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099873</td>\n",
              "      <td>0.131928</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.154694</td>\n",
              "      <td>0.166321</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.066095</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.101409</td>\n",
              "      <td>0.110938</td>\n",
              "      <td>0.8477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.177910</td>\n",
              "      <td>0.252201</td>\n",
              "      <td>0.317981</td>\n",
              "      <td>0.347175</td>\n",
              "      <td>0.364524</td>\n",
              "      <td>0.9012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.160380</td>\n",
              "      <td>0.199724</td>\n",
              "      <td>0.228341</td>\n",
              "      <td>0.252583</td>\n",
              "      <td>0.8987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178404</td>\n",
              "      <td>0.202983</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.252553</td>\n",
              "      <td>0.267580</td>\n",
              "      <td>0.8981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003296</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>0.020226</td>\n",
              "      <td>0.7771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.037561</td>\n",
              "      <td>0.077707</td>\n",
              "      <td>0.090674</td>\n",
              "      <td>0.094899</td>\n",
              "      <td>0.099813</td>\n",
              "      <td>0.7046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.8083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.8475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.8310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.039621</td>\n",
              "      <td>0.059188</td>\n",
              "      <td>0.102458</td>\n",
              "      <td>0.112114</td>\n",
              "      <td>0.119261</td>\n",
              "      <td>0.8508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.235488</td>\n",
              "      <td>0.294616</td>\n",
              "      <td>0.335833</td>\n",
              "      <td>0.351714</td>\n",
              "      <td>0.374158</td>\n",
              "      <td>0.9025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178651</td>\n",
              "      <td>0.212827</td>\n",
              "      <td>0.251968</td>\n",
              "      <td>0.290458</td>\n",
              "      <td>0.304879</td>\n",
              "      <td>0.9051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.036400</td>\n",
              "      <td>0.041794</td>\n",
              "      <td>0.045899</td>\n",
              "      <td>0.052843</td>\n",
              "      <td>0.057398</td>\n",
              "      <td>0.8582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.7365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008375</td>\n",
              "      <td>0.010967</td>\n",
              "      <td>0.012720</td>\n",
              "      <td>0.015672</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.7638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.005042</td>\n",
              "      <td>0.005956</td>\n",
              "      <td>0.008218</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.7927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.051832</td>\n",
              "      <td>0.059810</td>\n",
              "      <td>0.071744</td>\n",
              "      <td>0.081295</td>\n",
              "      <td>0.091288</td>\n",
              "      <td>0.8109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.025365</td>\n",
              "      <td>0.041389</td>\n",
              "      <td>0.049660</td>\n",
              "      <td>0.058027</td>\n",
              "      <td>0.067496</td>\n",
              "      <td>0.8676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.097349</td>\n",
              "      <td>0.132797</td>\n",
              "      <td>0.149361</td>\n",
              "      <td>0.166074</td>\n",
              "      <td>0.178771</td>\n",
              "      <td>0.8496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source                Prompt Type  \\\n",
              "0          SAGEConv            classic                          -   \n",
              "1          SAGEConv      classic + GPT    reasoning-based summary   \n",
              "2          SAGEConv            classic    reasoning-based summary   \n",
              "3      SkipConnSAGE            classic                          -   \n",
              "4      SkipConnSAGE      classic + GPT    reasoning-based summary   \n",
              "5      SkipConnSAGE            classic    reasoning-based summary   \n",
              "6   PostProcessSAGE            classic                          -   \n",
              "7   PostProcessSAGE      classic + GPT    reasoning-based summary   \n",
              "8   PostProcessSAGE            classic    reasoning-based summary   \n",
              "9          SAGEConv           classic                           -   \n",
              "10         SAGEConv  classic + BioBERT    reasoning-based summary   \n",
              "11         SAGEConv            classic    reasoning-based summary   \n",
              "12     SkipConnSAGE            classic                          -   \n",
              "13     SkipConnSAGE  classic + BioBERT    reasoning-based summary   \n",
              "14     SkipConnSAGE            classic    reasoning-based summary   \n",
              "15  PostProcessSAGE            classic                          -   \n",
              "16  PostProcessSAGE  classic + BioBERT    reasoning-based summary   \n",
              "17  PostProcessSAGE            classic    reasoning-based summary   \n",
              "18         SAGEConv            classic                          -   \n",
              "19         SAGEConv  classic + SciBERT    reasoning-based summary   \n",
              "20         SAGEConv            classic    reasoning-based summary   \n",
              "21     SkipConnSAGE            classic                          -   \n",
              "22     SkipConnSAGE  classic + SciBERT    reasoning-based summary   \n",
              "23     SkipConnSAGE            classic    reasoning-based summary   \n",
              "24  PostProcessSAGE            classic                          -   \n",
              "25  PostProcessSAGE  classic + SciBERT    reasoning-based summary   \n",
              "26  PostProcessSAGE            classic    reasoning-based summary   \n",
              "27         SAGEConv            classic                          -   \n",
              "28         SAGEConv     classic + BERT  explicit structural focus   \n",
              "29         SAGEConv            classic  explicit structural focus   \n",
              "30     SkipConnSAGE            classic                          -   \n",
              "31     SkipConnSAGE     classic + BERT  explicit structural focus   \n",
              "32     SkipConnSAGE            classic  explicit structural focus   \n",
              "33  PostProcessSAGE            classic                          -   \n",
              "34  PostProcessSAGE     classic + BERT  explicit structural focus   \n",
              "35  PostProcessSAGE            classic  explicit structural focus   \n",
              "36         SAGEConv            classic                          -   \n",
              "37         SAGEConv  classic + BioBERT  explicit structural focus   \n",
              "38         SAGEConv            classic  explicit structural focus   \n",
              "39     SkipConnSAGE            classic                          -   \n",
              "40     SkipConnSAGE  classic + BioBERT  explicit structural focus   \n",
              "41     SkipConnSAGE            classic  explicit structural focus   \n",
              "42  PostProcessSAGE            classic                          -   \n",
              "43  PostProcessSAGE  classic + BioBERT  explicit structural focus   \n",
              "44  PostProcessSAGE            classic  explicit structural focus   \n",
              "\n",
              "   GPT Injection         Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -            Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input            Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final      Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -            Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input            Neural     100  0.000809  0.001049  0.001288   \n",
              "5          final      Neural + GPT     100  0.003274  0.007911  0.012196   \n",
              "6              -            Neural      50  0.005206  0.210482  0.265707   \n",
              "7          input            Neural      50  0.007334  0.007334  0.132925   \n",
              "8          final      Neural + GPT      50  0.038385  0.048438  0.053443   \n",
              "9              -            Neural      50  0.112204  0.156620  0.191394   \n",
              "10         input            Neural      50  0.285020  0.370045  0.423773   \n",
              "11         final  Neural + BioBERT      50  0.054461  0.084194  0.098555   \n",
              "12             -            Neural      50  0.017919  0.031164  0.037546   \n",
              "13         input            Neural      50  0.000277  0.000622  0.001079   \n",
              "14         final  Neural + BioBERT      50  0.015762  0.019829  0.028309   \n",
              "15             -            Neural      50  0.010203  0.010203  0.010203   \n",
              "16         input            Neural      50  0.003813  0.003813  0.003813   \n",
              "17         final  Neural + BioBERT      50  0.071324  0.083932  0.100870   \n",
              "18             -            Neural      50  0.144394  0.244927  0.289559   \n",
              "19         input            Neural      50  0.159069  0.249339  0.278809   \n",
              "20         final  Neural + SciBERT      50  0.078329  0.182367  0.201028   \n",
              "21             -            Neural      50  0.008188  0.016661  0.019335   \n",
              "22         input            Neural      50  0.006270  0.013859  0.019290   \n",
              "23         final  Neural + SciBERT      50  0.026317  0.033389  0.041756   \n",
              "24             -            Neural      50  0.000562  0.000562  0.000562   \n",
              "25         input            Neural      50  0.099873  0.131928  0.144664   \n",
              "26         final  Neural + SciBERT      50  0.066095  0.080838  0.090659   \n",
              "27             -            Neural      50  0.177910  0.252201  0.317981   \n",
              "28         input            Neural      50  0.121800  0.160380  0.199724   \n",
              "29         final     Neural + BERT      50  0.178404  0.202983  0.235135   \n",
              "30             -            Neural      50  0.000015  0.003296  0.014518   \n",
              "31         input            Neural      50  0.037561  0.077707  0.090674   \n",
              "32         final     Neural + BERT      50  0.000165  0.000524  0.000914   \n",
              "33             -            Neural      50  0.000367  0.000367  0.000367   \n",
              "34         input            Neural      50  0.005439  0.005439  0.005439   \n",
              "35         final     Neural + BERT      50  0.039621  0.059188  0.102458   \n",
              "36             -            Neural      50  0.235488  0.294616  0.335833   \n",
              "37         input            Neural      50  0.178651  0.212827  0.251968   \n",
              "38         final  Neural + BioBERT      50  0.036400  0.041794  0.045899   \n",
              "39             -            Neural      50  0.000015  0.000037  0.000067   \n",
              "40         input            Neural      50  0.008375  0.010967  0.012720   \n",
              "41         final  Neural + BioBERT      50  0.002375  0.005042  0.005956   \n",
              "42             -            Neural      50  0.051832  0.059810  0.071744   \n",
              "43         input            Neural      50  0.025365  0.041389  0.049660   \n",
              "44         final  Neural + BioBERT      50  0.097349  0.132797  0.149361   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "5   0.014923  0.017664    0.7774  \n",
              "6   0.293904  0.322446    0.8781  \n",
              "7   0.153601  0.181296    0.8706  \n",
              "8   0.061211  0.068358    0.8070  \n",
              "9   0.222917  0.243204    0.9000  \n",
              "10  0.445228  0.485149    0.9281  \n",
              "11  0.116526  0.126265    0.8887  \n",
              "12  0.045494  0.050761    0.7755  \n",
              "13  0.001671  0.002682    0.7881  \n",
              "14  0.033471  0.037501    0.8014  \n",
              "15  0.010203  0.180517    0.8596  \n",
              "16  0.003813  0.003813    0.8574  \n",
              "17  0.115440  0.124819    0.8507  \n",
              "18  0.306767  0.362846    0.9031  \n",
              "19  0.292009  0.309688    0.8972  \n",
              "20  0.225502  0.243181    0.8985  \n",
              "21  0.025343  0.029223    0.7761  \n",
              "22  0.025395  0.031456    0.7997  \n",
              "23  0.049000  0.057698    0.7884  \n",
              "24  0.000562  0.000562    0.8339  \n",
              "25  0.154694  0.166321    0.8644  \n",
              "26  0.101409  0.110938    0.8477  \n",
              "27  0.347175  0.364524    0.9012  \n",
              "28  0.228341  0.252583    0.8987  \n",
              "29  0.252553  0.267580    0.8981  \n",
              "30  0.016676  0.020226    0.7771  \n",
              "31  0.094899  0.099813    0.7046  \n",
              "32  0.001618  0.002614    0.8083  \n",
              "33  0.000367  0.000367    0.8475  \n",
              "34  0.005439  0.005439    0.8310  \n",
              "35  0.112114  0.119261    0.8508  \n",
              "36  0.351714  0.374158    0.9025  \n",
              "37  0.290458  0.304879    0.9051  \n",
              "38  0.052843  0.057398    0.8582  \n",
              "39  0.000105  0.000127    0.7365  \n",
              "40  0.015672  0.019290    0.7638  \n",
              "41  0.008218  0.010742    0.7927  \n",
              "42  0.081295  0.091288    0.8109  \n",
              "43  0.058027  0.067496    0.8676  \n",
              "44  0.166074  0.178771    0.8496  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "new_rows = [\n",
        "    # SAGEConv + BioBERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_bio_base_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_bio_input_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_sage_bio_final_v2},\n",
        "\n",
        "    # SkipConnSAGE + BioBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_base_v2},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_input_v2},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_skipconn_bio_final_v2},\n",
        "\n",
        "    # PostProcessSAGE + BioBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_base_v2},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_input_v2},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_post_bio_final_v2},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Usamos el mismo modelo que en la primera iteración para SciBERT\n",
        "model_sci = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "embedding_scibert_v2 = model_sci.encode(gpt_response_text_v2)\n",
        "\n",
        "embedding_scibert_tensor_v2 = torch.tensor(embedding_scibert_v2, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5137\n",
            "Epoch 1 has loss 9.3594\n",
            "Epoch 2 has loss 8.5281\n",
            "Epoch 3 has loss 7.8336\n",
            "Epoch 4 has loss 7.4591\n",
            "Epoch 5 has loss 7.3369\n",
            "Epoch 6 has loss 7.2692\n",
            "Epoch 7 has loss 7.3721\n",
            "Epoch 8 has loss 7.18\n",
            "Epoch 9 has loss 7.0333\n",
            "Epoch 10 has loss 6.9283\n",
            "Epoch 11 has loss 6.5504\n",
            "Epoch 12 has loss 6.3238\n",
            "Epoch 13 has loss 6.1861\n",
            "Epoch 14 has loss 5.9972\n",
            "Epoch 15 has loss 5.8245\n",
            "Epoch 16 has loss 5.5918\n",
            "Epoch 17 has loss 5.6064\n",
            "Epoch 18 has loss 5.6387\n",
            "Epoch 19 has loss 5.5774\n",
            "Epoch 20 has loss 5.4682\n",
            "Epoch 21 has loss 5.2511\n",
            "Epoch 22 has loss 5.1148\n",
            "Epoch 23 has loss 5.0225\n",
            "Epoch 24 has loss 4.931\n",
            "Epoch 25 has loss 4.8485\n",
            "Epoch 26 has loss 4.8932\n",
            "Epoch 27 has loss 4.9039\n",
            "Epoch 28 has loss 4.804\n",
            "Epoch 29 has loss 4.7361\n",
            "Epoch 30 has loss 4.6841\n",
            "Epoch 31 has loss 4.5714\n",
            "Epoch 32 has loss 4.5102\n",
            "Epoch 33 has loss 4.5831\n",
            "Epoch 34 has loss 4.5741\n",
            "Epoch 35 has loss 4.4106\n",
            "Epoch 36 has loss 4.4647\n",
            "Epoch 37 has loss 4.3594\n",
            "Epoch 38 has loss 4.3167\n",
            "Epoch 39 has loss 4.273\n",
            "Epoch 40 has loss 4.3522\n",
            "Epoch 41 has loss 4.2453\n",
            "Epoch 42 has loss 4.1778\n",
            "Epoch 43 has loss 4.1023\n",
            "Epoch 44 has loss 4.1881\n",
            "Epoch 45 has loss 4.0549\n",
            "Epoch 46 has loss 3.9901\n",
            "Epoch 47 has loss 3.9703\n",
            "Epoch 48 has loss 3.9255\n",
            "Epoch 49 has loss 3.9514\n",
            "  32355 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  41324 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  46241 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  49655 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  51415 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (sin GPT):\n",
            "Hits@10: 0.2424\n",
            "Hits@20: 0.3096\n",
            "Hits@30: 0.3464\n",
            "Hits@40: 0.3720\n",
            "Hits@50: 0.3852\n",
            "Accuracy: 0.9073\n"
          ]
        }
      ],
      "source": [
        "model_sci_base = SAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=3,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_sci_base.reset_parameters()\n",
        "predictor_sci_base.reset_parameters()\n",
        "optimizer_sci_base = torch.optim.Adam(\n",
        "    list(model_sci_base.parameters()) + list(predictor_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_sci_base, predictor_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer_sci_base, 64 * 1024, 50)\n",
        "\n",
        "results_sci_base_v2 = test(model_sci_base, predictor_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (sin GPT):\")\n",
        "for k, v in results_sci_base_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7326\n",
            "Epoch 1 has loss 10.5212\n",
            "Epoch 2 has loss 9.2405\n",
            "Epoch 3 has loss 8.3597\n",
            "Epoch 4 has loss 8.0207\n",
            "Epoch 5 has loss 7.634\n",
            "Epoch 6 has loss 7.5215\n",
            "Epoch 7 has loss 7.4061\n",
            "Epoch 8 has loss 7.2763\n",
            "Epoch 9 has loss 7.2855\n",
            "Epoch 10 has loss 6.6877\n",
            "Epoch 11 has loss 6.3539\n",
            "Epoch 12 has loss 6.3905\n",
            "Epoch 13 has loss 6.0089\n",
            "Epoch 14 has loss 6.0576\n",
            "Epoch 15 has loss 5.7629\n",
            "Epoch 16 has loss 5.6254\n",
            "Epoch 17 has loss 5.5364\n",
            "Epoch 18 has loss 5.548\n",
            "Epoch 19 has loss 5.4686\n",
            "Epoch 20 has loss 5.4267\n",
            "Epoch 21 has loss 5.3198\n",
            "Epoch 22 has loss 5.2415\n",
            "Epoch 23 has loss 5.2179\n",
            "Epoch 24 has loss 5.1275\n",
            "Epoch 25 has loss 5.1768\n",
            "Epoch 26 has loss 5.1999\n",
            "Epoch 27 has loss 5.1975\n",
            "Epoch 28 has loss 5.1504\n",
            "Epoch 29 has loss 5.1112\n",
            "Epoch 30 has loss 5.0123\n",
            "Epoch 31 has loss 4.9946\n",
            "Epoch 32 has loss 4.987\n",
            "Epoch 33 has loss 4.9627\n",
            "Epoch 34 has loss 4.8889\n",
            "Epoch 35 has loss 4.9953\n",
            "Epoch 36 has loss 4.79\n",
            "Epoch 37 has loss 4.8366\n",
            "Epoch 38 has loss 4.7077\n",
            "Epoch 39 has loss 4.7031\n",
            "Epoch 40 has loss 4.4972\n",
            "Epoch 41 has loss 4.4639\n",
            "Epoch 42 has loss 4.4489\n",
            "Epoch 43 has loss 4.5405\n",
            "Epoch 44 has loss 4.499\n",
            "Epoch 45 has loss 4.3621\n",
            "Epoch 46 has loss 4.451\n",
            "Epoch 47 has loss 4.3017\n",
            "Epoch 48 has loss 4.2116\n",
            "Epoch 49 has loss 4.206\n",
            "  13249 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  19408 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  23650 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  28559 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  32196 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (como input):\n",
            "Hits@10: 0.0993\n",
            "Hits@20: 0.1454\n",
            "Hits@30: 0.1772\n",
            "Hits@40: 0.2139\n",
            "Hits@50: 0.2412\n",
            "Accuracy: 0.9026\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci_v2 = torch.cat([\n",
        "    emb,  \n",
        "    embedding_scibert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model_sci_input = SAGE(\n",
        "    in_channels=aug_emb_sci_v2.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_sci_input.reset_parameters()\n",
        "predictor_sci_input.reset_parameters()\n",
        "optimizer_sci_input = torch.optim.Adam(\n",
        "    list(model_sci_input.parameters()) + list(predictor_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_sci_input, predictor_sci_input, aug_emb_sci_v2, adj_t, split_edge, torch.nn.BCELoss(), optimizer_sci_input, 64 * 1024, 50)\n",
        "\n",
        "results_sci_input_v2 = test(model_sci_input, predictor_sci_input, aug_emb_sci_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (como input):\")\n",
        "for k, v in results_sci_input_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5259\n",
            "Epoch 1 has loss 10.2604\n",
            "Epoch 2 has loss 10.2192\n",
            "Epoch 3 has loss 10.1194\n",
            "Epoch 4 has loss 10.0236\n",
            "Epoch 5 has loss 9.9787\n",
            "Epoch 6 has loss 9.8711\n",
            "Epoch 7 has loss 9.754\n",
            "Epoch 8 has loss 9.8979\n",
            "Epoch 9 has loss 9.7589\n",
            "Epoch 10 has loss 9.6737\n",
            "Epoch 11 has loss 9.6416\n",
            "Epoch 12 has loss 9.603\n",
            "Epoch 13 has loss 9.6149\n",
            "Epoch 14 has loss 9.5574\n",
            "Epoch 15 has loss 9.5982\n",
            "Epoch 16 has loss 9.5564\n",
            "Epoch 17 has loss 9.524\n",
            "Epoch 18 has loss 9.5183\n",
            "Epoch 19 has loss 9.5009\n",
            "Epoch 20 has loss 9.4964\n",
            "Epoch 21 has loss 9.4994\n",
            "Epoch 22 has loss 9.5099\n",
            "Epoch 23 has loss 9.4916\n",
            "Epoch 24 has loss 9.4815\n",
            "Epoch 25 has loss 9.4749\n",
            "Epoch 26 has loss 9.4707\n",
            "Epoch 27 has loss 9.4931\n",
            "Epoch 28 has loss 9.4662\n",
            "Epoch 29 has loss 9.4687\n",
            "Epoch 30 has loss 9.4727\n",
            "Epoch 31 has loss 9.4511\n",
            "Epoch 32 has loss 9.4649\n",
            "Epoch 33 has loss 9.4716\n",
            "Epoch 34 has loss 9.446\n",
            "Epoch 35 has loss 9.4441\n",
            "Epoch 36 has loss 9.444\n",
            "Epoch 37 has loss 9.4459\n",
            "Epoch 38 has loss 9.4349\n",
            "Epoch 39 has loss 9.4439\n",
            "Epoch 40 has loss 9.4304\n",
            "Epoch 41 has loss 9.4528\n",
            "Epoch 42 has loss 9.4308\n",
            "Epoch 43 has loss 9.428\n",
            "Epoch 44 has loss 9.429\n",
            "Epoch 45 has loss 9.4392\n",
            "Epoch 46 has loss 9.422\n",
            "Epoch 47 has loss 9.4081\n",
            "Epoch 48 has loss 9.409\n",
            "Epoch 49 has loss 9.4015\n",
            "📉 Epoch 0, Loss: 23.1662\n",
            "📉 Epoch 1, Loss: 15.9606\n",
            "📉 Epoch 2, Loss: 14.1644\n",
            "📉 Epoch 3, Loss: 13.5313\n",
            "📉 Epoch 4, Loss: 13.0893\n",
            "📉 Epoch 5, Loss: 12.9346\n",
            "📉 Epoch 6, Loss: 12.7226\n",
            "📉 Epoch 7, Loss: 12.5698\n",
            "📉 Epoch 8, Loss: 12.5432\n",
            "📉 Epoch 9, Loss: 12.7149\n",
            "📉 Epoch 10, Loss: 12.5189\n",
            "📉 Epoch 11, Loss: 12.4102\n",
            "📉 Epoch 12, Loss: 12.3985\n",
            "📉 Epoch 13, Loss: 12.3827\n",
            "📉 Epoch 14, Loss: 12.4067\n",
            "📉 Epoch 15, Loss: 12.3245\n",
            "📉 Epoch 16, Loss: 12.2792\n",
            "📉 Epoch 17, Loss: 12.3518\n",
            "📉 Epoch 18, Loss: 12.4160\n",
            "📉 Epoch 19, Loss: 12.2501\n",
            "📉 Epoch 20, Loss: 12.2019\n",
            "📉 Epoch 21, Loss: 12.1603\n",
            "📉 Epoch 22, Loss: 12.1647\n",
            "📉 Epoch 23, Loss: 12.1471\n",
            "📉 Epoch 24, Loss: 12.1483\n",
            "📉 Epoch 25, Loss: 12.2285\n",
            "📉 Epoch 26, Loss: 12.3641\n",
            "📉 Epoch 27, Loss: 12.2374\n",
            "📉 Epoch 28, Loss: 12.1530\n",
            "📉 Epoch 29, Loss: 12.1208\n",
            "📉 Epoch 30, Loss: 12.1498\n",
            "📉 Epoch 31, Loss: 12.0979\n",
            "📉 Epoch 32, Loss: 12.1207\n",
            "📉 Epoch 33, Loss: 12.1590\n",
            "📉 Epoch 34, Loss: 12.0215\n",
            "📉 Epoch 35, Loss: 12.0225\n",
            "📉 Epoch 36, Loss: 11.9985\n",
            "📉 Epoch 37, Loss: 12.0131\n",
            "📉 Epoch 38, Loss: 12.0111\n",
            "📉 Epoch 39, Loss: 12.0281\n",
            "📉 Epoch 40, Loss: 11.9866\n",
            "📉 Epoch 41, Loss: 11.9658\n",
            "📉 Epoch 42, Loss: 11.9737\n",
            "📉 Epoch 43, Loss: 11.9776\n",
            "📉 Epoch 44, Loss: 11.9789\n",
            "📉 Epoch 45, Loss: 11.9406\n",
            "📉 Epoch 46, Loss: 11.9738\n",
            "📉 Epoch 47, Loss: 11.9471\n",
            "📉 Epoch 48, Loss: 11.9264\n",
            "📉 Epoch 49, Loss: 11.9190\n",
            "  4519 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4939 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5556 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  6022 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  6527 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (GPT final):\n",
            "Hits@10: 0.0339\n",
            "Hits@20: 0.0370\n",
            "Hits@30: 0.0416\n",
            "Hits@40: 0.0451\n",
            "Hits@50: 0.0489\n",
            "Accuracy: 0.8659\n"
          ]
        }
      ],
      "source": [
        "model_sci_final = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_sci_final = model_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_sci_final.size(1),\n",
        "    hidden_channels=z_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_sci_final.reset_parameters()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(predictor_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_sci_final.train()  \n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_sci_final(\n",
        "            z_sci_final[edge[0]], z_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_sci_final, embedding_scibert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sci_final_v2 = test(dummy_model, wrapped_predictor, z_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (GPT final):\")\n",
        "for k, v in results_sci_final_v2.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.3307\n",
            "Epoch 1 has loss 11.0361\n",
            "Epoch 2 has loss 10.4995\n",
            "Epoch 3 has loss 10.0409\n",
            "Epoch 4 has loss 10.0969\n",
            "Epoch 5 has loss 9.9179\n",
            "Epoch 6 has loss 10.0889\n",
            "Epoch 7 has loss 9.986\n",
            "Epoch 8 has loss 9.8589\n",
            "Epoch 9 has loss 10.1138\n",
            "Epoch 10 has loss 10.0221\n",
            "Epoch 11 has loss 9.8457\n",
            "Epoch 12 has loss 9.5974\n",
            "Epoch 13 has loss 9.2274\n",
            "Epoch 14 has loss 9.3787\n",
            "Epoch 15 has loss 8.8769\n",
            "Epoch 16 has loss 9.1728\n",
            "Epoch 17 has loss 8.7839\n",
            "Epoch 18 has loss 8.5294\n",
            "Epoch 19 has loss 8.8349\n",
            "Epoch 20 has loss 9.2619\n",
            "Epoch 21 has loss 8.9334\n",
            "Epoch 22 has loss 8.5288\n",
            "Epoch 23 has loss 8.5068\n",
            "Epoch 24 has loss 8.5836\n",
            "Epoch 25 has loss 8.3162\n",
            "Epoch 26 has loss 8.7394\n",
            "Epoch 27 has loss 8.5863\n",
            "Epoch 28 has loss 8.2609\n",
            "Epoch 29 has loss 8.1957\n",
            "Epoch 30 has loss 8.3181\n",
            "Epoch 31 has loss 7.8459\n",
            "Epoch 32 has loss 7.8623\n",
            "Epoch 33 has loss 8.2244\n",
            "Epoch 34 has loss 8.0653\n",
            "Epoch 35 has loss 7.8845\n",
            "Epoch 36 has loss 7.8207\n",
            "Epoch 37 has loss 7.8347\n",
            "Epoch 38 has loss 7.7006\n",
            "Epoch 39 has loss 7.8222\n",
            "Epoch 40 has loss 7.7832\n",
            "Epoch 41 has loss 7.9975\n",
            "Epoch 42 has loss 8.104\n",
            "Epoch 43 has loss 7.8403\n",
            "Epoch 44 has loss 7.6602\n",
            "Epoch 45 has loss 7.6462\n",
            "Epoch 46 has loss 7.8961\n",
            "Epoch 47 has loss 7.7927\n",
            "Epoch 48 has loss 7.7786\n",
            "Epoch 49 has loss 7.5583\n",
            "  768 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2660 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  3912 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  4447 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  5184 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (sin GPT):\n",
            "Hits@10: 0.0058\n",
            "Hits@20: 0.0199\n",
            "Hits@30: 0.0293\n",
            "Hits@40: 0.0333\n",
            "Hits@50: 0.0388\n",
            "Accuracy: 0.7828\n"
          ]
        }
      ],
      "source": [
        "model_skip_sci_base = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_skip_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_skip_sci_base.reset_parameters()\n",
        "predictor_skip_sci_base.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_skip_sci_base.parameters()) + list(predictor_skip_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_skip_sci_base, predictor_skip_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_sci_base = test(model_skip_sci_base, predictor_skip_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (sin GPT):\")\n",
        "for k, v in results_skip_sci_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4155\n",
            "Epoch 1 has loss 10.2346\n",
            "Epoch 2 has loss 10.7326\n",
            "Epoch 3 has loss 10.0449\n",
            "Epoch 4 has loss 9.9782\n",
            "Epoch 5 has loss 9.6629\n",
            "Epoch 6 has loss 9.4919\n",
            "Epoch 7 has loss 10.0674\n",
            "Epoch 8 has loss 9.939\n",
            "Epoch 9 has loss 9.6969\n",
            "Epoch 10 has loss 9.3401\n",
            "Epoch 11 has loss 9.1587\n",
            "Epoch 12 has loss 8.9924\n",
            "Epoch 13 has loss 8.7889\n",
            "Epoch 14 has loss 8.8377\n",
            "Epoch 15 has loss 8.6438\n",
            "Epoch 16 has loss 8.8999\n",
            "Epoch 17 has loss 8.5529\n",
            "Epoch 18 has loss 8.6389\n",
            "Epoch 19 has loss 8.4247\n",
            "Epoch 20 has loss 8.3958\n",
            "Epoch 21 has loss 8.4588\n",
            "Epoch 22 has loss 8.127\n",
            "Epoch 23 has loss 9.0358\n",
            "Epoch 24 has loss 8.2871\n",
            "Epoch 25 has loss 8.2791\n",
            "Epoch 26 has loss 8.3993\n",
            "Epoch 27 has loss 7.9595\n",
            "Epoch 28 has loss 7.8929\n",
            "Epoch 29 has loss 7.799\n",
            "Epoch 30 has loss 8.3069\n",
            "Epoch 31 has loss 7.8915\n",
            "Epoch 32 has loss 7.7961\n",
            "Epoch 33 has loss 7.284\n",
            "Epoch 34 has loss 7.4829\n",
            "Epoch 35 has loss 6.9839\n",
            "Epoch 36 has loss 7.3122\n",
            "Epoch 37 has loss 7.3192\n",
            "Epoch 38 has loss 6.8257\n",
            "Epoch 39 has loss 6.798\n",
            "Epoch 40 has loss 6.558\n",
            "Epoch 41 has loss 6.6138\n",
            "Epoch 42 has loss 6.4485\n",
            "Epoch 43 has loss 6.2857\n",
            "Epoch 44 has loss 6.0038\n",
            "Epoch 45 has loss 5.8621\n",
            "Epoch 46 has loss 5.8156\n",
            "Epoch 47 has loss 5.6924\n",
            "Epoch 48 has loss 5.7802\n",
            "Epoch 49 has loss 5.6946\n",
            "  13628 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  15916 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  17105 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  18303 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  19296 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (GPT como input):\n",
            "Hits@10: 0.1021\n",
            "Hits@20: 0.1192\n",
            "Hits@30: 0.1281\n",
            "Hits@40: 0.1371\n",
            "Hits@50: 0.1446\n",
            "Accuracy: 0.8120\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci_v2 = torch.cat([emb, embedding_scibert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model_skip_sci_input = SkipConnSAGE(\n",
        "    in_channels=aug_emb_sci_v2.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_skip_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_skip_sci_input.reset_parameters()\n",
        "predictor_skip_sci_input.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_skip_sci_input.parameters()) + list(predictor_skip_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_skip_sci_input, predictor_skip_sci_input, aug_emb_sci_v2, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_sci_input = test(model_skip_sci_input, predictor_skip_sci_input, aug_emb_sci_v2, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (GPT como input):\")\n",
        "for k, v in results_skip_sci_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.0788\n",
            "Epoch 1 has loss 10.6518\n",
            "Epoch 2 has loss 10.4319\n",
            "Epoch 3 has loss 10.3505\n",
            "Epoch 4 has loss 10.3942\n",
            "Epoch 5 has loss 10.3463\n",
            "Epoch 6 has loss 10.2936\n",
            "Epoch 7 has loss 10.2494\n",
            "Epoch 8 has loss 10.2332\n",
            "Epoch 9 has loss 10.1902\n",
            "Epoch 10 has loss 10.2103\n",
            "Epoch 11 has loss 10.1423\n",
            "Epoch 12 has loss 10.1542\n",
            "Epoch 13 has loss 10.1649\n",
            "Epoch 14 has loss 10.169\n",
            "Epoch 15 has loss 10.1457\n",
            "Epoch 16 has loss 10.0736\n",
            "Epoch 17 has loss 10.0668\n",
            "Epoch 18 has loss 10.0648\n",
            "Epoch 19 has loss 10.089\n",
            "Epoch 20 has loss 10.0821\n",
            "Epoch 21 has loss 10.1736\n",
            "Epoch 22 has loss 10.1226\n",
            "Epoch 23 has loss 10.0698\n",
            "Epoch 24 has loss 10.076\n",
            "Epoch 25 has loss 10.0864\n",
            "Epoch 26 has loss 10.0529\n",
            "Epoch 27 has loss 10.045\n",
            "Epoch 28 has loss 10.083\n",
            "Epoch 29 has loss 10.0095\n",
            "Epoch 30 has loss 9.998\n",
            "Epoch 31 has loss 10.0143\n",
            "Epoch 32 has loss 10.0004\n",
            "Epoch 33 has loss 10.0171\n",
            "Epoch 34 has loss 9.9857\n",
            "Epoch 35 has loss 9.9726\n",
            "Epoch 36 has loss 9.966\n",
            "Epoch 37 has loss 9.9711\n",
            "Epoch 38 has loss 9.9094\n",
            "Epoch 39 has loss 10.0077\n",
            "Epoch 40 has loss 9.9203\n",
            "Epoch 41 has loss 9.9179\n",
            "Epoch 42 has loss 9.919\n",
            "Epoch 43 has loss 9.8859\n",
            "Epoch 44 has loss 9.9685\n",
            "Epoch 45 has loss 9.8853\n",
            "Epoch 46 has loss 9.8944\n",
            "Epoch 47 has loss 9.9258\n",
            "Epoch 48 has loss 9.8924\n",
            "Epoch 49 has loss 9.8611\n",
            "📉 Epoch 0, Loss: 24.5279\n",
            "📉 Epoch 1, Loss: 18.1837\n",
            "📉 Epoch 2, Loss: 17.1920\n",
            "📉 Epoch 3, Loss: 16.6490\n",
            "📉 Epoch 4, Loss: 16.5779\n",
            "📉 Epoch 5, Loss: 17.0024\n",
            "📉 Epoch 6, Loss: 16.4800\n",
            "📉 Epoch 7, Loss: 16.3571\n",
            "📉 Epoch 8, Loss: 16.3423\n",
            "📉 Epoch 9, Loss: 16.3372\n",
            "📉 Epoch 10, Loss: 16.2931\n",
            "📉 Epoch 11, Loss: 16.2934\n",
            "📉 Epoch 12, Loss: 16.2646\n",
            "📉 Epoch 13, Loss: 16.7473\n",
            "📉 Epoch 14, Loss: 16.2734\n",
            "📉 Epoch 15, Loss: 16.5763\n",
            "📉 Epoch 16, Loss: 16.3787\n",
            "📉 Epoch 17, Loss: 16.8423\n",
            "📉 Epoch 18, Loss: 16.5122\n",
            "📉 Epoch 19, Loss: 16.2282\n",
            "📉 Epoch 20, Loss: 16.2703\n",
            "📉 Epoch 21, Loss: 16.3408\n",
            "📉 Epoch 22, Loss: 16.2597\n",
            "📉 Epoch 23, Loss: 16.2132\n",
            "📉 Epoch 24, Loss: 16.2255\n",
            "📉 Epoch 25, Loss: 16.2452\n",
            "📉 Epoch 26, Loss: 16.2258\n",
            "📉 Epoch 27, Loss: 16.2167\n",
            "📉 Epoch 28, Loss: 16.1980\n",
            "📉 Epoch 29, Loss: 16.3173\n",
            "📉 Epoch 30, Loss: 16.3558\n",
            "📉 Epoch 31, Loss: 16.7606\n",
            "📉 Epoch 32, Loss: 16.3631\n",
            "📉 Epoch 33, Loss: 16.2788\n",
            "📉 Epoch 34, Loss: 16.2278\n",
            "📉 Epoch 35, Loss: 16.2714\n",
            "📉 Epoch 36, Loss: 16.3218\n",
            "📉 Epoch 37, Loss: 16.4367\n",
            "📉 Epoch 38, Loss: 17.0133\n",
            "📉 Epoch 39, Loss: 16.4379\n",
            "📉 Epoch 40, Loss: 16.3364\n",
            "📉 Epoch 41, Loss: 16.4302\n",
            "📉 Epoch 42, Loss: 16.3869\n",
            "📉 Epoch 43, Loss: 16.3636\n",
            "📉 Epoch 44, Loss: 16.3536\n",
            "📉 Epoch 45, Loss: 16.4092\n",
            "📉 Epoch 46, Loss: 16.6091\n",
            "📉 Epoch 47, Loss: 16.4885\n",
            "📉 Epoch 48, Loss: 16.3496\n",
            "📉 Epoch 49, Loss: 16.5358\n",
            "  100 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  123 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  154 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  172 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  194 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (GPT como final):\n",
            "Hits@10: 0.0007\n",
            "Hits@20: 0.0009\n",
            "Hits@30: 0.0012\n",
            "Hits@40: 0.0013\n",
            "Hits@50: 0.0015\n",
            "Accuracy: 0.7849\n"
          ]
        }
      ],
      "source": [
        "model_skip_sci_final = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_skip_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_skip_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_skip_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_skip_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_sci_final = model_skip_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_skip_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_sci_final.size(1),\n",
        "    hidden_channels=z_skip_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_skip_sci_final.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(predictor_skip_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skip_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skip_sci_final(\n",
        "            z_skip_sci_final[edge[0]], z_skip_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_skip_sci = WrappedPredictor(predictor_skip_sci_final, embedding_scibert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skip_sci_final = test(dummy_model, wrapped_predictor_skip_sci, z_skip_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (GPT como final):\")\n",
        "for k, v in results_skip_sci_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.856\n",
            "Epoch 1 has loss 11.8474\n",
            "Epoch 2 has loss 11.7918\n",
            "Epoch 3 has loss 11.5791\n",
            "Epoch 4 has loss 10.9487\n",
            "Epoch 5 has loss 10.4406\n",
            "Epoch 6 has loss 10.1643\n",
            "Epoch 7 has loss 9.4288\n",
            "Epoch 8 has loss 9.0934\n",
            "Epoch 9 has loss 8.4463\n",
            "Epoch 10 has loss 7.8192\n",
            "Epoch 11 has loss 7.6119\n",
            "Epoch 12 has loss 7.5439\n",
            "Epoch 13 has loss 7.4395\n",
            "Epoch 14 has loss 7.3141\n",
            "Epoch 15 has loss 7.4917\n",
            "Epoch 16 has loss 7.1425\n",
            "Epoch 17 has loss 6.8338\n",
            "Epoch 18 has loss 6.7387\n",
            "Epoch 19 has loss 6.5144\n",
            "Epoch 20 has loss 6.2755\n",
            "Epoch 21 has loss 6.1411\n",
            "Epoch 22 has loss 6.2256\n",
            "Epoch 23 has loss 6.113\n",
            "Epoch 24 has loss 6.0156\n",
            "Epoch 25 has loss 5.8987\n",
            "Epoch 26 has loss 5.8663\n",
            "Epoch 27 has loss 5.9585\n",
            "Epoch 28 has loss 5.9178\n",
            "Epoch 29 has loss 5.9025\n",
            "Epoch 30 has loss 5.7551\n",
            "Epoch 31 has loss 5.6687\n",
            "Epoch 32 has loss 5.7028\n",
            "Epoch 33 has loss 5.5581\n",
            "Epoch 34 has loss 5.4909\n",
            "Epoch 35 has loss 5.5321\n",
            "Epoch 36 has loss 5.7194\n",
            "Epoch 37 has loss 5.5842\n",
            "Epoch 38 has loss 5.4559\n",
            "Epoch 39 has loss 5.3655\n",
            "Epoch 40 has loss 5.3721\n",
            "Epoch 41 has loss 5.4046\n",
            "Epoch 42 has loss 5.348\n",
            "Epoch 43 has loss 5.5361\n",
            "Epoch 44 has loss 5.5023\n",
            "Epoch 45 has loss 5.3744\n",
            "Epoch 46 has loss 5.4517\n",
            "Epoch 47 has loss 5.246\n",
            "Epoch 48 has loss 5.3541\n",
            "Epoch 49 has loss 5.2232\n",
            "  4496 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  6228 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  7770 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  10018 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  11557 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (sin GPT):\n",
            "Hits@10: 0.0337\n",
            "Hits@20: 0.0467\n",
            "Hits@30: 0.0582\n",
            "Hits@40: 0.0750\n",
            "Hits@50: 0.0866\n",
            "Accuracy: 0.8137\n"
          ]
        }
      ],
      "source": [
        "model_post_sci_base = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_post_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_post_sci_base.reset_parameters()\n",
        "predictor_post_sci_base.reset_parameters()\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_post_sci_base.parameters()) + list(predictor_post_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_post_sci_base, predictor_post_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_base = test(model_post_sci_base, predictor_post_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (sin GPT):\")\n",
        "for k, v in results_post_sci_base.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ embedding_scibert_tensor_v2[:10]: tensor([ 0.0590, -0.5027, -0.0948, -0.0193,  0.2944, -0.1483,  0.1095,  1.0214,\n",
            "        -0.0039,  0.1579])\n",
            "Epoch 0 has loss 11.7952\n",
            "Epoch 1 has loss 12.067\n",
            "Epoch 2 has loss 10.6492\n",
            "Epoch 3 has loss 9.7133\n",
            "Epoch 4 has loss 9.1301\n",
            "Epoch 5 has loss 8.1834\n",
            "Epoch 6 has loss 7.7833\n",
            "Epoch 7 has loss 7.639\n",
            "Epoch 8 has loss 7.6248\n",
            "Epoch 9 has loss 7.5129\n",
            "Epoch 10 has loss 7.4097\n",
            "Epoch 11 has loss 7.437\n",
            "Epoch 12 has loss 7.4541\n",
            "Epoch 13 has loss 7.3265\n",
            "Epoch 14 has loss 7.5796\n",
            "Epoch 15 has loss 7.3923\n",
            "Epoch 16 has loss 7.3384\n",
            "Epoch 17 has loss 7.5776\n",
            "Epoch 18 has loss 7.3593\n",
            "Epoch 19 has loss 7.2915\n",
            "Epoch 20 has loss 7.2454\n",
            "Epoch 21 has loss 7.328\n",
            "Epoch 22 has loss 7.298\n",
            "Epoch 23 has loss 7.1824\n",
            "Epoch 24 has loss 6.9158\n",
            "Epoch 25 has loss 7.0769\n",
            "Epoch 26 has loss 6.7322\n",
            "Epoch 27 has loss 6.47\n",
            "Epoch 28 has loss 6.3778\n",
            "Epoch 29 has loss 6.1672\n",
            "Epoch 30 has loss 6.0208\n",
            "Epoch 31 has loss 6.06\n",
            "Epoch 32 has loss 5.8317\n",
            "Epoch 33 has loss 5.7367\n",
            "Epoch 34 has loss 5.6549\n",
            "Epoch 35 has loss 5.4089\n",
            "Epoch 36 has loss 5.298\n",
            "Epoch 37 has loss 5.3039\n",
            "Epoch 38 has loss 5.3058\n",
            "Epoch 39 has loss 5.2662\n",
            "Epoch 40 has loss 5.0949\n",
            "Epoch 41 has loss 5.2339\n",
            "Epoch 42 has loss 5.1119\n",
            "Epoch 43 has loss 4.9891\n",
            "Epoch 44 has loss 4.8993\n",
            "Epoch 45 has loss 4.8846\n",
            "Epoch 46 has loss 4.9017\n",
            "Epoch 47 has loss 4.7972\n",
            "Epoch 48 has loss 4.7933\n",
            "Epoch 49 has loss 4.7378\n",
            "  2523 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4888 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  4990 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  4990 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4990 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (como input):\n",
            "Hits@10: 0.0189\n",
            "Hits@20: 0.0366\n",
            "Hits@30: 0.0374\n",
            "Hits@40: 0.0374\n",
            "Hits@50: 0.0374\n",
            "Accuracy: 0.8572\n"
          ]
        }
      ],
      "source": [
        "emb = torch.ones(num_nodes, 1).to(device)\n",
        "print(\"✅ embedding_scibert_tensor_v2[:10]:\", embedding_scibert_tensor_v2[:10])\n",
        "aug_emb_sci_input = torch.cat([emb, embedding_scibert_tensor_v2.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model_post_sci_input = PostProcessSAGE(\n",
        "    in_channels=aug_emb_sci_input.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "\n",
        "predictor_post_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_post_sci_input.reset_parameters()\n",
        "predictor_post_sci_input.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_post_sci_input.parameters()) + list(predictor_post_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_post_sci_input, predictor_post_sci_input, aug_emb_sci_input, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_input = test(model_post_sci_input, predictor_post_sci_input, aug_emb_sci_input, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (como input):\")\n",
        "for k, v in results_post_sci_input.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.9242\n",
            "Epoch 1 has loss 11.7878\n",
            "Epoch 2 has loss 11.5175\n",
            "Epoch 3 has loss 11.2032\n",
            "Epoch 4 has loss 11.1052\n",
            "Epoch 5 has loss 10.6424\n",
            "Epoch 6 has loss 10.5478\n",
            "Epoch 7 has loss 10.3047\n",
            "Epoch 8 has loss 10.0238\n",
            "Epoch 9 has loss 9.8149\n",
            "Epoch 10 has loss 9.4505\n",
            "Epoch 11 has loss 9.3472\n",
            "Epoch 12 has loss 9.2285\n",
            "Epoch 13 has loss 9.1306\n",
            "Epoch 14 has loss 9.0926\n",
            "Epoch 15 has loss 9.0685\n",
            "Epoch 16 has loss 9.0605\n",
            "Epoch 17 has loss 9.0967\n",
            "Epoch 18 has loss 9.0456\n",
            "Epoch 19 has loss 9.0559\n",
            "Epoch 20 has loss 9.0466\n",
            "Epoch 21 has loss 9.05\n",
            "Epoch 22 has loss 8.9234\n",
            "Epoch 23 has loss 9.2097\n",
            "Epoch 24 has loss 9.4765\n",
            "Epoch 25 has loss 9.1845\n",
            "Epoch 26 has loss 9.0623\n",
            "Epoch 27 has loss 9.049\n",
            "Epoch 28 has loss 9.066\n",
            "Epoch 29 has loss 9.0722\n",
            "Epoch 30 has loss 9.0375\n",
            "Epoch 31 has loss 9.0626\n",
            "Epoch 32 has loss 9.0902\n",
            "Epoch 33 has loss 9.1749\n",
            "Epoch 34 has loss 9.049\n",
            "Epoch 35 has loss 9.0751\n",
            "Epoch 36 has loss 9.0503\n",
            "Epoch 37 has loss 9.0198\n",
            "Epoch 38 has loss 9.4607\n",
            "Epoch 39 has loss 9.3934\n",
            "Epoch 40 has loss 9.0892\n",
            "Epoch 41 has loss 8.9052\n",
            "Epoch 42 has loss 8.9023\n",
            "Epoch 43 has loss 8.9354\n",
            "Epoch 44 has loss 8.9121\n",
            "Epoch 45 has loss 8.8311\n",
            "Epoch 46 has loss 8.9032\n",
            "Epoch 47 has loss 8.831\n",
            "Epoch 48 has loss 8.8173\n",
            "Epoch 49 has loss 8.8785\n",
            "📉 Epoch 0, Loss: 20.6778\n",
            "📉 Epoch 1, Loss: 15.8744\n",
            "📉 Epoch 2, Loss: 14.8107\n",
            "📉 Epoch 3, Loss: 13.9682\n",
            "📉 Epoch 4, Loss: 13.6932\n",
            "📉 Epoch 5, Loss: 13.6161\n",
            "📉 Epoch 6, Loss: 13.5268\n",
            "📉 Epoch 7, Loss: 13.4861\n",
            "📉 Epoch 8, Loss: 13.4805\n",
            "📉 Epoch 9, Loss: 13.4816\n",
            "📉 Epoch 10, Loss: 13.4361\n",
            "📉 Epoch 11, Loss: 13.4104\n",
            "📉 Epoch 12, Loss: 13.3988\n",
            "📉 Epoch 13, Loss: 13.3938\n",
            "📉 Epoch 14, Loss: 13.4018\n",
            "📉 Epoch 15, Loss: 13.3858\n",
            "📉 Epoch 16, Loss: 13.3891\n",
            "📉 Epoch 17, Loss: 13.3859\n",
            "📉 Epoch 18, Loss: 13.3930\n",
            "📉 Epoch 19, Loss: 13.3693\n",
            "📉 Epoch 20, Loss: 13.3726\n",
            "📉 Epoch 21, Loss: 13.3887\n",
            "📉 Epoch 22, Loss: 13.3724\n",
            "📉 Epoch 23, Loss: 13.3496\n",
            "📉 Epoch 24, Loss: 13.3583\n",
            "📉 Epoch 25, Loss: 13.3473\n",
            "📉 Epoch 26, Loss: 13.3655\n",
            "📉 Epoch 27, Loss: 13.3541\n",
            "📉 Epoch 28, Loss: 13.3607\n",
            "📉 Epoch 29, Loss: 13.3365\n",
            "📉 Epoch 30, Loss: 13.3339\n",
            "📉 Epoch 31, Loss: 13.3163\n",
            "📉 Epoch 32, Loss: 13.3239\n",
            "📉 Epoch 33, Loss: 13.3422\n",
            "📉 Epoch 34, Loss: 13.3448\n",
            "📉 Epoch 35, Loss: 13.3441\n",
            "📉 Epoch 36, Loss: 13.3472\n",
            "📉 Epoch 37, Loss: 13.3149\n",
            "📉 Epoch 38, Loss: 13.3178\n",
            "📉 Epoch 39, Loss: 13.3259\n",
            "📉 Epoch 40, Loss: 13.3675\n",
            "📉 Epoch 41, Loss: 13.3350\n",
            "📉 Epoch 42, Loss: 13.2977\n",
            "📉 Epoch 43, Loss: 13.3148\n",
            "📉 Epoch 44, Loss: 13.3201\n",
            "📉 Epoch 45, Loss: 13.2980\n",
            "📉 Epoch 46, Loss: 13.2982\n",
            "📉 Epoch 47, Loss: 13.2852\n",
            "📉 Epoch 48, Loss: 13.2882\n",
            "📉 Epoch 49, Loss: 13.2838\n",
            "  2213 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4177 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5468 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  5943 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  6602 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (GPT final):\n",
            "Hits@10: 0.0166\n",
            "Hits@20: 0.0313\n",
            "Hits@30: 0.0410\n",
            "Hits@40: 0.0445\n",
            "Hits@50: 0.0495\n",
            "Accuracy: 0.8349\n"
          ]
        }
      ],
      "source": [
        "model_post_sci_final = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_post_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_post_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_post_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_post_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_sci_final = model_post_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_post_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_sci_final.size(1),\n",
        "    hidden_channels=z_post_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v2.shape[0]\n",
        ").to(device)\n",
        "predictor_post_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_sci_final(\n",
        "            z_post_sci_final[edge[0]], z_post_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v2.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_post_sci = WrappedPredictor(predictor_post_sci_final, embedding_scibert_tensor_v2)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_sci_final = test(dummy_model, wrapped_predictor_post_sci, z_post_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (GPT final):\")\n",
        "for k, v in results_post_sci_final.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>100</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.014923</td>\n",
              "      <td>0.017664</td>\n",
              "      <td>0.7774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005206</td>\n",
              "      <td>0.210482</td>\n",
              "      <td>0.265707</td>\n",
              "      <td>0.293904</td>\n",
              "      <td>0.322446</td>\n",
              "      <td>0.8781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.132925</td>\n",
              "      <td>0.153601</td>\n",
              "      <td>0.181296</td>\n",
              "      <td>0.8706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>0.048438</td>\n",
              "      <td>0.053443</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.068358</td>\n",
              "      <td>0.8070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.112204</td>\n",
              "      <td>0.156620</td>\n",
              "      <td>0.191394</td>\n",
              "      <td>0.222917</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>0.9000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.285020</td>\n",
              "      <td>0.370045</td>\n",
              "      <td>0.423773</td>\n",
              "      <td>0.445228</td>\n",
              "      <td>0.485149</td>\n",
              "      <td>0.9281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.054461</td>\n",
              "      <td>0.084194</td>\n",
              "      <td>0.098555</td>\n",
              "      <td>0.116526</td>\n",
              "      <td>0.126265</td>\n",
              "      <td>0.8887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.017919</td>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.037546</td>\n",
              "      <td>0.045494</td>\n",
              "      <td>0.050761</td>\n",
              "      <td>0.7755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>0.7881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.015762</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.028309</td>\n",
              "      <td>0.033471</td>\n",
              "      <td>0.037501</td>\n",
              "      <td>0.8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.010203</td>\n",
              "      <td>0.180517</td>\n",
              "      <td>0.8596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.8574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.071324</td>\n",
              "      <td>0.083932</td>\n",
              "      <td>0.100870</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.124819</td>\n",
              "      <td>0.8507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.144394</td>\n",
              "      <td>0.244927</td>\n",
              "      <td>0.289559</td>\n",
              "      <td>0.306767</td>\n",
              "      <td>0.362846</td>\n",
              "      <td>0.9031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.159069</td>\n",
              "      <td>0.249339</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.292009</td>\n",
              "      <td>0.309688</td>\n",
              "      <td>0.8972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.182367</td>\n",
              "      <td>0.201028</td>\n",
              "      <td>0.225502</td>\n",
              "      <td>0.243181</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.019335</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.029223</td>\n",
              "      <td>0.7761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.013859</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.025395</td>\n",
              "      <td>0.031456</td>\n",
              "      <td>0.7997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.026317</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>0.041756</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.7884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.8339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099873</td>\n",
              "      <td>0.131928</td>\n",
              "      <td>0.144664</td>\n",
              "      <td>0.154694</td>\n",
              "      <td>0.166321</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.066095</td>\n",
              "      <td>0.080838</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.101409</td>\n",
              "      <td>0.110938</td>\n",
              "      <td>0.8477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.177910</td>\n",
              "      <td>0.252201</td>\n",
              "      <td>0.317981</td>\n",
              "      <td>0.347175</td>\n",
              "      <td>0.364524</td>\n",
              "      <td>0.9012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.160380</td>\n",
              "      <td>0.199724</td>\n",
              "      <td>0.228341</td>\n",
              "      <td>0.252583</td>\n",
              "      <td>0.8987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178404</td>\n",
              "      <td>0.202983</td>\n",
              "      <td>0.235135</td>\n",
              "      <td>0.252553</td>\n",
              "      <td>0.267580</td>\n",
              "      <td>0.8981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003296</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>0.020226</td>\n",
              "      <td>0.7771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.037561</td>\n",
              "      <td>0.077707</td>\n",
              "      <td>0.090674</td>\n",
              "      <td>0.094899</td>\n",
              "      <td>0.099813</td>\n",
              "      <td>0.7046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.8083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.8475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.8310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.039621</td>\n",
              "      <td>0.059188</td>\n",
              "      <td>0.102458</td>\n",
              "      <td>0.112114</td>\n",
              "      <td>0.119261</td>\n",
              "      <td>0.8508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.235488</td>\n",
              "      <td>0.294616</td>\n",
              "      <td>0.335833</td>\n",
              "      <td>0.351714</td>\n",
              "      <td>0.374158</td>\n",
              "      <td>0.9025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.178651</td>\n",
              "      <td>0.212827</td>\n",
              "      <td>0.251968</td>\n",
              "      <td>0.290458</td>\n",
              "      <td>0.304879</td>\n",
              "      <td>0.9051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.036400</td>\n",
              "      <td>0.041794</td>\n",
              "      <td>0.045899</td>\n",
              "      <td>0.052843</td>\n",
              "      <td>0.057398</td>\n",
              "      <td>0.8582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.7365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.008375</td>\n",
              "      <td>0.010967</td>\n",
              "      <td>0.012720</td>\n",
              "      <td>0.015672</td>\n",
              "      <td>0.019290</td>\n",
              "      <td>0.7638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.005042</td>\n",
              "      <td>0.005956</td>\n",
              "      <td>0.008218</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.7927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.051832</td>\n",
              "      <td>0.059810</td>\n",
              "      <td>0.071744</td>\n",
              "      <td>0.081295</td>\n",
              "      <td>0.091288</td>\n",
              "      <td>0.8109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BioBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.025365</td>\n",
              "      <td>0.041389</td>\n",
              "      <td>0.049660</td>\n",
              "      <td>0.058027</td>\n",
              "      <td>0.067496</td>\n",
              "      <td>0.8676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BioBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.097349</td>\n",
              "      <td>0.132797</td>\n",
              "      <td>0.149361</td>\n",
              "      <td>0.166074</td>\n",
              "      <td>0.178771</td>\n",
              "      <td>0.8496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.242380</td>\n",
              "      <td>0.309569</td>\n",
              "      <td>0.346403</td>\n",
              "      <td>0.371978</td>\n",
              "      <td>0.385163</td>\n",
              "      <td>0.9073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.099252</td>\n",
              "      <td>0.145390</td>\n",
              "      <td>0.177168</td>\n",
              "      <td>0.213943</td>\n",
              "      <td>0.241188</td>\n",
              "      <td>0.9026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.033853</td>\n",
              "      <td>0.036999</td>\n",
              "      <td>0.041621</td>\n",
              "      <td>0.045112</td>\n",
              "      <td>0.048895</td>\n",
              "      <td>0.8659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.005753</td>\n",
              "      <td>0.019927</td>\n",
              "      <td>0.029306</td>\n",
              "      <td>0.033314</td>\n",
              "      <td>0.038835</td>\n",
              "      <td>0.7828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.102091</td>\n",
              "      <td>0.119231</td>\n",
              "      <td>0.128138</td>\n",
              "      <td>0.137112</td>\n",
              "      <td>0.144551</td>\n",
              "      <td>0.8120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.7849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.033681</td>\n",
              "      <td>0.046656</td>\n",
              "      <td>0.058207</td>\n",
              "      <td>0.075047</td>\n",
              "      <td>0.086576</td>\n",
              "      <td>0.8137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.8669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>explicit structural focus</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.016578</td>\n",
              "      <td>0.031291</td>\n",
              "      <td>0.040962</td>\n",
              "      <td>0.044521</td>\n",
              "      <td>0.049457</td>\n",
              "      <td>0.8349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source                Prompt Type  \\\n",
              "0          SAGEConv            classic                          -   \n",
              "1          SAGEConv      classic + GPT    reasoning-based summary   \n",
              "2          SAGEConv            classic    reasoning-based summary   \n",
              "3      SkipConnSAGE            classic                          -   \n",
              "4      SkipConnSAGE      classic + GPT    reasoning-based summary   \n",
              "5      SkipConnSAGE            classic    reasoning-based summary   \n",
              "6   PostProcessSAGE            classic                          -   \n",
              "7   PostProcessSAGE      classic + GPT    reasoning-based summary   \n",
              "8   PostProcessSAGE            classic    reasoning-based summary   \n",
              "9          SAGEConv           classic                           -   \n",
              "10         SAGEConv  classic + BioBERT    reasoning-based summary   \n",
              "11         SAGEConv            classic    reasoning-based summary   \n",
              "12     SkipConnSAGE            classic                          -   \n",
              "13     SkipConnSAGE  classic + BioBERT    reasoning-based summary   \n",
              "14     SkipConnSAGE            classic    reasoning-based summary   \n",
              "15  PostProcessSAGE            classic                          -   \n",
              "16  PostProcessSAGE  classic + BioBERT    reasoning-based summary   \n",
              "17  PostProcessSAGE            classic    reasoning-based summary   \n",
              "18         SAGEConv            classic                          -   \n",
              "19         SAGEConv  classic + SciBERT    reasoning-based summary   \n",
              "20         SAGEConv            classic    reasoning-based summary   \n",
              "21     SkipConnSAGE            classic                          -   \n",
              "22     SkipConnSAGE  classic + SciBERT    reasoning-based summary   \n",
              "23     SkipConnSAGE            classic    reasoning-based summary   \n",
              "24  PostProcessSAGE            classic                          -   \n",
              "25  PostProcessSAGE  classic + SciBERT    reasoning-based summary   \n",
              "26  PostProcessSAGE            classic    reasoning-based summary   \n",
              "27         SAGEConv            classic                          -   \n",
              "28         SAGEConv     classic + BERT  explicit structural focus   \n",
              "29         SAGEConv            classic  explicit structural focus   \n",
              "30     SkipConnSAGE            classic                          -   \n",
              "31     SkipConnSAGE     classic + BERT  explicit structural focus   \n",
              "32     SkipConnSAGE            classic  explicit structural focus   \n",
              "33  PostProcessSAGE            classic                          -   \n",
              "34  PostProcessSAGE     classic + BERT  explicit structural focus   \n",
              "35  PostProcessSAGE            classic  explicit structural focus   \n",
              "36         SAGEConv            classic                          -   \n",
              "37         SAGEConv  classic + BioBERT  explicit structural focus   \n",
              "38         SAGEConv            classic  explicit structural focus   \n",
              "39     SkipConnSAGE            classic                          -   \n",
              "40     SkipConnSAGE  classic + BioBERT  explicit structural focus   \n",
              "41     SkipConnSAGE            classic  explicit structural focus   \n",
              "42  PostProcessSAGE            classic                          -   \n",
              "43  PostProcessSAGE  classic + BioBERT  explicit structural focus   \n",
              "44  PostProcessSAGE            classic  explicit structural focus   \n",
              "45         SAGEConv            classic                          -   \n",
              "46         SAGEConv  classic + SciBERT  explicit structural focus   \n",
              "47         SAGEConv            classic  explicit structural focus   \n",
              "48     SkipConnSAGE            classic                          -   \n",
              "49     SkipConnSAGE  classic + SciBERT  explicit structural focus   \n",
              "50     SkipConnSAGE            classic  explicit structural focus   \n",
              "51  PostProcessSAGE            classic                          -   \n",
              "52  PostProcessSAGE  classic + SciBERT  explicit structural focus   \n",
              "53  PostProcessSAGE            classic  explicit structural focus   \n",
              "\n",
              "   GPT Injection         Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -            Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input            Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final      Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -            Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input            Neural     100  0.000809  0.001049  0.001288   \n",
              "5          final      Neural + GPT     100  0.003274  0.007911  0.012196   \n",
              "6              -            Neural      50  0.005206  0.210482  0.265707   \n",
              "7          input            Neural      50  0.007334  0.007334  0.132925   \n",
              "8          final      Neural + GPT      50  0.038385  0.048438  0.053443   \n",
              "9              -            Neural      50  0.112204  0.156620  0.191394   \n",
              "10         input            Neural      50  0.285020  0.370045  0.423773   \n",
              "11         final  Neural + BioBERT      50  0.054461  0.084194  0.098555   \n",
              "12             -            Neural      50  0.017919  0.031164  0.037546   \n",
              "13         input            Neural      50  0.000277  0.000622  0.001079   \n",
              "14         final  Neural + BioBERT      50  0.015762  0.019829  0.028309   \n",
              "15             -            Neural      50  0.010203  0.010203  0.010203   \n",
              "16         input            Neural      50  0.003813  0.003813  0.003813   \n",
              "17         final  Neural + BioBERT      50  0.071324  0.083932  0.100870   \n",
              "18             -            Neural      50  0.144394  0.244927  0.289559   \n",
              "19         input            Neural      50  0.159069  0.249339  0.278809   \n",
              "20         final  Neural + SciBERT      50  0.078329  0.182367  0.201028   \n",
              "21             -            Neural      50  0.008188  0.016661  0.019335   \n",
              "22         input            Neural      50  0.006270  0.013859  0.019290   \n",
              "23         final  Neural + SciBERT      50  0.026317  0.033389  0.041756   \n",
              "24             -            Neural      50  0.000562  0.000562  0.000562   \n",
              "25         input            Neural      50  0.099873  0.131928  0.144664   \n",
              "26         final  Neural + SciBERT      50  0.066095  0.080838  0.090659   \n",
              "27             -            Neural      50  0.177910  0.252201  0.317981   \n",
              "28         input            Neural      50  0.121800  0.160380  0.199724   \n",
              "29         final     Neural + BERT      50  0.178404  0.202983  0.235135   \n",
              "30             -            Neural      50  0.000015  0.003296  0.014518   \n",
              "31         input            Neural      50  0.037561  0.077707  0.090674   \n",
              "32         final     Neural + BERT      50  0.000165  0.000524  0.000914   \n",
              "33             -            Neural      50  0.000367  0.000367  0.000367   \n",
              "34         input            Neural      50  0.005439  0.005439  0.005439   \n",
              "35         final     Neural + BERT      50  0.039621  0.059188  0.102458   \n",
              "36             -            Neural      50  0.235488  0.294616  0.335833   \n",
              "37         input            Neural      50  0.178651  0.212827  0.251968   \n",
              "38         final  Neural + BioBERT      50  0.036400  0.041794  0.045899   \n",
              "39             -            Neural      50  0.000015  0.000037  0.000067   \n",
              "40         input            Neural      50  0.008375  0.010967  0.012720   \n",
              "41         final  Neural + BioBERT      50  0.002375  0.005042  0.005956   \n",
              "42             -            Neural      50  0.051832  0.059810  0.071744   \n",
              "43         input            Neural      50  0.025365  0.041389  0.049660   \n",
              "44         final  Neural + BioBERT      50  0.097349  0.132797  0.149361   \n",
              "45             -            Neural      50  0.242380  0.309569  0.346403   \n",
              "46         input            Neural      50  0.099252  0.145390  0.177168   \n",
              "47         final  Neural + SciBERT      50  0.033853  0.036999  0.041621   \n",
              "48             -            Neural      50  0.005753  0.019927  0.029306   \n",
              "49         input            Neural      50  0.102091  0.119231  0.128138   \n",
              "50         final  Neural + SciBERT      50  0.000749  0.000921  0.001154   \n",
              "51             -            Neural      50  0.033681  0.046656  0.058207   \n",
              "52         input            Neural      50  0.000000  0.000000  0.000000   \n",
              "53         final  Neural + SciBERT      50  0.016578  0.031291  0.040962   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "5   0.014923  0.017664    0.7774  \n",
              "6   0.293904  0.322446    0.8781  \n",
              "7   0.153601  0.181296    0.8706  \n",
              "8   0.061211  0.068358    0.8070  \n",
              "9   0.222917  0.243204    0.9000  \n",
              "10  0.445228  0.485149    0.9281  \n",
              "11  0.116526  0.126265    0.8887  \n",
              "12  0.045494  0.050761    0.7755  \n",
              "13  0.001671  0.002682    0.7881  \n",
              "14  0.033471  0.037501    0.8014  \n",
              "15  0.010203  0.180517    0.8596  \n",
              "16  0.003813  0.003813    0.8574  \n",
              "17  0.115440  0.124819    0.8507  \n",
              "18  0.306767  0.362846    0.9031  \n",
              "19  0.292009  0.309688    0.8972  \n",
              "20  0.225502  0.243181    0.8985  \n",
              "21  0.025343  0.029223    0.7761  \n",
              "22  0.025395  0.031456    0.7997  \n",
              "23  0.049000  0.057698    0.7884  \n",
              "24  0.000562  0.000562    0.8339  \n",
              "25  0.154694  0.166321    0.8644  \n",
              "26  0.101409  0.110938    0.8477  \n",
              "27  0.347175  0.364524    0.9012  \n",
              "28  0.228341  0.252583    0.8987  \n",
              "29  0.252553  0.267580    0.8981  \n",
              "30  0.016676  0.020226    0.7771  \n",
              "31  0.094899  0.099813    0.7046  \n",
              "32  0.001618  0.002614    0.8083  \n",
              "33  0.000367  0.000367    0.8475  \n",
              "34  0.005439  0.005439    0.8310  \n",
              "35  0.112114  0.119261    0.8508  \n",
              "36  0.351714  0.374158    0.9025  \n",
              "37  0.290458  0.304879    0.9051  \n",
              "38  0.052843  0.057398    0.8582  \n",
              "39  0.000105  0.000127    0.7365  \n",
              "40  0.015672  0.019290    0.7638  \n",
              "41  0.008218  0.010742    0.7927  \n",
              "42  0.081295  0.091288    0.8109  \n",
              "43  0.058027  0.067496    0.8676  \n",
              "44  0.166074  0.178771    0.8496  \n",
              "45  0.371978  0.385163    0.9073  \n",
              "46  0.213943  0.241188    0.9026  \n",
              "47  0.045112  0.048895    0.8659  \n",
              "48  0.033314  0.038835    0.7828  \n",
              "49  0.137112  0.144551    0.8120  \n",
              "50  0.001288  0.001453    0.7849  \n",
              "51  0.075047  0.086576    0.8137  \n",
              "52  0.000000  0.000000    0.8669  \n",
              "53  0.044521  0.049457    0.8349  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "new_rows = [\n",
        "    # SAGEConv + SciBERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sci_base_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sci_input_v2},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_sci_final_v2},\n",
        "\n",
        "    # SkipConnSAGE + SciBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_sci_base},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_sci_input},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_skip_sci_final},\n",
        "\n",
        "    # PostProcessSAGE + SciBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_base},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_input},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": new_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_post_sci_final},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "# Aseguramos el orden de columnas\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary.to_csv(\"results_summary.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drug43: [Drug1036, Drug1149, Drug1185, Drug1251, Drug1553, Drug1673, Drug2323, Drug2346, Drug2513, Drug2566, Drug2712, Drug2736, Drug3000, Drug3246, Drug3330, Drug4076, Drug4149]\n",
            "Drug125: [Drug229, Drug743, Drug780, Drug968, Drug1083, Drug1116, Drug1177, Drug1438, Drug1765, Drug1770, Drug1872, Drug2346, Drug2646, Drug2893, Drug3330, Drug3785]\n",
            "Drug146: [Drug251, Drug548, Drug761, Drug925, Drug1099, Drug1154, Drug1177, Drug1438, Drug1553, Drug1666, Drug1673, Drug2017, Drug2041, Drug2323, Drug2346, Drug2712, Drug2893, Drug2938]\n",
            "Drug180: [Drug229, Drug780, Drug814, Drug925, Drug1036, Drug1083, Drug1177, Drug1185, Drug1203, Drug1293, Drug1872, Drug1881, Drug2132, Drug2303, Drug2423, Drug2435, Drug2513, Drug2756, Drug2826, Drug2842, Drug2854, Drug3049, Drug3094, Drug3095, Drug3501, Drug3580, Drug3703, Drug3720, Drug3721, Drug3994]\n",
            "Drug229: [Drug125, Drug180, Drug251, Drug780, Drug814, Drug925, Drug1036, Drug1079, Drug1185, Drug1765, Drug2132, Drug2303, Drug2346, Drug2435, Drug2646, Drug2756, Drug2842, Drug3049, Drug3095, Drug3330, Drug3720, Drug3994]\n",
            "Drug251: [Drug146, Drug229, Drug320, Drug349, Drug814, Drug888, Drug1036, Drug1083, Drug1110, Drug1185, Drug1251, Drug1293, Drug1345, Drug1624, Drug1666, Drug1872, Drug1881, Drug2132, Drug2303, Drug2344, Drug2346, Drug2423, Drug2435, Drug2480, Drug2498, Drug2513, Drug2808, Drug2826, Drug2842, Drug2854, Drug2893, Drug3094, Drug3095, Drug3472, Drug3501, Drug3580, Drug3720, Drug3721, Drug3979, Drug4123]\n",
            "Drug320: [Drug251, Drug548, Drug761, Drug925, Drug968, Drug1036, Drug1083, Drug1110, Drug1177, Drug1185, Drug1251, Drug1770, Drug2044, Drug2132, Drug2346, Drug2435, Drug2463, Drug2756, Drug2842, Drug2938, Drug3049, Drug3095, Drug3330, Drug3472, Drug3994]\n",
            "Drug349: [Drug251, Drug414, Drug780, Drug814, Drug1083, Drug1203, Drug1293, Drug1345, Drug1872, Drug2044, Drug2303, Drug2423, Drug2498, Drug2513, Drug2842, Drug2854, Drug3501, Drug3580, Drug3703, Drug3721, Drug3908, Drug3979, Drug4123]\n",
            "Drug414: [Drug349, Drug925, Drug1083, Drug1110, Drug1177, Drug1770, Drug1872, Drug2044, Drug2842, Drug2893]\n",
            "Drug548: [Drug146, Drug320, Drug761, Drug1293, Drug2346, Drug2463, Drug2513, Drug2756, Drug2777, Drug2938, Drug3472, Drug3501, Drug3580, Drug3721]\n",
            "\n",
            "🧠 Total nodos con al menos un vecino en el subgrafo: 106\n",
            "📝 Longitud del texto generado: 16,694 caracteres\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Elegimos 200 nodos únicos aleatoriamente del total de nodos\n",
        "num_nodes = adj_t.size(0)\n",
        "selected_nodes = set(random.sample(range(num_nodes), 120))\n",
        "\n",
        "# Pasamos el grafo a formato COO\n",
        "adj_t_coo = adj_t.t().coalesce()\n",
        "row, col, _ = adj_t_coo.coo()\n",
        "\n",
        "# Construimos el subgrafo inducido: aristas entre nodos seleccionados\n",
        "subgraph_dict = defaultdict(set)\n",
        "for u, v in zip(row.tolist(), col.tolist()):\n",
        "    if u in selected_nodes and v in selected_nodes:\n",
        "        subgraph_dict[u].add(v)\n",
        "        subgraph_dict[v].add(u)\n",
        "\n",
        "# Convertimos a representación tipo lista para el prompt\n",
        "subgraph_prompt_lines = []\n",
        "for node in sorted(subgraph_dict):\n",
        "    neighbors = sorted(subgraph_dict[node])\n",
        "    line = f\"Drug{node}: [{', '.join(f'Drug{n}' for n in neighbors)}]\"\n",
        "    subgraph_prompt_lines.append(line)\n",
        "\n",
        "# Texto resultante\n",
        "subgraph_prompt_text = \"\\n\".join(subgraph_prompt_lines)\n",
        "\n",
        "# Imprimimos ejemplo de las primeras 10 líneas\n",
        "print(\"\\n\".join(subgraph_prompt_lines[:10]))\n",
        "print(f\"\\n🧠 Total nodos con al menos un vecino en el subgrafo: {len(subgraph_dict)}\")\n",
        "print(f\"📝 Longitud del texto generado: {len(subgraph_prompt_text):,} caracteres\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Guardado en 'gpt_subgraph_prompt_expanded_1.txt'\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=\"sk-...\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a highly specialized assistant in graph-based machine learning applied to biomedical data. \"\n",
        "                \"You generate rich and technical reports describing the structural, semantic, and computational features of graphs. \"\n",
        "                \"Your outputs are intended to be used as textual embeddings to enhance GNN-based link prediction models.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "We are working with a biomedical graph representing drug–drug interactions (DDI). Below is a subgraph derived from the full DDI graph. Each line represents a drug and its direct neighbors (interacting drugs) within this subgraph:\n",
        "\n",
        "{subgraph_prompt_text}\n",
        "\n",
        "You MUST produce a **long and technical report** — **minimum 1500 words** — that maximizes semantic richness and diversity. Assume this response will be embedded and used to augment node representations in a downstream GNN for link prediction. Therefore, your text must include **abstract concepts**, **concrete node-level observations**, and **multi-scale analysis**.\n",
        "\n",
        "You MUST follow this **structure**, using rich language, and avoiding superficial summaries:\n",
        "\n",
        "---\n",
        "\n",
        "1. **Graph Structure**:\n",
        "   - Identify important patterns in connectivity (e.g., densely connected regions, sparse areas).\n",
        "   - Discuss the presence of central nodes, articulation points, bridges, and any non-trivial topological features.\n",
        "   - Identify recurring local structures (motifs, cliques, etc.) and explain their significance.\n",
        "\n",
        "2. **Community Detection and Roles**:\n",
        "   - Analyze the formation of communities or local clusters.\n",
        "   - Identify potential hub nodes and connector nodes between clusters.\n",
        "   - Comment on how these structural roles may influence diffusion or prediction in the graph.\n",
        "\n",
        "3. **Semantic Implications**:\n",
        "   - Speculate on the pharmacological or therapeutic interpretation of the observed clusters and hubs.\n",
        "   - Suggest hypotheses about drug categories or functional classes based on structural patterns.\n",
        "   - Highlight any unique or surprising local structures and propose possible biomedical interpretations.\n",
        "\n",
        "4. **Embedding Utility**:\n",
        "   - Assume your response will be embedded and used as input to a downstream GNN for link prediction. Therefore, use varied and information-rich descriptions, avoiding repetition.\n",
        "   - Include both high-level summaries and specific node-level insights to maximize embedding informativeness.\n",
        "   - Introduce speculative thoughts or edge-case interpretations to improve embedding diversity and generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### Your audience:\n",
        "Assume the reader is designing an ML pipeline and will use your text as a vector embedding (via Sentence Transformers or similar) to enrich node representations in a GNN. Therefore, your response should:\n",
        "- Include a **rich, diverse vocabulary** with both abstract and concrete observations.\n",
        "- Provide **multi-scale insights** (local, meso, global).\n",
        "- Mention specific node IDs when appropriate.\n",
        "- Explore the graph from multiple angles: topological, semantic, pharmacological, and computational.\n",
        "- Introduce **novel hypotheses** and **speculative interpretations** based on graph topology.\n",
        "\n",
        "**Output Guidelines**:\n",
        "- Be rigorous and exhaustive. Think like a graph scientist writing for ML practitioners.\n",
        "- Avoid reiteration, but ensure lexical and conceptual richness.\n",
        "- Mention specific node IDs (e.g., Drug143, Drug826) where appropriate.\n",
        "- Treat this as a real research input: your text will influence how the model understands the structure.\n",
        "- Do **not** summarize too early — each section must be self-contained and in-depth.\n",
        "- You are expected to write **a document that can stand on its own** as an advanced graph analysis report.\n",
        "\n",
        "Now begin your full technical report.\n",
        "\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "gpt_response_text_1 = response.choices[0].message.content\n",
        "\n",
        "with open(\"gpt_subgraph_prompt_expanded_1_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(gpt_response_text_1)\n",
        "\n",
        "print(\"\\n✅ Guardado en 'gpt_subgraph_prompt_expanded_1.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Guardado en 'gpt_subgraph_prompt_expanded_2_run2.txt'\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a highly specialized assistant in graph-based machine learning applied to biomedical data. \"\n",
        "                \"You generate rich and technical reports describing the structural, semantic, and computational features of graphs. \"\n",
        "                \"Your outputs are intended to be used as textual embeddings to enhance GNN-based link prediction models.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "We are performing an iterative refinement process.\n",
        "\n",
        "Below is the first version of your report:\n",
        "\n",
        "{gpt_response_text_1}\n",
        "\n",
        "Your goal now is to produce an **additional complementary text** (minimum 1500 words) that provides new and useful perspectives, examples, and technical content related to this DDI subgraph. \n",
        "\n",
        "Very important:\n",
        "\n",
        "- This new text will be **concatenated** with the previous one to create a richer embedding.\n",
        "- Therefore, you must focus on writing **new content** — avoid repetition or simple rephrasing.\n",
        "- You may structure your text as you see fit (no strict template required).\n",
        "- Your writing should aim to enrich the final embedding: add new graph insights, topological patterns, community roles, semantic interpretations, etc.\n",
        "\n",
        "Imagine you are a graph scientist collaborating in this project: what other angles, hypotheses, or ideas can you contribute to make the embedding more informative for GNN-based link prediction?\n",
        "\n",
        "Please proceed with this **second iteration**.\n",
        "\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "gpt_response_text_2 = response.choices[0].message.content\n",
        "\n",
        "with open(\"gpt_subgraph_prompt_expanded_2_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(gpt_response_text_2)\n",
        "\n",
        "print(\"\\n✅ Guardado en 'gpt_subgraph_prompt_expanded_2_run2.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Guardado en 'gpt_subgraph_prompt_expanded_3_run2.txt'\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a highly specialized assistant in graph-based machine learning applied to biomedical data. \"\n",
        "                \"You generate rich and technical reports describing the structural, semantic, and computational features of graphs. \"\n",
        "                \"Your outputs are intended to be used as textual embeddings to enhance GNN-based link prediction models.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "We are performing an **iterative refinement process** to generate embeddings from a biomedical DDI subgraph.\n",
        "\n",
        "Below are the **first two versions** of your reports:\n",
        "\n",
        "===== VERSION 1 =====\n",
        "{gpt_response_text_1}\n",
        "\n",
        "===== VERSION 2 =====\n",
        "{gpt_response_text_2}\n",
        "\n",
        "Your task now is to produce a **third and final complementary text** (minimum 1500 words).\n",
        "\n",
        "Very important:\n",
        "\n",
        "- The embedding used in our GNN pipeline will be built by **concatenating** version 1 + version 2 + this new version.\n",
        "- Therefore, your new text should contribute **new perspectives, new examples, new semantic or structural insights** — do not repeat content already present in versions 1 and 2.\n",
        "- It is perfectly acceptable to focus on new angles, node-level details, domain knowledge, or alternative interpretations.\n",
        "- You are free to choose the structure — what matters is providing **rich and diverse content** that will improve the embedding’s utility for link prediction.\n",
        "\n",
        "Please think like an expert graph scientist: what complementary information would you add so that, when combined, the three texts result in a powerful, multi-faceted embedding?\n",
        "\n",
        "Please proceed with this **third iteration**.\n",
        "\"\"\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "gpt_response_text_3 = response.choices[0].message.content\n",
        "\n",
        "with open(\"gpt_subgraph_prompt_expanded_3_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(gpt_response_text_3)\n",
        "\n",
        "print(\"\\n✅ Guardado en 'gpt_subgraph_prompt_expanded_3_run2.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Guardado en 'gpt_subgraph_prompt_expanded_expanded_run2.txt'\n"
          ]
        }
      ],
      "source": [
        "# Concatenación de las 3 respuestas\n",
        "full_text = gpt_response_text_1 + \"\\n\\n\" + gpt_response_text_2 + \"\\n\\n\" + gpt_response_text_3\n",
        "\n",
        "with open(\"gpt_subgraph_prompt_expanded_expanded_run2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(full_text)\n",
        "\n",
        "print(\"\\n✅ Guardado en 'gpt_subgraph_prompt_expanded_expanded_run2.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensión del nuevo embedding: (384,)\n",
            "Primeros valores: [ 0.0110441  -0.03314336 -0.02280774 -0.09702291  0.03004836 -0.04436129\n",
            " -0.03722152  0.07913473  0.04737891 -0.05260352]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Leer el nuevo archivo\n",
        "with open(\"gpt_subgraph_prompt_expanded_expanded_run2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt_response_text_v4 = f.read()\n",
        "\n",
        "# Usamos el mismo modelo SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embedding_v4 = model.encode(gpt_response_text_v4)\n",
        "\n",
        "print(\"Dimensión del nuevo embedding:\", embedding_v4.shape)\n",
        "print(\"Primeros valores:\", embedding_v4[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding v4 guardado en 'gpt_ddi_embedding_v4_run2.npy'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\"gpt_ddi_embedding_v4_run2.npy\", embedding_v4)\n",
        "print(\"✅ Embedding v4 guardado en 'gpt_ddi_embedding_v4_run2.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_global_v4 = np.load(\"gpt_ddi_embedding_v4_run2.npy\")\n",
        "embedding_global_v4 = torch.tensor(embedding_global_v4, dtype=torch.float32).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb = torch.ones(num_nodes, 1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "aug_emb_v4 = torch.cat([emb, embedding_global_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.5781\n",
            "Epoch 1 has loss 9.8675\n",
            "Epoch 2 has loss 9.5512\n",
            "Epoch 3 has loss 8.8603\n",
            "Epoch 4 has loss 8.6719\n",
            "Epoch 5 has loss 7.9256\n",
            "Epoch 6 has loss 7.4528\n",
            "Epoch 7 has loss 7.5255\n",
            "Epoch 8 has loss 7.5617\n",
            "Epoch 9 has loss 7.463\n",
            "Epoch 10 has loss 7.4022\n",
            "Epoch 11 has loss 7.3778\n",
            "Epoch 12 has loss 7.343\n",
            "Epoch 13 has loss 7.3384\n",
            "Epoch 14 has loss 7.2749\n",
            "Epoch 15 has loss 7.1876\n",
            "Epoch 16 has loss 7.0794\n",
            "Epoch 17 has loss 6.9605\n",
            "Epoch 18 has loss 6.737\n",
            "Epoch 19 has loss 6.4915\n",
            "Epoch 20 has loss 6.4438\n",
            "Epoch 21 has loss 6.0623\n",
            "Epoch 22 has loss 6.0343\n",
            "Epoch 23 has loss 5.7265\n",
            "Epoch 24 has loss 5.5087\n",
            "Epoch 25 has loss 5.6364\n",
            "Epoch 26 has loss 5.4678\n",
            "Epoch 27 has loss 5.3915\n",
            "Epoch 28 has loss 5.3346\n",
            "Epoch 29 has loss 5.1483\n",
            "Epoch 30 has loss 5.103\n",
            "Epoch 31 has loss 5.1494\n",
            "Epoch 32 has loss 5.091\n",
            "Epoch 33 has loss 5.037\n",
            "Epoch 34 has loss 4.8685\n",
            "Epoch 35 has loss 4.7404\n",
            "Epoch 36 has loss 4.8209\n",
            "Epoch 37 has loss 4.9006\n",
            "Epoch 38 has loss 4.6773\n",
            "Epoch 39 has loss 4.6158\n",
            "Epoch 40 has loss 4.6221\n",
            "Epoch 41 has loss 4.4724\n",
            "Epoch 42 has loss 4.4408\n",
            "Epoch 43 has loss 4.4817\n",
            "Epoch 44 has loss 4.4898\n",
            "Epoch 45 has loss 4.5079\n",
            "Epoch 46 has loss 4.4684\n",
            "Epoch 47 has loss 4.364\n",
            "Epoch 48 has loss 4.3106\n",
            "Epoch 49 has loss 4.2744\n",
            "  21218 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  36430 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  40624 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  44842 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  47701 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SAGEConv (base, v3):\n",
            "Hits@10: 0.1589\n",
            "Hits@20: 0.2729\n",
            "Hits@30: 0.3043\n",
            "Hits@40: 0.3359\n",
            "Hits@50: 0.3573\n",
            "Accuracy: 0.8942\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SAGEConv (base, v3):\")\n",
        "for k, v in results_sage_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.2321\n",
            "Epoch 1 has loss 10.1196\n",
            "Epoch 2 has loss 9.4705\n",
            "Epoch 3 has loss 10.1256\n",
            "Epoch 4 has loss 9.2618\n",
            "Epoch 5 has loss 9.3739\n",
            "Epoch 6 has loss 8.9796\n",
            "Epoch 7 has loss 8.1358\n",
            "Epoch 8 has loss 7.6573\n",
            "Epoch 9 has loss 7.5334\n",
            "Epoch 10 has loss 7.4008\n",
            "Epoch 11 has loss 7.0455\n",
            "Epoch 12 has loss 6.5729\n",
            "Epoch 13 has loss 6.234\n",
            "Epoch 14 has loss 6.0391\n",
            "Epoch 15 has loss 5.8556\n",
            "Epoch 16 has loss 5.7378\n",
            "Epoch 17 has loss 5.7174\n",
            "Epoch 18 has loss 5.6228\n",
            "Epoch 19 has loss 5.6009\n",
            "Epoch 20 has loss 5.5276\n",
            "Epoch 21 has loss 5.6204\n",
            "Epoch 22 has loss 5.6115\n",
            "Epoch 23 has loss 5.3736\n",
            "Epoch 24 has loss 5.3678\n",
            "Epoch 25 has loss 5.4683\n",
            "Epoch 26 has loss 5.1263\n",
            "Epoch 27 has loss 4.9433\n",
            "Epoch 28 has loss 4.9772\n",
            "Epoch 29 has loss 4.9133\n",
            "Epoch 30 has loss 4.7658\n",
            "Epoch 31 has loss 4.7746\n",
            "Epoch 32 has loss 4.6894\n",
            "Epoch 33 has loss 4.5748\n",
            "Epoch 34 has loss 4.5179\n",
            "Epoch 35 has loss 4.5571\n",
            "Epoch 36 has loss 4.5016\n",
            "Epoch 37 has loss 4.4971\n",
            "Epoch 38 has loss 4.5315\n",
            "Epoch 39 has loss 4.4096\n",
            "Epoch 40 has loss 4.4877\n",
            "Epoch 41 has loss 4.374\n",
            "Epoch 42 has loss 4.3005\n",
            "Epoch 43 has loss 4.2942\n",
            "Epoch 44 has loss 4.3409\n",
            "Epoch 45 has loss 4.28\n",
            "Epoch 46 has loss 4.2881\n",
            "Epoch 47 has loss 4.2082\n",
            "Epoch 48 has loss 4.2719\n",
            "Epoch 49 has loss 4.2277\n",
            "  29757 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  37551 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  43356 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  49202 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  51951 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔗 Resultados SAGEConv (con GPT v3 en input):\n",
            "Hits@10: 0.2229\n",
            "Hits@20: 0.2813\n",
            "Hits@30: 0.3248\n",
            "Hits@40: 0.3686\n",
            "Hits@50: 0.3892\n",
            "Accuracy: 0.8996\n"
          ]
        }
      ],
      "source": [
        "aug_emb_input_v4 = torch.cat([emb, embedding_global_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb_input_v4.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
        "\n",
        "train(model, predictor, aug_emb_input_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_input_v4 = test(model, predictor, aug_emb_input_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔗 Resultados SAGEConv (con GPT v3 en input):\")\n",
        "for k, v in results_sage_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7513\n",
            "Epoch 1 has loss 10.2811\n",
            "Epoch 2 has loss 10.2161\n",
            "Epoch 3 has loss 10.1678\n",
            "Epoch 4 has loss 10.0547\n",
            "Epoch 5 has loss 9.9899\n",
            "Epoch 6 has loss 9.9618\n",
            "Epoch 7 has loss 9.8318\n",
            "Epoch 8 has loss 9.6789\n",
            "Epoch 9 has loss 9.607\n",
            "Epoch 10 has loss 9.6237\n",
            "Epoch 11 has loss 9.5517\n",
            "Epoch 12 has loss 9.502\n",
            "Epoch 13 has loss 9.4978\n",
            "Epoch 14 has loss 9.5142\n",
            "Epoch 15 has loss 9.5046\n",
            "Epoch 16 has loss 9.4877\n",
            "Epoch 17 has loss 9.529\n",
            "Epoch 18 has loss 9.5003\n",
            "Epoch 19 has loss 9.4604\n",
            "Epoch 20 has loss 9.5132\n",
            "Epoch 21 has loss 9.4624\n",
            "Epoch 22 has loss 9.4599\n",
            "Epoch 23 has loss 9.4571\n",
            "Epoch 24 has loss 9.4428\n",
            "Epoch 25 has loss 9.4516\n",
            "Epoch 26 has loss 9.4455\n",
            "Epoch 27 has loss 9.4495\n",
            "Epoch 28 has loss 9.4353\n",
            "Epoch 29 has loss 9.452\n",
            "Epoch 30 has loss 9.4517\n",
            "Epoch 31 has loss 9.4264\n",
            "Epoch 32 has loss 9.4266\n",
            "Epoch 33 has loss 9.4266\n",
            "Epoch 34 has loss 9.4229\n",
            "Epoch 35 has loss 9.4092\n",
            "Epoch 36 has loss 9.4264\n",
            "Epoch 37 has loss 9.4111\n",
            "Epoch 38 has loss 9.4386\n",
            "Epoch 39 has loss 9.4401\n",
            "Epoch 40 has loss 9.401\n",
            "Epoch 41 has loss 9.414\n",
            "Epoch 42 has loss 9.4171\n",
            "Epoch 43 has loss 9.4124\n",
            "Epoch 44 has loss 9.4222\n",
            "Epoch 45 has loss 9.403\n",
            "Epoch 46 has loss 9.3912\n",
            "Epoch 47 has loss 9.4017\n",
            "Epoch 48 has loss 9.3927\n",
            "Epoch 49 has loss 9.4109\n",
            "📉 Epoch 0, Loss: 16.3930\n",
            "📉 Epoch 1, Loss: 13.6643\n",
            "📉 Epoch 2, Loss: 12.8842\n",
            "📉 Epoch 3, Loss: 12.3220\n",
            "📉 Epoch 4, Loss: 12.2846\n",
            "📉 Epoch 5, Loss: 11.8508\n",
            "📉 Epoch 6, Loss: 11.7359\n",
            "📉 Epoch 7, Loss: 11.5835\n",
            "📉 Epoch 8, Loss: 11.5018\n",
            "📉 Epoch 9, Loss: 11.4546\n",
            "📉 Epoch 10, Loss: 11.4072\n",
            "📉 Epoch 11, Loss: 11.5394\n",
            "📉 Epoch 12, Loss: 11.4355\n",
            "📉 Epoch 13, Loss: 11.3489\n",
            "📉 Epoch 14, Loss: 11.3030\n",
            "📉 Epoch 15, Loss: 11.2531\n",
            "📉 Epoch 16, Loss: 11.2085\n",
            "📉 Epoch 17, Loss: 11.2129\n",
            "📉 Epoch 18, Loss: 11.2528\n",
            "📉 Epoch 19, Loss: 11.1710\n",
            "📉 Epoch 20, Loss: 11.1354\n",
            "📉 Epoch 21, Loss: 11.1376\n",
            "📉 Epoch 22, Loss: 11.0955\n",
            "📉 Epoch 23, Loss: 11.0753\n",
            "📉 Epoch 24, Loss: 11.0814\n",
            "📉 Epoch 25, Loss: 11.0580\n",
            "📉 Epoch 26, Loss: 11.0293\n",
            "📉 Epoch 27, Loss: 11.0233\n",
            "📉 Epoch 28, Loss: 11.0137\n",
            "📉 Epoch 29, Loss: 11.0359\n",
            "📉 Epoch 30, Loss: 11.0088\n",
            "📉 Epoch 31, Loss: 11.0957\n",
            "📉 Epoch 32, Loss: 11.0341\n",
            "📉 Epoch 33, Loss: 10.9743\n",
            "📉 Epoch 34, Loss: 10.9806\n",
            "📉 Epoch 35, Loss: 11.0075\n",
            "📉 Epoch 36, Loss: 10.9807\n",
            "📉 Epoch 37, Loss: 10.9564\n",
            "📉 Epoch 38, Loss: 10.9488\n",
            "📉 Epoch 39, Loss: 10.9204\n",
            "📉 Epoch 40, Loss: 10.9249\n",
            "📉 Epoch 41, Loss: 10.8850\n",
            "📉 Epoch 42, Loss: 10.8702\n",
            "📉 Epoch 43, Loss: 10.9415\n",
            "📉 Epoch 44, Loss: 10.8784\n",
            "📉 Epoch 45, Loss: 10.8555\n",
            "📉 Epoch 46, Loss: 10.8515\n",
            "📉 Epoch 47, Loss: 10.8478\n",
            "📉 Epoch 48, Loss: 10.8247\n",
            "📉 Epoch 49, Loss: 10.8216\n",
            "  15465 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  18198 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  20759 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  23134 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  24622 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv (con GPT v3 en la capa final):\n",
            "Hits@10: 0.1159\n",
            "Hits@20: 0.1363\n",
            "Hits@30: 0.1555\n",
            "Hits@40: 0.1733\n",
            "Hits@50: 0.1844\n",
            "Accuracy: 0.8903\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "predictor_dummy = DotProductLinkPredictor().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, predictor_dummy, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_v4 = model(emb, adj_t)\n",
        "\n",
        "predictor_final_v4 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_v4.size(1),\n",
        "    hidden_channels=z_v4.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_final_v4.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_final_v4.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_final_v4.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_final_v4(\n",
        "            z_v4[edge[0]], z_v4[edge[1]],\n",
        "            embedding_global_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_final_v4 = WrappedPredictor(predictor_final_v4, embedding_global_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_final_v4 = test(dummy_model, wrapped_predictor_final_v4, z_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv (con GPT v3 en la capa final):\")\n",
        "for k, v in results_sage_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📘 Resultados SAGEConv (con GPT v3 en la capa final):\n",
            "Hits@10: 0.0362\n",
            "Hits@20: 0.0601\n",
            "Hits@30: 0.0848\n",
            "Hits@40: 0.1010\n",
            "Hits@50: 0.1106\n",
            "Accuracy: 0.8778\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n📘 Resultados SAGEConv (con GPT v3 en la capa final):\")\n",
        "for k, v in results_sage_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4478\n",
            "Epoch 1 has loss 10.3916\n",
            "Epoch 2 has loss 10.4043\n",
            "Epoch 3 has loss 10.1195\n",
            "Epoch 4 has loss 10.3095\n",
            "Epoch 5 has loss 10.1256\n",
            "Epoch 6 has loss 9.7005\n",
            "Epoch 7 has loss 9.5149\n",
            "Epoch 8 has loss 9.5189\n",
            "Epoch 9 has loss 9.8531\n",
            "Epoch 10 has loss 9.6711\n",
            "Epoch 11 has loss 9.6616\n",
            "Epoch 12 has loss 9.7209\n",
            "Epoch 13 has loss 9.1798\n",
            "Epoch 14 has loss 8.9018\n",
            "Epoch 15 has loss 8.9791\n",
            "Epoch 16 has loss 8.8211\n",
            "Epoch 17 has loss 9.1717\n",
            "Epoch 18 has loss 9.2544\n",
            "Epoch 19 has loss 8.6538\n",
            "Epoch 20 has loss 9.1361\n",
            "Epoch 21 has loss 8.5381\n",
            "Epoch 22 has loss 8.576\n",
            "Epoch 23 has loss 8.3935\n",
            "Epoch 24 has loss 8.3968\n",
            "Epoch 25 has loss 8.4275\n",
            "Epoch 26 has loss 8.1686\n",
            "Epoch 27 has loss 8.4823\n",
            "Epoch 28 has loss 8.0407\n",
            "Epoch 29 has loss 8.3869\n",
            "Epoch 30 has loss 7.9835\n",
            "Epoch 31 has loss 7.9278\n",
            "Epoch 32 has loss 7.7376\n",
            "Epoch 33 has loss 7.8469\n",
            "Epoch 34 has loss 7.8024\n",
            "Epoch 35 has loss 7.8534\n",
            "Epoch 36 has loss 7.7714\n",
            "Epoch 37 has loss 7.7022\n",
            "Epoch 38 has loss 7.7322\n",
            "Epoch 39 has loss 7.6943\n",
            "Epoch 40 has loss 7.9052\n",
            "Epoch 41 has loss 7.7002\n",
            "Epoch 42 has loss 7.5928\n",
            "Epoch 43 has loss 7.856\n",
            "Epoch 44 has loss 7.8212\n",
            "Epoch 45 has loss 7.6402\n",
            "Epoch 46 has loss 7.2796\n",
            "Epoch 47 has loss 7.0839\n",
            "Epoch 48 has loss 7.0156\n",
            "Epoch 49 has loss 6.847\n",
            "  118 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  250 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  558 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  1194 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  2189 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE (sin embedding v3):\n",
            "Hits@10: 0.0009\n",
            "Hits@20: 0.0019\n",
            "Hits@30: 0.0042\n",
            "Hits@40: 0.0089\n",
            "Hits@50: 0.0164\n",
            "Accuracy: 0.7712\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE (sin embedding v3):\")\n",
        "for k, v in results_skip_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7157\n",
            "Epoch 1 has loss 10.2977\n",
            "Epoch 2 has loss 10.1601\n",
            "Epoch 3 has loss 10.4198\n",
            "Epoch 4 has loss 9.919\n",
            "Epoch 5 has loss 10.0918\n",
            "Epoch 6 has loss 9.8296\n",
            "Epoch 7 has loss 9.7283\n",
            "Epoch 8 has loss 9.9376\n",
            "Epoch 9 has loss 9.6167\n",
            "Epoch 10 has loss 9.8305\n",
            "Epoch 11 has loss 9.7774\n",
            "Epoch 12 has loss 9.581\n",
            "Epoch 13 has loss 9.4563\n",
            "Epoch 14 has loss 9.61\n",
            "Epoch 15 has loss 9.6762\n",
            "Epoch 16 has loss 9.3756\n",
            "Epoch 17 has loss 9.2776\n",
            "Epoch 18 has loss 9.4854\n",
            "Epoch 19 has loss 9.2987\n",
            "Epoch 20 has loss 9.4292\n",
            "Epoch 21 has loss 8.9469\n",
            "Epoch 22 has loss 9.3691\n",
            "Epoch 23 has loss 8.8322\n",
            "Epoch 24 has loss 9.2448\n",
            "Epoch 25 has loss 9.005\n",
            "Epoch 26 has loss 8.7653\n",
            "Epoch 27 has loss 8.9496\n",
            "Epoch 28 has loss 8.4026\n",
            "Epoch 29 has loss 8.4693\n",
            "Epoch 30 has loss 9.2591\n",
            "Epoch 31 has loss 9.4069\n",
            "Epoch 32 has loss 8.7659\n",
            "Epoch 33 has loss 8.7604\n",
            "Epoch 34 has loss 8.6021\n",
            "Epoch 35 has loss 8.6722\n",
            "Epoch 36 has loss 8.3341\n",
            "Epoch 37 has loss 8.9126\n",
            "Epoch 38 has loss 8.7931\n",
            "Epoch 39 has loss 9.0971\n",
            "Epoch 40 has loss 8.7261\n",
            "Epoch 41 has loss 8.6849\n",
            "Epoch 42 has loss 8.7646\n",
            "Epoch 43 has loss 8.8347\n",
            "Epoch 44 has loss 8.3386\n",
            "Epoch 45 has loss 8.4025\n",
            "Epoch 46 has loss 8.3325\n",
            "Epoch 47 has loss 8.3733\n",
            "Epoch 48 has loss 8.2286\n",
            "Epoch 49 has loss 7.9046\n",
            "  1 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  60 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  194 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  353 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📙 Resultados SkipConnSAGE (embedding v3 como input):\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0004\n",
            "Hits@40: 0.0015\n",
            "Hits@50: 0.0026\n",
            "Accuracy: 0.7928\n"
          ]
        }
      ],
      "source": [
        "aug_emb_v4 = torch.cat([\n",
        "    emb,\n",
        "    embedding_global_v4.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb_v4.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_input_v4 = test(model, predictor, aug_emb_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📙 Resultados SkipConnSAGE (embedding v3 como input):\")\n",
        "for k, v in results_skip_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1762\n",
            "Epoch 1 has loss 10.6584\n",
            "Epoch 2 has loss 10.4364\n",
            "Epoch 3 has loss 10.3527\n",
            "Epoch 4 has loss 10.2744\n",
            "Epoch 5 has loss 10.2776\n",
            "Epoch 6 has loss 10.2631\n",
            "Epoch 7 has loss 10.2164\n",
            "Epoch 8 has loss 10.1762\n",
            "Epoch 9 has loss 10.1683\n",
            "Epoch 10 has loss 10.1603\n",
            "Epoch 11 has loss 10.1623\n",
            "Epoch 12 has loss 10.1254\n",
            "Epoch 13 has loss 10.0816\n",
            "Epoch 14 has loss 10.1136\n",
            "Epoch 15 has loss 10.0937\n",
            "Epoch 16 has loss 10.0629\n",
            "Epoch 17 has loss 10.0966\n",
            "Epoch 18 has loss 10.0866\n",
            "Epoch 19 has loss 10.0847\n",
            "Epoch 20 has loss 10.0521\n",
            "Epoch 21 has loss 10.0298\n",
            "Epoch 22 has loss 10.0524\n",
            "Epoch 23 has loss 10.0479\n",
            "Epoch 24 has loss 10.0251\n",
            "Epoch 25 has loss 10.0225\n",
            "Epoch 26 has loss 9.9968\n",
            "Epoch 27 has loss 9.9789\n",
            "Epoch 28 has loss 10.0223\n",
            "Epoch 29 has loss 9.9962\n",
            "Epoch 30 has loss 9.9558\n",
            "Epoch 31 has loss 9.9546\n",
            "Epoch 32 has loss 9.9119\n",
            "Epoch 33 has loss 9.8926\n",
            "Epoch 34 has loss 9.8534\n",
            "Epoch 35 has loss 9.8507\n",
            "Epoch 36 has loss 9.8097\n",
            "Epoch 37 has loss 9.7853\n",
            "Epoch 38 has loss 9.7563\n",
            "Epoch 39 has loss 9.7732\n",
            "Epoch 40 has loss 9.7346\n",
            "Epoch 41 has loss 9.6985\n",
            "Epoch 42 has loss 9.6462\n",
            "Epoch 43 has loss 9.5764\n",
            "Epoch 44 has loss 9.5582\n",
            "Epoch 45 has loss 9.556\n",
            "Epoch 46 has loss 9.5676\n",
            "Epoch 47 has loss 9.5318\n",
            "Epoch 48 has loss 9.5377\n",
            "Epoch 49 has loss 9.521\n",
            "📉 Epoch 0, Loss: 20.1155\n",
            "📉 Epoch 1, Loss: 17.0899\n",
            "📉 Epoch 2, Loss: 16.8417\n",
            "📉 Epoch 3, Loss: 15.9470\n",
            "📉 Epoch 4, Loss: 15.8611\n",
            "📉 Epoch 5, Loss: 15.7767\n",
            "📉 Epoch 6, Loss: 15.8035\n",
            "📉 Epoch 7, Loss: 15.9227\n",
            "📉 Epoch 8, Loss: 15.7595\n",
            "📉 Epoch 9, Loss: 15.7114\n",
            "📉 Epoch 10, Loss: 15.7145\n",
            "📉 Epoch 11, Loss: 15.7291\n",
            "📉 Epoch 12, Loss: 15.6551\n",
            "📉 Epoch 13, Loss: 15.5764\n",
            "📉 Epoch 14, Loss: 15.6182\n",
            "📉 Epoch 15, Loss: 15.9614\n",
            "📉 Epoch 16, Loss: 15.7950\n",
            "📉 Epoch 17, Loss: 15.5691\n",
            "📉 Epoch 18, Loss: 15.5593\n",
            "📉 Epoch 19, Loss: 15.7988\n",
            "📉 Epoch 20, Loss: 15.7086\n",
            "📉 Epoch 21, Loss: 15.5386\n",
            "📉 Epoch 22, Loss: 16.6566\n",
            "📉 Epoch 23, Loss: 16.9454\n",
            "📉 Epoch 24, Loss: 15.7697\n",
            "📉 Epoch 25, Loss: 15.6580\n",
            "📉 Epoch 26, Loss: 15.5684\n",
            "📉 Epoch 27, Loss: 15.5824\n",
            "📉 Epoch 28, Loss: 15.6449\n",
            "📉 Epoch 29, Loss: 15.6252\n",
            "📉 Epoch 30, Loss: 15.5373\n",
            "📉 Epoch 31, Loss: 15.4897\n",
            "📉 Epoch 32, Loss: 15.5037\n",
            "📉 Epoch 33, Loss: 15.5053\n",
            "📉 Epoch 34, Loss: 15.5509\n",
            "📉 Epoch 35, Loss: 15.5325\n",
            "📉 Epoch 36, Loss: 15.5492\n",
            "📉 Epoch 37, Loss: 15.4644\n",
            "📉 Epoch 38, Loss: 15.4614\n",
            "📉 Epoch 39, Loss: 15.5223\n",
            "📉 Epoch 40, Loss: 15.4589\n",
            "📉 Epoch 41, Loss: 15.3806\n",
            "📉 Epoch 42, Loss: 15.4095\n",
            "📉 Epoch 43, Loss: 15.4854\n",
            "📉 Epoch 44, Loss: 15.5211\n",
            "📉 Epoch 45, Loss: 15.4034\n",
            "📉 Epoch 46, Loss: 15.4610\n",
            "📉 Epoch 47, Loss: 15.4268\n",
            "📉 Epoch 48, Loss: 15.4991\n",
            "📉 Epoch 49, Loss: 15.7563\n",
            "  7 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  423 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2230 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3084 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  3929 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📗 Resultados SkipConnSAGE (embedding v3 como capa final):\n",
            "Hits@10: 0.0001\n",
            "Hits@20: 0.0032\n",
            "Hits@30: 0.0167\n",
            "Hits@40: 0.0231\n",
            "Hits@50: 0.0294\n",
            "Accuracy: 0.7808\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_v4 = model(emb, adj_t)\n",
        "\n",
        "predictor = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_v4.size(1),\n",
        "    hidden_channels=z_skip_v4.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v4.shape[0]\n",
        ").to(device)\n",
        "predictor.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "predictor.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor(\n",
        "            z_skip_v4[edge[0]], z_skip_v4[edge[1]],\n",
        "            embedding_global_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor, embedding_global_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skip_final_v4 = test(dummy_model, wrapped_predictor, z_skip_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📗 Resultados SkipConnSAGE (embedding v3 como capa final):\")\n",
        "for k, v in results_skip_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.0108\n",
            "Epoch 1 has loss 11.7838\n",
            "Epoch 2 has loss 11.7839\n",
            "Epoch 3 has loss 11.7477\n",
            "Epoch 4 has loss 22.7771\n",
            "Epoch 5 has loss 10.3063\n",
            "Epoch 6 has loss 9.7799\n",
            "Epoch 7 has loss 8.556\n",
            "Epoch 8 has loss 7.9577\n",
            "Epoch 9 has loss 7.662\n",
            "Epoch 10 has loss 7.5699\n",
            "Epoch 11 has loss 7.4893\n",
            "Epoch 12 has loss 7.5607\n",
            "Epoch 13 has loss 7.496\n",
            "Epoch 14 has loss 7.4347\n",
            "Epoch 15 has loss 7.4428\n",
            "Epoch 16 has loss 7.4556\n",
            "Epoch 17 has loss 7.433\n",
            "Epoch 18 has loss 7.3833\n",
            "Epoch 19 has loss 7.3698\n",
            "Epoch 20 has loss 7.3551\n",
            "Epoch 21 has loss 7.4293\n",
            "Epoch 22 has loss 7.4021\n",
            "Epoch 23 has loss 7.3688\n",
            "Epoch 24 has loss 7.3947\n",
            "Epoch 25 has loss 7.3658\n",
            "Epoch 26 has loss 7.3764\n",
            "Epoch 27 has loss 7.3719\n",
            "Epoch 28 has loss 7.3454\n",
            "Epoch 29 has loss 7.3089\n",
            "Epoch 30 has loss 7.3478\n",
            "Epoch 31 has loss 7.3476\n",
            "Epoch 32 has loss 7.3019\n",
            "Epoch 33 has loss 7.3073\n",
            "Epoch 34 has loss 7.279\n",
            "Epoch 35 has loss 7.2647\n",
            "Epoch 36 has loss 7.2332\n",
            "Epoch 37 has loss 7.242\n",
            "Epoch 38 has loss 7.809\n",
            "Epoch 39 has loss 7.3696\n",
            "Epoch 40 has loss 7.2767\n",
            "Epoch 41 has loss 7.2947\n",
            "Epoch 42 has loss 7.2597\n",
            "Epoch 43 has loss 7.3376\n",
            "Epoch 44 has loss 7.1998\n",
            "Epoch 45 has loss 6.8793\n",
            "Epoch 46 has loss 6.6228\n",
            "Epoch 47 has loss 6.2458\n",
            "Epoch 48 has loss 6.4247\n",
            "Epoch 49 has loss 6.2833\n",
            "  1919 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2790 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  3246 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3614 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4260 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE (Prompt V3, sin BERT):\n",
            "Hits@10: 0.0144\n",
            "Hits@20: 0.0209\n",
            "Hits@30: 0.0243\n",
            "Hits@40: 0.0271\n",
            "Hits@50: 0.0319\n",
            "Accuracy: 0.8050\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE (Prompt V3, sin BERT):\")\n",
        "for k, v in results_post_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.064\n",
            "Epoch 1 has loss 14.2043\n",
            "Epoch 2 has loss 10.3431\n",
            "Epoch 3 has loss 9.6399\n",
            "Epoch 4 has loss 8.9594\n",
            "Epoch 5 has loss 8.6609\n",
            "Epoch 6 has loss 8.3414\n",
            "Epoch 7 has loss 8.5378\n",
            "Epoch 8 has loss 7.7445\n",
            "Epoch 9 has loss 7.5741\n",
            "Epoch 10 has loss 7.196\n",
            "Epoch 11 has loss 7.0672\n",
            "Epoch 12 has loss 6.7801\n",
            "Epoch 13 has loss 6.5404\n",
            "Epoch 14 has loss 6.5015\n",
            "Epoch 15 has loss 6.4684\n",
            "Epoch 16 has loss 6.2188\n",
            "Epoch 17 has loss 6.7078\n",
            "Epoch 18 has loss 6.3561\n",
            "Epoch 19 has loss 6.3298\n",
            "Epoch 20 has loss 6.1116\n",
            "Epoch 21 has loss 6.0215\n",
            "Epoch 22 has loss 6.0951\n",
            "Epoch 23 has loss 5.9834\n",
            "Epoch 24 has loss 5.7841\n",
            "Epoch 25 has loss 5.7804\n",
            "Epoch 26 has loss 5.7201\n",
            "Epoch 27 has loss 5.7653\n",
            "Epoch 28 has loss 5.4929\n",
            "Epoch 29 has loss 5.6528\n",
            "Epoch 30 has loss 5.3984\n",
            "Epoch 31 has loss 5.9336\n",
            "Epoch 32 has loss 5.875\n",
            "Epoch 33 has loss 5.8416\n",
            "Epoch 34 has loss 5.6093\n",
            "Epoch 35 has loss 5.5026\n",
            "Epoch 36 has loss 5.3589\n",
            "Epoch 37 has loss 5.2282\n",
            "Epoch 38 has loss 5.2873\n",
            "Epoch 39 has loss 5.1098\n",
            "Epoch 40 has loss 5.1955\n",
            "Epoch 41 has loss 5.1338\n",
            "Epoch 42 has loss 5.1478\n",
            "Epoch 43 has loss 5.0171\n",
            "Epoch 44 has loss 5.1225\n",
            "Epoch 45 has loss 5.3163\n",
            "Epoch 46 has loss 4.9395\n",
            "Epoch 47 has loss 4.945\n",
            "Epoch 48 has loss 5.0171\n",
            "Epoch 49 has loss 4.9674\n",
            "  22026 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  25464 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26811 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  28643 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  29728 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧩 Resultados PostProcessSAGE (Prompt V3, con BERT como input):\n",
            "Hits@10: 0.1650\n",
            "Hits@20: 0.1908\n",
            "Hits@30: 0.2008\n",
            "Hits@40: 0.2146\n",
            "Hits@50: 0.2227\n",
            "Accuracy: 0.8442\n"
          ]
        }
      ],
      "source": [
        "aug_emb_v4 = torch.cat([emb, embedding_global_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb_v4.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bert_input_v4 = test(model, predictor, aug_emb_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧩 Resultados PostProcessSAGE (Prompt V3, con BERT como input):\")\n",
        "for k, v in results_post_bert_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8855\n",
            "Epoch 1 has loss 11.8465\n",
            "Epoch 2 has loss 11.8337\n",
            "Epoch 3 has loss 11.7291\n",
            "Epoch 4 has loss 11.1443\n",
            "Epoch 5 has loss 11.037\n",
            "Epoch 6 has loss 11.21\n",
            "Epoch 7 has loss 10.8193\n",
            "Epoch 8 has loss 10.7583\n",
            "Epoch 9 has loss 10.7279\n",
            "Epoch 10 has loss 9.7389\n",
            "Epoch 11 has loss 9.4526\n",
            "Epoch 12 has loss 9.3344\n",
            "Epoch 13 has loss 9.2853\n",
            "Epoch 14 has loss 9.2479\n",
            "Epoch 15 has loss 9.1408\n",
            "Epoch 16 has loss 9.0659\n",
            "Epoch 17 has loss 9.105\n",
            "Epoch 18 has loss 9.0603\n",
            "Epoch 19 has loss 9.0246\n",
            "Epoch 20 has loss 9.0491\n",
            "Epoch 21 has loss 9.0567\n",
            "Epoch 22 has loss 9.477\n",
            "Epoch 23 has loss 9.5206\n",
            "Epoch 24 has loss 8.9854\n",
            "Epoch 25 has loss 8.8873\n",
            "Epoch 26 has loss 8.7744\n",
            "Epoch 27 has loss 8.7443\n",
            "Epoch 28 has loss 8.6677\n",
            "Epoch 29 has loss 8.8039\n",
            "Epoch 30 has loss 8.7867\n",
            "Epoch 31 has loss 8.6599\n",
            "Epoch 32 has loss 8.6431\n",
            "Epoch 33 has loss 8.6412\n",
            "Epoch 34 has loss 8.5769\n",
            "Epoch 35 has loss 8.7495\n",
            "Epoch 36 has loss 8.5826\n",
            "Epoch 37 has loss 8.627\n",
            "Epoch 38 has loss 8.5943\n",
            "Epoch 39 has loss 8.6436\n",
            "Epoch 40 has loss 8.5311\n",
            "Epoch 41 has loss 8.5806\n",
            "Epoch 42 has loss 8.5162\n",
            "Epoch 43 has loss 8.4615\n",
            "Epoch 44 has loss 8.4429\n",
            "Epoch 45 has loss 8.4654\n",
            "Epoch 46 has loss 8.4712\n",
            "Epoch 47 has loss 8.4137\n",
            "Epoch 48 has loss 8.2834\n",
            "Epoch 49 has loss 8.3228\n",
            "📉 Epoch 0, Loss: 21.1324\n",
            "📉 Epoch 1, Loss: 19.2671\n",
            "📉 Epoch 2, Loss: 18.5122\n",
            "📉 Epoch 3, Loss: 18.1375\n",
            "📉 Epoch 4, Loss: 17.9501\n",
            "📉 Epoch 5, Loss: 17.8628\n",
            "📉 Epoch 6, Loss: 17.6223\n",
            "📉 Epoch 7, Loss: 17.1445\n",
            "📉 Epoch 8, Loss: 17.6399\n",
            "📉 Epoch 9, Loss: 15.2495\n",
            "📉 Epoch 10, Loss: 12.9123\n",
            "📉 Epoch 11, Loss: 12.8154\n",
            "📉 Epoch 12, Loss: 12.7830\n",
            "📉 Epoch 13, Loss: 12.7969\n",
            "📉 Epoch 14, Loss: 12.8163\n",
            "📉 Epoch 15, Loss: 12.7681\n",
            "📉 Epoch 16, Loss: 12.7066\n",
            "📉 Epoch 17, Loss: 12.7403\n",
            "📉 Epoch 18, Loss: 12.7163\n",
            "📉 Epoch 19, Loss: 12.7435\n",
            "📉 Epoch 20, Loss: 12.6742\n",
            "📉 Epoch 21, Loss: 12.6901\n",
            "📉 Epoch 22, Loss: 12.6971\n",
            "📉 Epoch 23, Loss: 12.7134\n",
            "📉 Epoch 24, Loss: 12.6885\n",
            "📉 Epoch 25, Loss: 12.6696\n",
            "📉 Epoch 26, Loss: 12.6524\n",
            "📉 Epoch 27, Loss: 12.6205\n",
            "📉 Epoch 28, Loss: 12.6234\n",
            "📉 Epoch 29, Loss: 12.6262\n",
            "📉 Epoch 30, Loss: 12.6137\n",
            "📉 Epoch 31, Loss: 12.6312\n",
            "📉 Epoch 32, Loss: 12.6665\n",
            "📉 Epoch 33, Loss: 12.6407\n",
            "📉 Epoch 34, Loss: 12.6160\n",
            "📉 Epoch 35, Loss: 12.5916\n",
            "📉 Epoch 36, Loss: 12.5902\n",
            "📉 Epoch 37, Loss: 12.5775\n",
            "📉 Epoch 38, Loss: 12.5905\n",
            "📉 Epoch 39, Loss: 12.5998\n",
            "📉 Epoch 40, Loss: 12.5749\n",
            "📉 Epoch 41, Loss: 12.5699\n",
            "📉 Epoch 42, Loss: 12.5559\n",
            "📉 Epoch 43, Loss: 12.5618\n",
            "📉 Epoch 44, Loss: 12.5468\n",
            "📉 Epoch 45, Loss: 12.5561\n",
            "📉 Epoch 46, Loss: 12.5466\n",
            "📉 Epoch 47, Loss: 12.5402\n",
            "📉 Epoch 48, Loss: 12.5411\n",
            "📉 Epoch 49, Loss: 12.5988\n",
            "  4431 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  6741 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  10002 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  11182 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  12296 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados PostProcessSAGE (Prompt V3, con BERT como señal final):\n",
            "Hits@10: 0.0332\n",
            "Hits@20: 0.0505\n",
            "Hits@30: 0.0749\n",
            "Hits@40: 0.0838\n",
            "Hits@50: 0.0921\n",
            "Accuracy: 0.8423\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_bert_final_v4 = model(emb, adj_t)\n",
        "\n",
        "predictor_post_bert_final_v4 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_bert_final_v4.size(1),\n",
        "    hidden_channels=z_post_bert_final_v4.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_global_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_post_bert_final_v4.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_bert_final_v4.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_bert_final_v4.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in torch.utils.data.DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_bert_final_v4(\n",
        "            z_post_bert_final_v4[edge[0]], z_post_bert_final_v4[edge[1]],\n",
        "            embedding_global_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_post_bert_final_v4, embedding_global_v4)\n",
        "dummy_model = DummyModel()\n",
        "results_post_bert_final_v4 = test(dummy_model, wrapped_predictor, z_post_bert_final_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados PostProcessSAGE (Prompt V3, con BERT como señal final):\")\n",
        "for k, v in results_post_bert_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.004180</td>\n",
              "      <td>0.008945</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>0.7712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.7928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + BERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.003169</td>\n",
              "      <td>0.016705</td>\n",
              "      <td>0.023103</td>\n",
              "      <td>0.029433</td>\n",
              "      <td>0.7808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.014376</td>\n",
              "      <td>0.020901</td>\n",
              "      <td>0.024317</td>\n",
              "      <td>0.027073</td>\n",
              "      <td>0.031913</td>\n",
              "      <td>0.8050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + BERT</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.165002</td>\n",
              "      <td>0.190757</td>\n",
              "      <td>0.200848</td>\n",
              "      <td>0.214572</td>\n",
              "      <td>0.222700</td>\n",
              "      <td>0.8442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model Embedding Source                       Prompt Type  \\\n",
              "0          SAGEConv          classic                                 -   \n",
              "1          SAGEConv    classic + GPT           reasoning-based summary   \n",
              "2          SAGEConv          classic           reasoning-based summary   \n",
              "3      SkipConnSAGE          classic                                 -   \n",
              "4      SkipConnSAGE    classic + GPT           reasoning-based summary   \n",
              "..              ...              ...                               ...   \n",
              "57     SkipConnSAGE          classic                                 -   \n",
              "58     SkipConnSAGE   classic + BERT  structural descriptive expansion   \n",
              "59     SkipConnSAGE          classic  structural descriptive expansion   \n",
              "60  PostProcessSAGE          classic                                 -   \n",
              "61  PostProcessSAGE   classic + BERT  structural descriptive expansion   \n",
              "\n",
              "   GPT Injection      Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -         Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input         Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final   Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -         Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input         Neural     100  0.000809  0.001049  0.001288   \n",
              "..           ...            ...     ...       ...       ...       ...   \n",
              "57             -         Neural      50  0.000884  0.001873  0.004180   \n",
              "58         input         Neural      50  0.000007  0.000015  0.000449   \n",
              "59         final  Neural + BERT      50  0.000052  0.003169  0.016705   \n",
              "60             -         Neural      50  0.014376  0.020901  0.024317   \n",
              "61         input         Neural      50  0.165002  0.190757  0.200848   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "..       ...       ...       ...  \n",
              "57  0.008945  0.016398    0.7712  \n",
              "58  0.001453  0.002644    0.7928  \n",
              "59  0.023103  0.029433    0.7808  \n",
              "60  0.027073  0.031913    0.8050  \n",
              "61  0.214572  0.222700    0.8442  \n",
              "\n",
              "[62 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "third_prompt_type = \"structural descriptive expansion\"\n",
        "\n",
        "new_rows = [\n",
        "    # SAGEConv + BERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_base_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_input_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_sage_final_v4},\n",
        "\n",
        "    # SkipConnSAGE + BERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_base_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_input_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_skip_final_v4},\n",
        "\n",
        "    # PostProcessSAGE + BERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_base_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + BERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bert_input_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BERT\", \"Epochs\": 50, **results_post_bert_final_v4},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "import IPython.display as display\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary.to_csv(\"results_summary.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /Users/matigasstron/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Usamos el mismo modelo que en la primera iteración para SciBERT\n",
        "model_sci = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "embedding_scibert_v4 = model_sci.encode(gpt_response_text_v4)\n",
        "\n",
        "embedding_scibert_tensor_v4 = torch.tensor(embedding_scibert_v4, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.4745\n",
            "Epoch 1 has loss 9.3267\n",
            "Epoch 2 has loss 8.2533\n",
            "Epoch 3 has loss 8.0883\n",
            "Epoch 4 has loss 7.7554\n",
            "Epoch 5 has loss 7.638\n",
            "Epoch 6 has loss 7.4195\n",
            "Epoch 7 has loss 7.2509\n",
            "Epoch 8 has loss 6.8789\n",
            "Epoch 9 has loss 6.396\n",
            "Epoch 10 has loss 6.6422\n",
            "Epoch 11 has loss 6.1319\n",
            "Epoch 12 has loss 5.9739\n",
            "Epoch 13 has loss 5.8297\n",
            "Epoch 14 has loss 5.6472\n",
            "Epoch 15 has loss 5.5234\n",
            "Epoch 16 has loss 5.5329\n",
            "Epoch 17 has loss 5.4285\n",
            "Epoch 18 has loss 5.2768\n",
            "Epoch 19 has loss 5.3353\n",
            "Epoch 20 has loss 5.2379\n",
            "Epoch 21 has loss 5.1469\n",
            "Epoch 22 has loss 5.1413\n",
            "Epoch 23 has loss 5.1006\n",
            "Epoch 24 has loss 5.1318\n",
            "Epoch 25 has loss 4.9579\n",
            "Epoch 26 has loss 4.8759\n",
            "Epoch 27 has loss 4.8015\n",
            "Epoch 28 has loss 4.7344\n",
            "Epoch 29 has loss 4.7412\n",
            "Epoch 30 has loss 4.707\n",
            "Epoch 31 has loss 4.6167\n",
            "Epoch 32 has loss 4.558\n",
            "Epoch 33 has loss 4.5572\n",
            "Epoch 34 has loss 4.5706\n",
            "Epoch 35 has loss 4.4273\n",
            "Epoch 36 has loss 4.4485\n",
            "Epoch 37 has loss 4.4168\n",
            "Epoch 38 has loss 4.4475\n",
            "Epoch 39 has loss 4.3986\n",
            "Epoch 40 has loss 4.4549\n",
            "Epoch 41 has loss 4.4593\n",
            "Epoch 42 has loss 4.2683\n",
            "Epoch 43 has loss 4.3067\n",
            "Epoch 44 has loss 4.2314\n",
            "Epoch 45 has loss 4.2218\n",
            "Epoch 46 has loss 4.1794\n",
            "Epoch 47 has loss 4.241\n",
            "Epoch 48 has loss 4.134\n",
            "Epoch 49 has loss 4.0088\n",
            "  15645 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  23531 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26802 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  30683 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  34021 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (sin GPT v3):\n",
            "Hits@10: 0.1172\n",
            "Hits@20: 0.1763\n",
            "Hits@30: 0.2008\n",
            "Hits@40: 0.2299\n",
            "Hits@50: 0.2549\n",
            "Accuracy: 0.9067\n"
          ]
        }
      ],
      "source": [
        "model_sci_base = SAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=3,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_sci_base.reset_parameters()\n",
        "predictor_sci_base.reset_parameters()\n",
        "\n",
        "optimizer_sci_base = torch.optim.Adam(\n",
        "    list(model_sci_base.parameters()) + list(predictor_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_sci_base, predictor_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer_sci_base, 64 * 1024, 50)\n",
        "\n",
        "results_sci_base_v4 = test(model_sci_base, predictor_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (sin GPT v3):\")\n",
        "for k, v in results_sci_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6845\n",
            "Epoch 1 has loss 10.4272\n",
            "Epoch 2 has loss 9.7214\n",
            "Epoch 3 has loss 8.8142\n",
            "Epoch 4 has loss 8.4367\n",
            "Epoch 5 has loss 7.6926\n",
            "Epoch 6 has loss 7.5153\n",
            "Epoch 7 has loss 7.3858\n",
            "Epoch 8 has loss 6.8047\n",
            "Epoch 9 has loss 6.4831\n",
            "Epoch 10 has loss 6.2901\n",
            "Epoch 11 has loss 6.4411\n",
            "Epoch 12 has loss 6.3312\n",
            "Epoch 13 has loss 6.5315\n",
            "Epoch 14 has loss 6.2789\n",
            "Epoch 15 has loss 6.1835\n",
            "Epoch 16 has loss 6.0597\n",
            "Epoch 17 has loss 5.9506\n",
            "Epoch 18 has loss 5.8384\n",
            "Epoch 19 has loss 5.7892\n",
            "Epoch 20 has loss 5.8959\n",
            "Epoch 21 has loss 6.0731\n",
            "Epoch 22 has loss 5.812\n",
            "Epoch 23 has loss 5.5736\n",
            "Epoch 24 has loss 5.4068\n",
            "Epoch 25 has loss 5.3248\n",
            "Epoch 26 has loss 5.1029\n",
            "Epoch 27 has loss 4.9843\n",
            "Epoch 28 has loss 4.9493\n",
            "Epoch 29 has loss 4.8849\n",
            "Epoch 30 has loss 4.8252\n",
            "Epoch 31 has loss 4.8483\n",
            "Epoch 32 has loss 4.7661\n",
            "Epoch 33 has loss 4.7753\n",
            "Epoch 34 has loss 4.795\n",
            "Epoch 35 has loss 4.7031\n",
            "Epoch 36 has loss 4.702\n",
            "Epoch 37 has loss 4.6355\n",
            "Epoch 38 has loss 4.5825\n",
            "Epoch 39 has loss 4.6615\n",
            "Epoch 40 has loss 4.58\n",
            "Epoch 41 has loss 4.5117\n",
            "Epoch 42 has loss 4.5257\n",
            "Epoch 43 has loss 4.4489\n",
            "Epoch 44 has loss 4.3496\n",
            "Epoch 45 has loss 4.4317\n",
            "Epoch 46 has loss 4.6023\n",
            "Epoch 47 has loss 4.9006\n",
            "Epoch 48 has loss 4.6168\n",
            "Epoch 49 has loss 4.4836\n",
            "  27932 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  31773 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  35195 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  37022 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  39585 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (como input con prompt v3):\n",
            "Hits@10: 0.2092\n",
            "Hits@20: 0.2380\n",
            "Hits@30: 0.2637\n",
            "Hits@40: 0.2773\n",
            "Hits@50: 0.2965\n",
            "Accuracy: 0.8904\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci_v4 = torch.cat([\n",
        "    emb,  \n",
        "    embedding_scibert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)\n",
        "], dim=1)\n",
        "\n",
        "model_sci_input = SAGE(\n",
        "    in_channels=aug_emb_sci_v4.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_sci_input.reset_parameters()\n",
        "predictor_sci_input.reset_parameters()\n",
        "\n",
        "optimizer_sci_input = torch.optim.Adam(\n",
        "    list(model_sci_input.parameters()) + list(predictor_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_sci_input, predictor_sci_input, aug_emb_sci_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer_sci_input, 64 * 1024, 50)\n",
        "\n",
        "results_sci_input_v4 = test(model_sci_input, predictor_sci_input, aug_emb_sci_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (como input con prompt v3):\")\n",
        "for k, v in results_sci_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6814\n",
            "Epoch 1 has loss 10.2543\n",
            "Epoch 2 has loss 10.1991\n",
            "Epoch 3 has loss 10.1761\n",
            "Epoch 4 has loss 10.0973\n",
            "Epoch 5 has loss 9.9383\n",
            "Epoch 6 has loss 9.8134\n",
            "Epoch 7 has loss 9.7112\n",
            "Epoch 8 has loss 9.6733\n",
            "Epoch 9 has loss 9.5529\n",
            "Epoch 10 has loss 9.5314\n",
            "Epoch 11 has loss 9.5133\n",
            "Epoch 12 has loss 9.5349\n",
            "Epoch 13 has loss 9.5412\n",
            "Epoch 14 has loss 9.4785\n",
            "Epoch 15 has loss 9.464\n",
            "Epoch 16 has loss 9.4559\n",
            "Epoch 17 has loss 9.4496\n",
            "Epoch 18 has loss 9.4489\n",
            "Epoch 19 has loss 9.4323\n",
            "Epoch 20 has loss 9.4476\n",
            "Epoch 21 has loss 9.4461\n",
            "Epoch 22 has loss 9.4275\n",
            "Epoch 23 has loss 9.4055\n",
            "Epoch 24 has loss 9.4108\n",
            "Epoch 25 has loss 9.4356\n",
            "Epoch 26 has loss 9.4626\n",
            "Epoch 27 has loss 9.4191\n",
            "Epoch 28 has loss 9.4098\n",
            "Epoch 29 has loss 9.4026\n",
            "Epoch 30 has loss 9.4347\n",
            "Epoch 31 has loss 9.4091\n",
            "Epoch 32 has loss 9.4019\n",
            "Epoch 33 has loss 9.4035\n",
            "Epoch 34 has loss 9.4066\n",
            "Epoch 35 has loss 9.3888\n",
            "Epoch 36 has loss 9.3953\n",
            "Epoch 37 has loss 9.4268\n",
            "Epoch 38 has loss 9.3947\n",
            "Epoch 39 has loss 9.399\n",
            "Epoch 40 has loss 9.3749\n",
            "Epoch 41 has loss 9.4053\n",
            "Epoch 42 has loss 9.4178\n",
            "Epoch 43 has loss 9.4036\n",
            "Epoch 44 has loss 9.3821\n",
            "Epoch 45 has loss 9.3853\n",
            "Epoch 46 has loss 9.3913\n",
            "Epoch 47 has loss 9.3775\n",
            "Epoch 48 has loss 9.3814\n",
            "Epoch 49 has loss 9.3795\n",
            "📉 Epoch 0, Loss: 21.2685\n",
            "📉 Epoch 1, Loss: 14.5499\n",
            "📉 Epoch 2, Loss: 13.5435\n",
            "📉 Epoch 3, Loss: 12.8339\n",
            "📉 Epoch 4, Loss: 12.4164\n",
            "📉 Epoch 5, Loss: 12.2644\n",
            "📉 Epoch 6, Loss: 12.0664\n",
            "📉 Epoch 7, Loss: 11.6767\n",
            "📉 Epoch 8, Loss: 12.3749\n",
            "📉 Epoch 9, Loss: 11.6577\n",
            "📉 Epoch 10, Loss: 11.6871\n",
            "📉 Epoch 11, Loss: 11.4990\n",
            "📉 Epoch 12, Loss: 11.3823\n",
            "📉 Epoch 13, Loss: 11.4004\n",
            "📉 Epoch 14, Loss: 11.5269\n",
            "📉 Epoch 15, Loss: 11.2556\n",
            "📉 Epoch 16, Loss: 11.2046\n",
            "📉 Epoch 17, Loss: 11.0659\n",
            "📉 Epoch 18, Loss: 11.1823\n",
            "📉 Epoch 19, Loss: 11.1472\n",
            "📉 Epoch 20, Loss: 11.0225\n",
            "📉 Epoch 21, Loss: 11.0672\n",
            "📉 Epoch 22, Loss: 11.0415\n",
            "📉 Epoch 23, Loss: 11.0258\n",
            "📉 Epoch 24, Loss: 10.9174\n",
            "📉 Epoch 25, Loss: 10.8711\n",
            "📉 Epoch 26, Loss: 10.9937\n",
            "📉 Epoch 27, Loss: 10.9788\n",
            "📉 Epoch 28, Loss: 10.8787\n",
            "📉 Epoch 29, Loss: 10.8887\n",
            "📉 Epoch 30, Loss: 10.8486\n",
            "📉 Epoch 31, Loss: 10.9149\n",
            "📉 Epoch 32, Loss: 10.8449\n",
            "📉 Epoch 33, Loss: 10.8377\n",
            "📉 Epoch 34, Loss: 10.8064\n",
            "📉 Epoch 35, Loss: 10.9951\n",
            "📉 Epoch 36, Loss: 10.8677\n",
            "📉 Epoch 37, Loss: 10.8044\n",
            "📉 Epoch 38, Loss: 10.7912\n",
            "📉 Epoch 39, Loss: 10.7609\n",
            "📉 Epoch 40, Loss: 10.7338\n",
            "📉 Epoch 41, Loss: 10.7262\n",
            "📉 Epoch 42, Loss: 10.7555\n",
            "📉 Epoch 43, Loss: 11.1440\n",
            "📉 Epoch 44, Loss: 11.0596\n",
            "📉 Epoch 45, Loss: 10.9486\n",
            "📉 Epoch 46, Loss: 10.8614\n",
            "📉 Epoch 47, Loss: 10.7829\n",
            "📉 Epoch 48, Loss: 10.7769\n",
            "📉 Epoch 49, Loss: 10.7383\n",
            "  6822 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  9673 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  11325 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  12482 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  14002 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + SciBERT (GPT final v3):\n",
            "Hits@10: 0.0511\n",
            "Hits@20: 0.0725\n",
            "Hits@30: 0.0848\n",
            "Hits@40: 0.0935\n",
            "Hits@50: 0.1049\n",
            "Accuracy: 0.8925\n"
          ]
        }
      ],
      "source": [
        "model_sci_final = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_sci_final = model_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_sci_final.size(1),\n",
        "    hidden_channels=z_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_sci_final.reset_parameters()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(predictor_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_sci_final.train()  \n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_sci_final(\n",
        "            z_sci_final[edge[0]], z_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_sci_final, embedding_scibert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sci_final_v4 = test(dummy_model, wrapped_predictor, z_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + SciBERT (GPT final v3):\")\n",
        "for k, v in results_sci_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.4254\n",
            "Epoch 1 has loss 10.3119\n",
            "Epoch 2 has loss 9.9925\n",
            "Epoch 3 has loss 9.8927\n",
            "Epoch 4 has loss 9.8019\n",
            "Epoch 5 has loss 11.3976\n",
            "Epoch 6 has loss 11.5633\n",
            "Epoch 7 has loss 10.7361\n",
            "Epoch 8 has loss 10.3372\n",
            "Epoch 9 has loss 9.6355\n",
            "Epoch 10 has loss 9.7896\n",
            "Epoch 11 has loss 9.6526\n",
            "Epoch 12 has loss 9.7516\n",
            "Epoch 13 has loss 9.5632\n",
            "Epoch 14 has loss 9.6112\n",
            "Epoch 15 has loss 9.8585\n",
            "Epoch 16 has loss 9.7235\n",
            "Epoch 17 has loss 9.5694\n",
            "Epoch 18 has loss 9.2296\n",
            "Epoch 19 has loss 9.2955\n",
            "Epoch 20 has loss 9.5925\n",
            "Epoch 21 has loss 9.1757\n",
            "Epoch 22 has loss 8.8705\n",
            "Epoch 23 has loss 8.5599\n",
            "Epoch 24 has loss 8.6641\n",
            "Epoch 25 has loss 8.4465\n",
            "Epoch 26 has loss 8.4279\n",
            "Epoch 27 has loss 8.4338\n",
            "Epoch 28 has loss 8.243\n",
            "Epoch 29 has loss 8.1433\n",
            "Epoch 30 has loss 8.0755\n",
            "Epoch 31 has loss 8.0532\n",
            "Epoch 32 has loss 7.9988\n",
            "Epoch 33 has loss 8.0431\n",
            "Epoch 34 has loss 7.9249\n",
            "Epoch 35 has loss 7.888\n",
            "Epoch 36 has loss 7.9485\n",
            "Epoch 37 has loss 7.877\n",
            "Epoch 38 has loss 7.726\n",
            "Epoch 39 has loss 7.6402\n",
            "Epoch 40 has loss 7.8315\n",
            "Epoch 41 has loss 7.7392\n",
            "Epoch 42 has loss 7.9438\n",
            "Epoch 43 has loss 8.5334\n",
            "Epoch 44 has loss 8.0656\n",
            "Epoch 45 has loss 7.7154\n",
            "Epoch 46 has loss 7.6275\n",
            "Epoch 47 has loss 7.6983\n",
            "Epoch 48 has loss 7.6032\n",
            "Epoch 49 has loss 7.6679\n",
            "  2567 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  3937 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  4629 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  5834 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  6516 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "RESET NUMEROOOOO TRESSSSSSS\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (sin GPT v3):\n",
            "Hits@10: 0.0192\n",
            "Hits@20: 0.0295\n",
            "Hits@30: 0.0347\n",
            "Hits@40: 0.0437\n",
            "Hits@50: 0.0488\n",
            "Accuracy: 0.7465\n"
          ]
        }
      ],
      "source": [
        "model_skip_sci_base = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_skip_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_skip_sci_base.reset_parameters()\n",
        "predictor_skip_sci_base.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_skip_sci_base.parameters()) + list(predictor_skip_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_skip_sci_base, predictor_skip_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_sci_base_v4 = test(model_skip_sci_base, predictor_skip_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"RESET NUMEROOOOO TRESSSSSSS\")\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (sin GPT v3):\")\n",
        "for k, v in results_skip_sci_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.6691\n",
            "Epoch 1 has loss 10.5713\n",
            "Epoch 2 has loss 10.3334\n",
            "Epoch 3 has loss 10.2095\n",
            "Epoch 4 has loss 10.3457\n",
            "Epoch 5 has loss 10.2056\n",
            "Epoch 6 has loss 10.2345\n",
            "Epoch 7 has loss 10.0144\n",
            "Epoch 8 has loss 9.8746\n",
            "Epoch 9 has loss 10.056\n",
            "Epoch 10 has loss 9.7821\n",
            "Epoch 11 has loss 9.427\n",
            "Epoch 12 has loss 9.7118\n",
            "Epoch 13 has loss 9.7092\n",
            "Epoch 14 has loss 9.4547\n",
            "Epoch 15 has loss 9.7168\n",
            "Epoch 16 has loss 9.367\n",
            "Epoch 17 has loss 9.4876\n",
            "Epoch 18 has loss 9.5262\n",
            "Epoch 19 has loss 9.2033\n",
            "Epoch 20 has loss 8.7798\n",
            "Epoch 21 has loss 9.65\n",
            "Epoch 22 has loss 9.1945\n",
            "Epoch 23 has loss 8.9146\n",
            "Epoch 24 has loss 8.8828\n",
            "Epoch 25 has loss 8.746\n",
            "Epoch 26 has loss 8.5672\n",
            "Epoch 27 has loss 9.4839\n",
            "Epoch 28 has loss 9.1324\n",
            "Epoch 29 has loss 8.889\n",
            "Epoch 30 has loss 8.5245\n",
            "Epoch 31 has loss 8.6256\n",
            "Epoch 32 has loss 8.3551\n",
            "Epoch 33 has loss 8.1571\n",
            "Epoch 34 has loss 8.2764\n",
            "Epoch 35 has loss 8.1392\n",
            "Epoch 36 has loss 8.1604\n",
            "Epoch 37 has loss 7.9201\n",
            "Epoch 38 has loss 8.3822\n",
            "Epoch 39 has loss 8.1137\n",
            "Epoch 40 has loss 8.1269\n",
            "Epoch 41 has loss 7.9298\n",
            "Epoch 42 has loss 8.6697\n",
            "Epoch 43 has loss 7.9545\n",
            "Epoch 44 has loss 8.1111\n",
            "Epoch 45 has loss 8.2318\n",
            "Epoch 46 has loss 7.6833\n",
            "Epoch 47 has loss 7.3557\n",
            "Epoch 48 has loss 7.4381\n",
            "Epoch 49 has loss 7.0556\n",
            "  2435 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  3981 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5005 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  6180 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  6815 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (GPT como input v3):\n",
            "Hits@10: 0.0182\n",
            "Hits@20: 0.0298\n",
            "Hits@30: 0.0375\n",
            "Hits@40: 0.0463\n",
            "Hits@50: 0.0511\n",
            "Accuracy: 0.7711\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci_v4 = torch.cat([emb, embedding_scibert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model_skip_sci_input = SkipConnSAGE(\n",
        "    in_channels=aug_emb_sci_v4.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_skip_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "\n",
        "model_skip_sci_input.reset_parameters()\n",
        "predictor_skip_sci_input.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_skip_sci_input.parameters()) + list(predictor_skip_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_skip_sci_input, predictor_skip_sci_input, aug_emb_sci_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skip_sci_inpu_v4 = test(model_skip_sci_input, predictor_skip_sci_input, aug_emb_sci_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (GPT como input v3):\")\n",
        "for k, v in results_skip_sci_inpu_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.2659\n",
            "Epoch 1 has loss 10.7278\n",
            "Epoch 2 has loss 10.4171\n",
            "Epoch 3 has loss 10.313\n",
            "Epoch 4 has loss 10.2795\n",
            "Epoch 5 has loss 10.2994\n",
            "Epoch 6 has loss 10.2293\n",
            "Epoch 7 has loss 10.2202\n",
            "Epoch 8 has loss 10.205\n",
            "Epoch 9 has loss 10.1872\n",
            "Epoch 10 has loss 10.2074\n",
            "Epoch 11 has loss 10.1347\n",
            "Epoch 12 has loss 10.1387\n",
            "Epoch 13 has loss 10.1442\n",
            "Epoch 14 has loss 10.1579\n",
            "Epoch 15 has loss 10.0935\n",
            "Epoch 16 has loss 10.0884\n",
            "Epoch 17 has loss 10.1432\n",
            "Epoch 18 has loss 10.2334\n",
            "Epoch 19 has loss 10.097\n",
            "Epoch 20 has loss 10.0989\n",
            "Epoch 21 has loss 10.1261\n",
            "Epoch 22 has loss 10.102\n",
            "Epoch 23 has loss 10.0886\n",
            "Epoch 24 has loss 10.073\n",
            "Epoch 25 has loss 10.0385\n",
            "Epoch 26 has loss 10.0643\n",
            "Epoch 27 has loss 10.0655\n",
            "Epoch 28 has loss 10.0127\n",
            "Epoch 29 has loss 10.0219\n",
            "Epoch 30 has loss 9.9932\n",
            "Epoch 31 has loss 10.0158\n",
            "Epoch 32 has loss 9.9644\n",
            "Epoch 33 has loss 9.9489\n",
            "Epoch 34 has loss 9.9457\n",
            "Epoch 35 has loss 9.9828\n",
            "Epoch 36 has loss 9.9465\n",
            "Epoch 37 has loss 9.938\n",
            "Epoch 38 has loss 9.941\n",
            "Epoch 39 has loss 9.9068\n",
            "Epoch 40 has loss 9.9618\n",
            "Epoch 41 has loss 9.9104\n",
            "Epoch 42 has loss 9.8634\n",
            "Epoch 43 has loss 9.8871\n",
            "Epoch 44 has loss 9.8841\n",
            "Epoch 45 has loss 9.871\n",
            "Epoch 46 has loss 9.8388\n",
            "Epoch 47 has loss 9.8416\n",
            "Epoch 48 has loss 9.8186\n",
            "Epoch 49 has loss 9.7809\n",
            "📉 Epoch 0, Loss: 25.0467\n",
            "📉 Epoch 1, Loss: 19.6123\n",
            "📉 Epoch 2, Loss: 17.0805\n",
            "📉 Epoch 3, Loss: 16.5175\n",
            "📉 Epoch 4, Loss: 16.4200\n",
            "📉 Epoch 5, Loss: 16.2565\n",
            "📉 Epoch 6, Loss: 16.1264\n",
            "📉 Epoch 7, Loss: 15.9887\n",
            "📉 Epoch 8, Loss: 16.1571\n",
            "📉 Epoch 9, Loss: 15.9242\n",
            "📉 Epoch 10, Loss: 15.8091\n",
            "📉 Epoch 11, Loss: 15.8959\n",
            "📉 Epoch 12, Loss: 15.7194\n",
            "📉 Epoch 13, Loss: 15.7857\n",
            "📉 Epoch 14, Loss: 15.7024\n",
            "📉 Epoch 15, Loss: 15.6557\n",
            "📉 Epoch 16, Loss: 15.6689\n",
            "📉 Epoch 17, Loss: 15.5871\n",
            "📉 Epoch 18, Loss: 15.5700\n",
            "📉 Epoch 19, Loss: 15.4555\n",
            "📉 Epoch 20, Loss: 15.4289\n",
            "📉 Epoch 21, Loss: 15.4104\n",
            "📉 Epoch 22, Loss: 15.5330\n",
            "📉 Epoch 23, Loss: 15.5111\n",
            "📉 Epoch 24, Loss: 15.3757\n",
            "📉 Epoch 25, Loss: 15.2729\n",
            "📉 Epoch 26, Loss: 15.3063\n",
            "📉 Epoch 27, Loss: 15.2858\n",
            "📉 Epoch 28, Loss: 15.3353\n",
            "📉 Epoch 29, Loss: 15.5204\n",
            "📉 Epoch 30, Loss: 15.3373\n",
            "📉 Epoch 31, Loss: 15.2028\n",
            "📉 Epoch 32, Loss: 15.4273\n",
            "📉 Epoch 33, Loss: 15.3932\n",
            "📉 Epoch 34, Loss: 15.4364\n",
            "📉 Epoch 35, Loss: 15.2836\n",
            "📉 Epoch 36, Loss: 15.2678\n",
            "📉 Epoch 37, Loss: 15.1612\n",
            "📉 Epoch 38, Loss: 15.4175\n",
            "📉 Epoch 39, Loss: 15.4913\n",
            "📉 Epoch 40, Loss: 15.3102\n",
            "📉 Epoch 41, Loss: 15.4946\n",
            "📉 Epoch 42, Loss: 15.3122\n",
            "📉 Epoch 43, Loss: 15.2630\n",
            "📉 Epoch 44, Loss: 15.4542\n",
            "📉 Epoch 45, Loss: 15.2536\n",
            "📉 Epoch 46, Loss: 15.5173\n",
            "📉 Epoch 47, Loss: 15.5702\n",
            "📉 Epoch 48, Loss: 15.2785\n",
            "📉 Epoch 49, Loss: 15.6837\n",
            "  437 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  940 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  1555 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  2275 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  3046 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SkipConnSAGE + SciBERT (GPT como final v3):\n",
            "Hits@10: 0.0033\n",
            "Hits@20: 0.0070\n",
            "Hits@30: 0.0116\n",
            "Hits@40: 0.0170\n",
            "Hits@50: 0.0228\n",
            "Accuracy: 0.7878\n"
          ]
        }
      ],
      "source": [
        "model_skip_sci_final = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model_skip_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_skip_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_skip_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_skip_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_skip_sci_final = model_skip_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_skip_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skip_sci_final.size(1),\n",
        "    hidden_channels=z_skip_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "\n",
        "predictor_skip_sci_final.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(predictor_skip_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skip_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skip_sci_final(\n",
        "            z_skip_sci_final[edge[0]], z_skip_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_skip_sci = WrappedPredictor(predictor_skip_sci_final, embedding_scibert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skip_sci_final_v4 = test(dummy_model, wrapped_predictor_skip_sci, z_skip_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SkipConnSAGE + SciBERT (GPT como final v3):\")\n",
        "for k, v in results_skip_sci_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 24.5108\n",
            "Epoch 1 has loss 11.7944\n",
            "Epoch 2 has loss 10.7985\n",
            "Epoch 3 has loss 10.2528\n",
            "Epoch 4 has loss 8.699\n",
            "Epoch 5 has loss 7.9313\n",
            "Epoch 6 has loss 7.7998\n",
            "Epoch 7 has loss 7.6599\n",
            "Epoch 8 has loss 7.4666\n",
            "Epoch 9 has loss 7.3138\n",
            "Epoch 10 has loss 7.1521\n",
            "Epoch 11 has loss 6.9611\n",
            "Epoch 12 has loss 6.7737\n",
            "Epoch 13 has loss 6.7923\n",
            "Epoch 14 has loss 6.6717\n",
            "Epoch 15 has loss 6.3914\n",
            "Epoch 16 has loss 6.2601\n",
            "Epoch 17 has loss 6.0265\n",
            "Epoch 18 has loss 5.8835\n",
            "Epoch 19 has loss 5.9226\n",
            "Epoch 20 has loss 5.9568\n",
            "Epoch 21 has loss 5.8196\n",
            "Epoch 22 has loss 5.7038\n",
            "Epoch 23 has loss 5.7104\n",
            "Epoch 24 has loss 5.545\n",
            "Epoch 25 has loss 5.4302\n",
            "Epoch 26 has loss 5.3857\n",
            "Epoch 27 has loss 5.3578\n",
            "Epoch 28 has loss 5.2184\n",
            "Epoch 29 has loss 5.2727\n",
            "Epoch 30 has loss 5.1029\n",
            "Epoch 31 has loss 5.0675\n",
            "Epoch 32 has loss 5.1255\n",
            "Epoch 33 has loss 5.2485\n",
            "Epoch 34 has loss 5.2165\n",
            "Epoch 35 has loss 4.9449\n",
            "Epoch 36 has loss 4.9655\n",
            "Epoch 37 has loss 4.8386\n",
            "Epoch 38 has loss 5.0054\n",
            "Epoch 39 has loss 4.9623\n",
            "Epoch 40 has loss 4.894\n",
            "Epoch 41 has loss 4.867\n",
            "Epoch 42 has loss 4.8741\n",
            "Epoch 43 has loss 4.848\n",
            "Epoch 44 has loss 4.902\n",
            "Epoch 45 has loss 4.8488\n",
            "Epoch 46 has loss 4.8145\n",
            "Epoch 47 has loss 4.7847\n",
            "Epoch 48 has loss 4.6604\n",
            "Epoch 49 has loss 4.6879\n",
            "  569 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  31507 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  39562 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  41360 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  43651 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (sin GPT v3):\n",
            "Hits@10: 0.0043\n",
            "Hits@20: 0.2360\n",
            "Hits@30: 0.2964\n",
            "Hits@40: 0.3098\n",
            "Hits@50: 0.3270\n",
            "Accuracy: 0.8742\n"
          ]
        }
      ],
      "source": [
        "emb = torch.ones(num_nodes, 1).to(device)\n",
        "\n",
        "model_post_sci_base = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_post_sci_base = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_post_sci_base.reset_parameters()\n",
        "predictor_post_sci_base.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_post_sci_base.parameters()) + list(predictor_post_sci_base.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_post_sci_base, predictor_post_sci_base, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_base_v4 = test(model_post_sci_base, predictor_post_sci_base, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (sin GPT v3):\")\n",
        "for k, v in results_post_sci_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.882\n",
            "Epoch 1 has loss 11.7808\n",
            "Epoch 2 has loss 10.0257\n",
            "Epoch 3 has loss 9.2996\n",
            "Epoch 4 has loss 8.3827\n",
            "Epoch 5 has loss 7.9048\n",
            "Epoch 6 has loss 7.7429\n",
            "Epoch 7 has loss 7.7261\n",
            "Epoch 8 has loss 7.5838\n",
            "Epoch 9 has loss 7.4582\n",
            "Epoch 10 has loss 7.3563\n",
            "Epoch 11 has loss 7.3735\n",
            "Epoch 12 has loss 7.3759\n",
            "Epoch 13 has loss 7.3304\n",
            "Epoch 14 has loss 7.0499\n",
            "Epoch 15 has loss 7.0151\n",
            "Epoch 16 has loss 6.6417\n",
            "Epoch 17 has loss 6.6692\n",
            "Epoch 18 has loss 6.1478\n",
            "Epoch 19 has loss 6.1158\n",
            "Epoch 20 has loss 5.9459\n",
            "Epoch 21 has loss 5.9682\n",
            "Epoch 22 has loss 5.8753\n",
            "Epoch 23 has loss 5.8714\n",
            "Epoch 24 has loss 5.7377\n",
            "Epoch 25 has loss 5.8943\n",
            "Epoch 26 has loss 5.6877\n",
            "Epoch 27 has loss 5.4919\n",
            "Epoch 28 has loss 5.3632\n",
            "Epoch 29 has loss 5.3432\n",
            "Epoch 30 has loss 5.4579\n",
            "Epoch 31 has loss 5.3669\n",
            "Epoch 32 has loss 5.2999\n",
            "Epoch 33 has loss 5.1413\n",
            "Epoch 34 has loss 5.2299\n",
            "Epoch 35 has loss 5.4173\n",
            "Epoch 36 has loss 5.047\n",
            "Epoch 37 has loss 5.0858\n",
            "Epoch 38 has loss 4.9312\n",
            "Epoch 39 has loss 4.8892\n",
            "Epoch 40 has loss 4.8627\n",
            "Epoch 41 has loss 4.9495\n",
            "Epoch 42 has loss 4.7866\n",
            "Epoch 43 has loss 4.7777\n",
            "Epoch 44 has loss 4.7222\n",
            "Epoch 45 has loss 4.6529\n",
            "Epoch 46 has loss 4.5796\n",
            "Epoch 47 has loss 4.7361\n",
            "Epoch 48 has loss 4.8128\n",
            "Epoch 49 has loss 4.6446\n",
            "  25608 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  36110 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  40339 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  43328 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  45290 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (como input v3):\n",
            "Hits@10: 0.1918\n",
            "Hits@20: 0.2705\n",
            "Hits@30: 0.3022\n",
            "Hits@40: 0.3246\n",
            "Hits@50: 0.3393\n",
            "Accuracy: 0.8819\n"
          ]
        }
      ],
      "source": [
        "aug_emb_sci_input_v4 = torch.cat([emb, embedding_scibert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model_post_sci_input = PostProcessSAGE(\n",
        "    in_channels=aug_emb_sci_input_v4.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor_post_sci_input = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_post_sci_input.reset_parameters()\n",
        "predictor_post_sci_input.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model_post_sci_input.parameters()) + list(predictor_post_sci_input.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model_post_sci_input, predictor_post_sci_input, aug_emb_sci_input_v4, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_sci_input_v4 = test(model_post_sci_input, predictor_post_sci_input, aug_emb_sci_input_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (como input v3):\")\n",
        "for k, v in results_post_sci_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.9544\n",
            "Epoch 1 has loss 11.7838\n",
            "Epoch 2 has loss 11.6709\n",
            "Epoch 3 has loss 11.4958\n",
            "Epoch 4 has loss 11.1262\n",
            "Epoch 5 has loss 10.9317\n",
            "Epoch 6 has loss 10.5531\n",
            "Epoch 7 has loss 10.3033\n",
            "Epoch 8 has loss 10.3184\n",
            "Epoch 9 has loss 9.7789\n",
            "Epoch 10 has loss 9.4606\n",
            "Epoch 11 has loss 9.2835\n",
            "Epoch 12 has loss 9.5881\n",
            "Epoch 13 has loss 9.4081\n",
            "Epoch 14 has loss 9.1627\n",
            "Epoch 15 has loss 9.0148\n",
            "Epoch 16 has loss 8.9751\n",
            "Epoch 17 has loss 9.1378\n",
            "Epoch 18 has loss 8.9991\n",
            "Epoch 19 has loss 8.8115\n",
            "Epoch 20 has loss 8.9698\n",
            "Epoch 21 has loss 9.2973\n",
            "Epoch 22 has loss 9.0942\n",
            "Epoch 23 has loss 9.0087\n",
            "Epoch 24 has loss 9.0601\n",
            "Epoch 25 has loss 9.191\n",
            "Epoch 26 has loss 9.2823\n",
            "Epoch 27 has loss 8.8377\n",
            "Epoch 28 has loss 8.8617\n",
            "Epoch 29 has loss 9.0246\n",
            "Epoch 30 has loss 8.7076\n",
            "Epoch 31 has loss 8.9391\n",
            "Epoch 32 has loss 8.8014\n",
            "Epoch 33 has loss 8.6621\n",
            "Epoch 34 has loss 8.6533\n",
            "Epoch 35 has loss 8.6729\n",
            "Epoch 36 has loss 8.5308\n",
            "Epoch 37 has loss 8.6428\n",
            "Epoch 38 has loss 8.615\n",
            "Epoch 39 has loss 8.556\n",
            "Epoch 40 has loss 8.5015\n",
            "Epoch 41 has loss 8.4982\n",
            "Epoch 42 has loss 8.5261\n",
            "Epoch 43 has loss 8.5055\n",
            "Epoch 44 has loss 8.3754\n",
            "Epoch 45 has loss 8.3337\n",
            "Epoch 46 has loss 8.3376\n",
            "Epoch 47 has loss 8.4218\n",
            "Epoch 48 has loss 8.4086\n",
            "Epoch 49 has loss 8.449\n",
            "📉 Epoch 0, Loss: 24.5770\n",
            "📉 Epoch 1, Loss: 19.5850\n",
            "📉 Epoch 2, Loss: 19.0827\n",
            "📉 Epoch 3, Loss: 15.6443\n",
            "📉 Epoch 4, Loss: 13.3789\n",
            "📉 Epoch 5, Loss: 13.1294\n",
            "📉 Epoch 6, Loss: 12.9595\n",
            "📉 Epoch 7, Loss: 12.8897\n",
            "📉 Epoch 8, Loss: 12.8515\n",
            "📉 Epoch 9, Loss: 12.7995\n",
            "📉 Epoch 10, Loss: 12.7817\n",
            "📉 Epoch 11, Loss: 12.7861\n",
            "📉 Epoch 12, Loss: 12.7098\n",
            "📉 Epoch 13, Loss: 12.7095\n",
            "📉 Epoch 14, Loss: 12.6758\n",
            "📉 Epoch 15, Loss: 12.6463\n",
            "📉 Epoch 16, Loss: 12.6164\n",
            "📉 Epoch 17, Loss: 12.6403\n",
            "📉 Epoch 18, Loss: 12.5721\n",
            "📉 Epoch 19, Loss: 12.5817\n",
            "📉 Epoch 20, Loss: 12.5770\n",
            "📉 Epoch 21, Loss: 12.5366\n",
            "📉 Epoch 22, Loss: 12.5346\n",
            "📉 Epoch 23, Loss: 12.5203\n",
            "📉 Epoch 24, Loss: 12.5295\n",
            "📉 Epoch 25, Loss: 12.5364\n",
            "📉 Epoch 26, Loss: 12.5371\n",
            "📉 Epoch 27, Loss: 12.5099\n",
            "📉 Epoch 28, Loss: 12.5007\n",
            "📉 Epoch 29, Loss: 12.4914\n",
            "📉 Epoch 30, Loss: 12.4934\n",
            "📉 Epoch 31, Loss: 12.4880\n",
            "📉 Epoch 32, Loss: 12.5117\n",
            "📉 Epoch 33, Loss: 12.4654\n",
            "📉 Epoch 34, Loss: 12.4619\n",
            "📉 Epoch 35, Loss: 12.4783\n",
            "📉 Epoch 36, Loss: 12.4548\n",
            "📉 Epoch 37, Loss: 12.4484\n",
            "📉 Epoch 38, Loss: 12.4502\n",
            "📉 Epoch 39, Loss: 12.4362\n",
            "📉 Epoch 40, Loss: 12.4448\n",
            "📉 Epoch 41, Loss: 12.4607\n",
            "📉 Epoch 42, Loss: 12.4334\n",
            "📉 Epoch 43, Loss: 12.5520\n",
            "📉 Epoch 44, Loss: 12.4452\n",
            "📉 Epoch 45, Loss: 12.4454\n",
            "📉 Epoch 46, Loss: 12.4392\n",
            "📉 Epoch 47, Loss: 12.4343\n",
            "📉 Epoch 48, Loss: 12.4093\n",
            "📉 Epoch 49, Loss: 12.4139\n",
            "  5265 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  23185 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  26470 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  28912 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  29740 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + SciBERT (GPT final v3):\n",
            "Hits@10: 0.0394\n",
            "Hits@20: 0.1737\n",
            "Hits@30: 0.1983\n",
            "Hits@40: 0.2166\n",
            "Hits@50: 0.2228\n",
            "Accuracy: 0.8473\n"
          ]
        }
      ],
      "source": [
        "model_post_sci_final = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model_post_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model_post_sci_final.parameters(), lr=0.01)\n",
        "\n",
        "train(model_post_sci_final, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model_post_sci_final.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_sci_final = model_post_sci_final(emb, adj_t)\n",
        "\n",
        "predictor_post_sci_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_sci_final.size(1),\n",
        "    hidden_channels=z_post_sci_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_scibert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_post_sci_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_sci_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_sci_final.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_sci_final(\n",
        "            z_post_sci_final[edge[0]], z_post_sci_final[edge[1]],\n",
        "            embedding_scibert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_post_sci = WrappedPredictor(predictor_post_sci_final, embedding_scibert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_sci_final_v4 = test(dummy_model, wrapped_predictor_post_sci, z_post_sci_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + SciBERT (GPT final v3):\")\n",
        "for k, v in results_post_sci_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
        "embedding_biobert_v4 = model.encode(gpt_response_text_v4)\n",
        "embedding_biobert_tensor_v4 = torch.tensor(embedding_biobert_v4, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.796\n",
            "Epoch 1 has loss 9.7388\n",
            "Epoch 2 has loss 8.7285\n",
            "Epoch 3 has loss 7.8293\n",
            "Epoch 4 has loss 7.3975\n",
            "Epoch 5 has loss 7.2612\n",
            "Epoch 6 has loss 7.1543\n",
            "Epoch 7 has loss 7.1659\n",
            "Epoch 8 has loss 7.1661\n",
            "Epoch 9 has loss 6.8226\n",
            "Epoch 10 has loss 6.3791\n",
            "Epoch 11 has loss 6.082\n",
            "Epoch 12 has loss 5.8196\n",
            "Epoch 13 has loss 5.627\n",
            "Epoch 14 has loss 5.3275\n",
            "Epoch 15 has loss 5.2696\n",
            "Epoch 16 has loss 5.0343\n",
            "Epoch 17 has loss 4.9345\n",
            "Epoch 18 has loss 4.8655\n",
            "Epoch 19 has loss 4.9275\n",
            "Epoch 20 has loss 4.8468\n",
            "Epoch 21 has loss 4.8336\n",
            "Epoch 22 has loss 4.7104\n",
            "Epoch 23 has loss 4.679\n",
            "Epoch 24 has loss 4.6135\n",
            "Epoch 25 has loss 4.6204\n",
            "Epoch 26 has loss 4.5593\n",
            "Epoch 27 has loss 4.5216\n",
            "Epoch 28 has loss 4.5347\n",
            "Epoch 29 has loss 4.4224\n",
            "Epoch 30 has loss 4.374\n",
            "Epoch 31 has loss 4.3288\n",
            "Epoch 32 has loss 4.3412\n",
            "Epoch 33 has loss 4.3313\n",
            "Epoch 34 has loss 4.2745\n",
            "Epoch 35 has loss 4.2685\n",
            "Epoch 36 has loss 4.2951\n",
            "Epoch 37 has loss 4.4417\n",
            "Epoch 38 has loss 4.2649\n",
            "Epoch 39 has loss 4.2107\n",
            "Epoch 40 has loss 4.1744\n",
            "Epoch 41 has loss 4.0936\n",
            "Epoch 42 has loss 4.1206\n",
            "Epoch 43 has loss 4.1063\n",
            "Epoch 44 has loss 4.0853\n",
            "Epoch 45 has loss 4.0848\n",
            "Epoch 46 has loss 4.0839\n",
            "Epoch 47 has loss 4.0401\n",
            "Epoch 48 has loss 3.9683\n",
            "Epoch 49 has loss 3.9974\n",
            "  23246 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  31326 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  35586 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  40270 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  42344 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados SAGEConv + BioBERT (sin usar el embedding):\n",
            "Hits@10: 0.1741\n",
            "Hits@20: 0.2347\n",
            "Hits@30: 0.2666\n",
            "Hits@40: 0.3017\n",
            "Hits@50: 0.3172\n",
            "Accuracy: 0.9134\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_bio_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados SAGEConv + BioBERT (sin usar el embedding):\")\n",
        "for k, v in results_sage_bio_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6101\n",
            "Epoch 1 has loss 10.3137\n",
            "Epoch 2 has loss 9.1476\n",
            "Epoch 3 has loss 8.8625\n",
            "Epoch 4 has loss 8.1824\n",
            "Epoch 5 has loss 7.7461\n",
            "Epoch 6 has loss 7.6163\n",
            "Epoch 7 has loss 7.4311\n",
            "Epoch 8 has loss 7.4727\n",
            "Epoch 9 has loss 7.3679\n",
            "Epoch 10 has loss 7.4092\n",
            "Epoch 11 has loss 7.2135\n",
            "Epoch 12 has loss 6.811\n",
            "Epoch 13 has loss 6.7919\n",
            "Epoch 14 has loss 6.2537\n",
            "Epoch 15 has loss 5.9917\n",
            "Epoch 16 has loss 5.9047\n",
            "Epoch 17 has loss 5.7248\n",
            "Epoch 18 has loss 6.0512\n",
            "Epoch 19 has loss 5.7847\n",
            "Epoch 20 has loss 5.6234\n",
            "Epoch 21 has loss 5.4188\n",
            "Epoch 22 has loss 5.613\n",
            "Epoch 23 has loss 5.3045\n",
            "Epoch 24 has loss 5.12\n",
            "Epoch 25 has loss 5.2324\n",
            "Epoch 26 has loss 5.0148\n",
            "Epoch 27 has loss 4.8962\n",
            "Epoch 28 has loss 4.7632\n",
            "Epoch 29 has loss 4.8668\n",
            "Epoch 30 has loss 4.7145\n",
            "Epoch 31 has loss 4.7608\n",
            "Epoch 32 has loss 4.7185\n",
            "Epoch 33 has loss 4.5517\n",
            "Epoch 34 has loss 4.5467\n",
            "Epoch 35 has loss 4.5405\n",
            "Epoch 36 has loss 4.5278\n",
            "Epoch 37 has loss 4.6151\n",
            "Epoch 38 has loss 4.5213\n",
            "Epoch 39 has loss 4.4325\n",
            "Epoch 40 has loss 4.3282\n",
            "Epoch 41 has loss 4.377\n",
            "Epoch 42 has loss 4.3353\n",
            "Epoch 43 has loss 4.231\n",
            "Epoch 44 has loss 4.317\n",
            "Epoch 45 has loss 4.2288\n",
            "Epoch 46 has loss 4.2145\n",
            "Epoch 47 has loss 4.185\n",
            "Epoch 48 has loss 4.1321\n",
            "Epoch 49 has loss 4.1937\n",
            "  28052 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  31769 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  35205 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  38725 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  42443 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SAGEConv + BioBERT (como input):\n",
            "Hits@10: 0.2101\n",
            "Hits@20: 0.2380\n",
            "Hits@30: 0.2637\n",
            "Hits@40: 0.2901\n",
            "Hits@50: 0.3180\n",
            "Accuracy: 0.8999\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_sage_bio_input_v4 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SAGEConv + BioBERT (como input):\")\n",
        "for k, v in results_sage_bio_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.7663\n",
            "Epoch 1 has loss 10.3427\n",
            "Epoch 2 has loss 10.2211\n",
            "Epoch 3 has loss 10.1365\n",
            "Epoch 4 has loss 10.0583\n",
            "Epoch 5 has loss 10.0385\n",
            "Epoch 6 has loss 10.0271\n",
            "Epoch 7 has loss 9.9677\n",
            "Epoch 8 has loss 9.9626\n",
            "Epoch 9 has loss 9.8658\n",
            "Epoch 10 has loss 9.749\n",
            "Epoch 11 has loss 9.6961\n",
            "Epoch 12 has loss 9.5828\n",
            "Epoch 13 has loss 9.5631\n",
            "Epoch 14 has loss 9.5521\n",
            "Epoch 15 has loss 9.5184\n",
            "Epoch 16 has loss 9.5196\n",
            "Epoch 17 has loss 9.5233\n",
            "Epoch 18 has loss 9.4956\n",
            "Epoch 19 has loss 9.5034\n",
            "Epoch 20 has loss 9.507\n",
            "Epoch 21 has loss 9.4879\n",
            "Epoch 22 has loss 9.489\n",
            "Epoch 23 has loss 9.4852\n",
            "Epoch 24 has loss 9.4806\n",
            "Epoch 25 has loss 9.48\n",
            "Epoch 26 has loss 9.484\n",
            "Epoch 27 has loss 9.4626\n",
            "Epoch 28 has loss 9.4667\n",
            "Epoch 29 has loss 9.4548\n",
            "Epoch 30 has loss 9.4588\n",
            "Epoch 31 has loss 9.5118\n",
            "Epoch 32 has loss 9.4746\n",
            "Epoch 33 has loss 9.4634\n",
            "Epoch 34 has loss 9.4633\n",
            "Epoch 35 has loss 9.5203\n",
            "Epoch 36 has loss 9.452\n",
            "Epoch 37 has loss 9.4336\n",
            "Epoch 38 has loss 9.4285\n",
            "Epoch 39 has loss 9.4638\n",
            "Epoch 40 has loss 9.4486\n",
            "Epoch 41 has loss 9.4436\n",
            "Epoch 42 has loss 9.4457\n",
            "Epoch 43 has loss 9.4232\n",
            "Epoch 44 has loss 9.4288\n",
            "Epoch 45 has loss 9.4229\n",
            "Epoch 46 has loss 9.4344\n",
            "Epoch 47 has loss 9.4937\n",
            "Epoch 48 has loss 9.4639\n",
            "Epoch 49 has loss 9.4359\n",
            "📉 Epoch 0, Loss: 22.6879\n",
            "📉 Epoch 1, Loss: 15.5068\n",
            "📉 Epoch 2, Loss: 14.6170\n",
            "📉 Epoch 3, Loss: 13.7233\n",
            "📉 Epoch 4, Loss: 13.2589\n",
            "📉 Epoch 5, Loss: 12.9616\n",
            "📉 Epoch 6, Loss: 12.7887\n",
            "📉 Epoch 7, Loss: 12.7123\n",
            "📉 Epoch 8, Loss: 12.6620\n",
            "📉 Epoch 9, Loss: 12.5023\n",
            "📉 Epoch 10, Loss: 12.5978\n",
            "📉 Epoch 11, Loss: 12.3932\n",
            "📉 Epoch 12, Loss: 12.4345\n",
            "📉 Epoch 13, Loss: 12.3418\n",
            "📉 Epoch 14, Loss: 12.2565\n",
            "📉 Epoch 15, Loss: 12.3891\n",
            "📉 Epoch 16, Loss: 12.2364\n",
            "📉 Epoch 17, Loss: 12.1729\n",
            "📉 Epoch 18, Loss: 12.1595\n",
            "📉 Epoch 19, Loss: 12.1270\n",
            "📉 Epoch 20, Loss: 12.2778\n",
            "📉 Epoch 21, Loss: 12.2696\n",
            "📉 Epoch 22, Loss: 12.1406\n",
            "📉 Epoch 23, Loss: 12.0808\n",
            "📉 Epoch 24, Loss: 12.0394\n",
            "📉 Epoch 25, Loss: 12.0289\n",
            "📉 Epoch 26, Loss: 12.0904\n",
            "📉 Epoch 27, Loss: 12.0674\n",
            "📉 Epoch 28, Loss: 12.0153\n",
            "📉 Epoch 29, Loss: 11.9951\n",
            "📉 Epoch 30, Loss: 12.2578\n",
            "📉 Epoch 31, Loss: 12.0469\n",
            "📉 Epoch 32, Loss: 12.0159\n",
            "📉 Epoch 33, Loss: 11.9675\n",
            "📉 Epoch 34, Loss: 11.9714\n",
            "📉 Epoch 35, Loss: 11.9379\n",
            "📉 Epoch 36, Loss: 11.9296\n",
            "📉 Epoch 37, Loss: 12.0361\n",
            "📉 Epoch 38, Loss: 12.1867\n",
            "📉 Epoch 39, Loss: 11.8907\n",
            "📉 Epoch 40, Loss: 11.8770\n",
            "📉 Epoch 41, Loss: 11.9050\n",
            "📉 Epoch 42, Loss: 11.8769\n",
            "📉 Epoch 43, Loss: 11.8808\n",
            "📉 Epoch 44, Loss: 11.8732\n",
            "📉 Epoch 45, Loss: 11.8622\n",
            "📉 Epoch 46, Loss: 11.9251\n",
            "📉 Epoch 47, Loss: 11.8746\n",
            "📉 Epoch 48, Loss: 11.8842\n",
            "📉 Epoch 49, Loss: 11.8250\n",
            "  5747 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  7761 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  8615 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  9423 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  9886 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🔬 Resultados SAGEConv + BioBERT (en predictor final v3):\n",
            "Hits@10: 0.0431\n",
            "Hits@20: 0.0581\n",
            "Hits@30: 0.0645\n",
            "Hits@40: 0.0706\n",
            "Hits@50: 0.0741\n",
            "Accuracy: 0.8662\n"
          ]
        }
      ],
      "source": [
        "model = SAGE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_bio_final = model(emb, adj_t)\n",
        "\n",
        "predictor_bio_final = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_bio_final.size(1),\n",
        "    hidden_channels=z_bio_final.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_bio_final.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_bio_final.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_bio_final.train()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "    pos_labels = torch.ones(pos_edges.size(0), device=device)\n",
        "\n",
        "    neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "    neg_labels = torch.zeros(neg_edges.size(0), device=device)\n",
        "\n",
        "    edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_bio_final(\n",
        "            z_bio_final[edge[0]], z_bio_final[edge[1]],\n",
        "            embedding_biobert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor_bio = WrappedPredictor(predictor_bio_final, embedding_biobert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_sage_bio_final_v4 = test(dummy_model, wrapped_predictor_bio, z_bio_final, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🔬 Resultados SAGEConv + BioBERT (en predictor final v3):\")\n",
        "for k, v in results_sage_bio_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.656\n",
            "Epoch 1 has loss 10.4995\n",
            "Epoch 2 has loss 10.2172\n",
            "Epoch 3 has loss 10.1507\n",
            "Epoch 4 has loss 9.7129\n",
            "Epoch 5 has loss 9.5015\n",
            "Epoch 6 has loss 9.6422\n",
            "Epoch 7 has loss 9.4112\n",
            "Epoch 8 has loss 9.2922\n",
            "Epoch 9 has loss 9.207\n",
            "Epoch 10 has loss 9.5341\n",
            "Epoch 11 has loss 9.3817\n",
            "Epoch 12 has loss 9.1905\n",
            "Epoch 13 has loss 8.7916\n",
            "Epoch 14 has loss 9.47\n",
            "Epoch 15 has loss 9.1326\n",
            "Epoch 16 has loss 8.9139\n",
            "Epoch 17 has loss 8.8251\n",
            "Epoch 18 has loss 9.1263\n",
            "Epoch 19 has loss 8.8446\n",
            "Epoch 20 has loss 8.9598\n",
            "Epoch 21 has loss 9.1372\n",
            "Epoch 22 has loss 9.6082\n",
            "Epoch 23 has loss 9.0805\n",
            "Epoch 24 has loss 8.5584\n",
            "Epoch 25 has loss 8.3107\n",
            "Epoch 26 has loss 8.2566\n",
            "Epoch 27 has loss 8.5006\n",
            "Epoch 28 has loss 8.5981\n",
            "Epoch 29 has loss 8.4278\n",
            "Epoch 30 has loss 8.3897\n",
            "Epoch 31 has loss 8.4641\n",
            "Epoch 32 has loss 8.1141\n",
            "Epoch 33 has loss 8.3758\n",
            "Epoch 34 has loss 8.3376\n",
            "Epoch 35 has loss 8.2768\n",
            "Epoch 36 has loss 8.0264\n",
            "Epoch 37 has loss 8.3187\n",
            "Epoch 38 has loss 8.1592\n",
            "Epoch 39 has loss 7.9316\n",
            "Epoch 40 has loss 7.9449\n",
            "Epoch 41 has loss 7.8631\n",
            "Epoch 42 has loss 7.9249\n",
            "Epoch 43 has loss 8.3255\n",
            "Epoch 44 has loss 8.1592\n",
            "Epoch 45 has loss 7.8452\n",
            "Epoch 46 has loss 7.9951\n",
            "Epoch 47 has loss 7.5332\n",
            "Epoch 48 has loss 7.2823\n",
            "Epoch 49 has loss 7.0299\n",
            "  6565 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  9206 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  10280 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  11350 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  12067 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE + BioBERT (sin GPT v3):\n",
            "Hits@10: 0.0492\n",
            "Hits@20: 0.0690\n",
            "Hits@30: 0.0770\n",
            "Hits@40: 0.0850\n",
            "Hits@50: 0.0904\n",
            "Accuracy: 0.7783\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE + BioBERT (sin GPT v3):\")\n",
        "for k, v in results_skipconn_bio_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.6301\n",
            "Epoch 1 has loss 10.419\n",
            "Epoch 2 has loss 10.1011\n",
            "Epoch 3 has loss 10.2816\n",
            "Epoch 4 has loss 9.8787\n",
            "Epoch 5 has loss 9.8276\n",
            "Epoch 6 has loss 10.1089\n",
            "Epoch 7 has loss 10.1484\n",
            "Epoch 8 has loss 9.8773\n",
            "Epoch 9 has loss 9.6872\n",
            "Epoch 10 has loss 9.7046\n",
            "Epoch 11 has loss 9.872\n",
            "Epoch 12 has loss 9.8403\n",
            "Epoch 13 has loss 9.4647\n",
            "Epoch 14 has loss 9.6798\n",
            "Epoch 15 has loss 9.5661\n",
            "Epoch 16 has loss 9.437\n",
            "Epoch 17 has loss 9.367\n",
            "Epoch 18 has loss 9.319\n",
            "Epoch 19 has loss 9.0568\n",
            "Epoch 20 has loss 9.0099\n",
            "Epoch 21 has loss 8.8202\n",
            "Epoch 22 has loss 9.1138\n",
            "Epoch 23 has loss 8.5922\n",
            "Epoch 24 has loss 9.1301\n",
            "Epoch 25 has loss 8.8348\n",
            "Epoch 26 has loss 9.2935\n",
            "Epoch 27 has loss 9.587\n",
            "Epoch 28 has loss 9.1895\n",
            "Epoch 29 has loss 8.856\n",
            "Epoch 30 has loss 9.1963\n",
            "Epoch 31 has loss 8.9289\n",
            "Epoch 32 has loss 8.979\n",
            "Epoch 33 has loss 9.3738\n",
            "Epoch 34 has loss 8.9752\n",
            "Epoch 35 has loss 9.0424\n",
            "Epoch 36 has loss 8.8265\n",
            "Epoch 37 has loss 9.0742\n",
            "Epoch 38 has loss 8.8999\n",
            "Epoch 39 has loss 9.0359\n",
            "Epoch 40 has loss 8.8709\n",
            "Epoch 41 has loss 8.9152\n",
            "Epoch 42 has loss 9.3074\n",
            "Epoch 43 has loss 8.8981\n",
            "Epoch 44 has loss 9.2915\n",
            "Epoch 45 has loss 9.603\n",
            "Epoch 46 has loss 8.7834\n",
            "Epoch 47 has loss 8.4865\n",
            "Epoch 48 has loss 8.9486\n",
            "Epoch 49 has loss 8.6378\n",
            "  0 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  0 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  177 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧪 Resultados SkipConnSAGE + BioBERT (como input v3):\n",
            "Hits@10: 0.0000\n",
            "Hits@20: 0.0000\n",
            "Hits@30: 0.0000\n",
            "Hits@40: 0.0000\n",
            "Hits@50: 0.0013\n",
            "Accuracy: 0.7642\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = SkipConnSAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_skipconn_bio_input_v4 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧪 Resultados SkipConnSAGE + BioBERT (como input v3):\")\n",
        "for k, v in results_skipconn_bio_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 12.1496\n",
            "Epoch 1 has loss 10.6072\n",
            "Epoch 2 has loss 10.4042\n",
            "Epoch 3 has loss 10.4292\n",
            "Epoch 4 has loss 10.3735\n",
            "Epoch 5 has loss 10.3174\n",
            "Epoch 6 has loss 10.2571\n",
            "Epoch 7 has loss 10.2644\n",
            "Epoch 8 has loss 10.2409\n",
            "Epoch 9 has loss 10.2128\n",
            "Epoch 10 has loss 10.1851\n",
            "Epoch 11 has loss 10.166\n",
            "Epoch 12 has loss 10.2041\n",
            "Epoch 13 has loss 10.1369\n",
            "Epoch 14 has loss 10.1202\n",
            "Epoch 15 has loss 10.1172\n",
            "Epoch 16 has loss 10.1132\n",
            "Epoch 17 has loss 10.1223\n",
            "Epoch 18 has loss 10.1192\n",
            "Epoch 19 has loss 10.0914\n",
            "Epoch 20 has loss 10.0741\n",
            "Epoch 21 has loss 10.07\n",
            "Epoch 22 has loss 10.0874\n",
            "Epoch 23 has loss 10.0406\n",
            "Epoch 24 has loss 10.0555\n",
            "Epoch 25 has loss 10.0313\n",
            "Epoch 26 has loss 10.0315\n",
            "Epoch 27 has loss 10.0224\n",
            "Epoch 28 has loss 9.9725\n",
            "Epoch 29 has loss 10.022\n",
            "Epoch 30 has loss 9.9843\n",
            "Epoch 31 has loss 9.9406\n",
            "Epoch 32 has loss 9.9841\n",
            "Epoch 33 has loss 9.9564\n",
            "Epoch 34 has loss 9.9761\n",
            "Epoch 35 has loss 9.9479\n",
            "Epoch 36 has loss 9.9605\n",
            "Epoch 37 has loss 9.9656\n",
            "Epoch 38 has loss 9.9287\n",
            "Epoch 39 has loss 9.901\n",
            "Epoch 40 has loss 9.9093\n",
            "Epoch 41 has loss 9.9068\n",
            "Epoch 42 has loss 9.9727\n",
            "Epoch 43 has loss 9.8756\n",
            "Epoch 44 has loss 9.8379\n",
            "Epoch 45 has loss 9.8954\n",
            "Epoch 46 has loss 9.787\n",
            "Epoch 47 has loss 9.7569\n",
            "Epoch 48 has loss 9.7666\n",
            "Epoch 49 has loss 9.7141\n",
            "📉 Epoch 0, Loss: 25.7219\n",
            "📉 Epoch 1, Loss: 22.9674\n",
            "📉 Epoch 2, Loss: 22.4614\n",
            "📉 Epoch 3, Loss: 20.9323\n",
            "📉 Epoch 4, Loss: 20.5185\n",
            "📉 Epoch 5, Loss: 20.3397\n",
            "📉 Epoch 6, Loss: 20.1924\n",
            "📉 Epoch 7, Loss: 20.0673\n",
            "📉 Epoch 8, Loss: 19.9982\n",
            "📉 Epoch 9, Loss: 19.9813\n",
            "📉 Epoch 10, Loss: 19.9497\n",
            "📉 Epoch 11, Loss: 19.9158\n",
            "📉 Epoch 12, Loss: 19.9141\n",
            "📉 Epoch 13, Loss: 19.8860\n",
            "📉 Epoch 14, Loss: 19.9140\n",
            "📉 Epoch 15, Loss: 19.8730\n",
            "📉 Epoch 16, Loss: 19.8714\n",
            "📉 Epoch 17, Loss: 19.8809\n",
            "📉 Epoch 18, Loss: 19.8944\n",
            "📉 Epoch 19, Loss: 19.8304\n",
            "📉 Epoch 20, Loss: 19.8320\n",
            "📉 Epoch 21, Loss: 19.8322\n",
            "📉 Epoch 22, Loss: 19.8483\n",
            "📉 Epoch 23, Loss: 19.8184\n",
            "📉 Epoch 24, Loss: 19.8159\n",
            "📉 Epoch 25, Loss: 19.7908\n",
            "📉 Epoch 26, Loss: 19.7878\n",
            "📉 Epoch 27, Loss: 19.7976\n",
            "📉 Epoch 28, Loss: 19.7871\n",
            "📉 Epoch 29, Loss: 19.7992\n",
            "📉 Epoch 30, Loss: 19.8100\n",
            "📉 Epoch 31, Loss: 19.7670\n",
            "📉 Epoch 32, Loss: 19.8245\n",
            "📉 Epoch 33, Loss: 19.7521\n",
            "📉 Epoch 34, Loss: 19.7252\n",
            "📉 Epoch 35, Loss: 19.7440\n",
            "📉 Epoch 36, Loss: 19.7288\n",
            "📉 Epoch 37, Loss: 19.7114\n",
            "📉 Epoch 38, Loss: 19.7160\n",
            "📉 Epoch 39, Loss: 19.7284\n",
            "📉 Epoch 40, Loss: 19.7587\n",
            "📉 Epoch 41, Loss: 19.6836\n",
            "📉 Epoch 42, Loss: 19.6759\n",
            "📉 Epoch 43, Loss: 19.7282\n",
            "📉 Epoch 44, Loss: 19.6788\n",
            "📉 Epoch 45, Loss: 19.7126\n",
            "📉 Epoch 46, Loss: 19.7113\n",
            "📉 Epoch 47, Loss: 19.6731\n",
            "📉 Epoch 48, Loss: 19.6818\n",
            "📉 Epoch 49, Loss: 19.6692\n",
            "  1751 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  3207 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  3635 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  4306 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  5182 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "🧬 Resultados SkipConnSAGE + BioBERT (GPT final v3):\n",
            "Hits@10: 0.0131\n",
            "Hits@20: 0.0240\n",
            "Hits@30: 0.0272\n",
            "Hits@40: 0.0323\n",
            "Hits@50: 0.0388\n",
            "Accuracy: 0.7800\n"
          ]
        }
      ],
      "source": [
        "model = SkipConnSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_layers=7,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_skipconn_bio_v4 = model(emb, adj_t)\n",
        "\n",
        "# Usar predictor con inyección final\n",
        "predictor_skipconn_bio_final_v4 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_skipconn_bio_v4.size(1),\n",
        "    hidden_channels=z_skipconn_bio_v4.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_skipconn_bio_final_v4.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_skipconn_bio_final_v4.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_skipconn_bio_final_v4.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_skipconn_bio_final_v4(\n",
        "            z_skipconn_bio_v4[edge[0]], z_skipconn_bio_v4[edge[1]],\n",
        "            embedding_biobert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_skipconn_bio_final_v4, embedding_biobert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_skipconn_bio_final_v4 = test(dummy_model, wrapped_predictor, z_skipconn_bio_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n🧬 Resultados SkipConnSAGE + BioBERT (GPT final v3):\")\n",
        "for k, v in results_skipconn_bio_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 13.0318\n",
            "Epoch 1 has loss 11.2332\n",
            "Epoch 2 has loss 10.5735\n",
            "Epoch 3 has loss 10.0855\n",
            "Epoch 4 has loss 10.172\n",
            "Epoch 5 has loss 9.5412\n",
            "Epoch 6 has loss 8.8327\n",
            "Epoch 7 has loss 8.2942\n",
            "Epoch 8 has loss 8.1128\n",
            "Epoch 9 has loss 7.7172\n",
            "Epoch 10 has loss 7.5277\n",
            "Epoch 11 has loss 7.4988\n",
            "Epoch 12 has loss 7.3473\n",
            "Epoch 13 has loss 7.3965\n",
            "Epoch 14 has loss 7.3553\n",
            "Epoch 15 has loss 7.2551\n",
            "Epoch 16 has loss 7.4189\n",
            "Epoch 17 has loss 7.2081\n",
            "Epoch 18 has loss 7.2823\n",
            "Epoch 19 has loss 7.2411\n",
            "Epoch 20 has loss 7.0908\n",
            "Epoch 21 has loss 7.0145\n",
            "Epoch 22 has loss 6.7453\n",
            "Epoch 23 has loss 6.7255\n",
            "Epoch 24 has loss 6.4255\n",
            "Epoch 25 has loss 6.4015\n",
            "Epoch 26 has loss 6.165\n",
            "Epoch 27 has loss 6.0119\n",
            "Epoch 28 has loss 6.0943\n",
            "Epoch 29 has loss 5.9623\n",
            "Epoch 30 has loss 5.989\n",
            "Epoch 31 has loss 6.1847\n",
            "Epoch 32 has loss 5.7996\n",
            "Epoch 33 has loss 5.7274\n",
            "Epoch 34 has loss 5.7203\n",
            "Epoch 35 has loss 5.9593\n",
            "Epoch 36 has loss 5.7905\n",
            "Epoch 37 has loss 5.6051\n",
            "Epoch 38 has loss 5.5206\n",
            "Epoch 39 has loss 5.5082\n",
            "Epoch 40 has loss 5.4948\n",
            "Epoch 41 has loss 5.4066\n",
            "Epoch 42 has loss 5.5104\n",
            "Epoch 43 has loss 5.4246\n",
            "Epoch 44 has loss 5.3573\n",
            "Epoch 45 has loss 5.2522\n",
            "Epoch 46 has loss 5.3309\n",
            "Epoch 47 has loss 5.2194\n",
            "Epoch 48 has loss 5.1471\n",
            "Epoch 49 has loss 5.2017\n",
            "  3267 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  4441 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  5655 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  7061 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  7659 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (sin GPT v3):\n",
            "Hits@10: 0.0245\n",
            "Hits@20: 0.0333\n",
            "Hits@30: 0.0424\n",
            "Hits@40: 0.0529\n",
            "Hits@50: 0.0574\n",
            "Accuracy: 0.8558\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_base_v4 = test(model, predictor, emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (sin GPT v3):\")\n",
        "for k, v in results_post_bio_base_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.8408\n",
            "Epoch 1 has loss 11.8\n",
            "Epoch 2 has loss 21.2238\n",
            "Epoch 3 has loss 9.7878\n",
            "Epoch 4 has loss 9.2757\n",
            "Epoch 5 has loss 8.5656\n",
            "Epoch 6 has loss 7.8301\n",
            "Epoch 7 has loss 7.6222\n",
            "Epoch 8 has loss 7.6951\n",
            "Epoch 9 has loss 7.5051\n",
            "Epoch 10 has loss 7.5846\n",
            "Epoch 11 has loss 7.4582\n",
            "Epoch 12 has loss 7.4093\n",
            "Epoch 13 has loss 7.4639\n",
            "Epoch 14 has loss 7.6276\n",
            "Epoch 15 has loss 7.3498\n",
            "Epoch 16 has loss 7.2598\n",
            "Epoch 17 has loss 7.2384\n",
            "Epoch 18 has loss 7.1711\n",
            "Epoch 19 has loss 7.3134\n",
            "Epoch 20 has loss 7.443\n",
            "Epoch 21 has loss 6.843\n",
            "Epoch 22 has loss 6.4811\n",
            "Epoch 23 has loss 6.4025\n",
            "Epoch 24 has loss 6.1474\n",
            "Epoch 25 has loss 5.9743\n",
            "Epoch 26 has loss 5.9387\n",
            "Epoch 27 has loss 5.8188\n",
            "Epoch 28 has loss 5.9219\n",
            "Epoch 29 has loss 5.7494\n",
            "Epoch 30 has loss 5.7633\n",
            "Epoch 31 has loss 5.535\n",
            "Epoch 32 has loss 6.0055\n",
            "Epoch 33 has loss 5.6346\n",
            "Epoch 34 has loss 5.5473\n",
            "Epoch 35 has loss 5.3721\n",
            "Epoch 36 has loss 5.2304\n",
            "Epoch 37 has loss 5.1713\n",
            "Epoch 38 has loss 5.3576\n",
            "Epoch 39 has loss 5.3763\n",
            "Epoch 40 has loss 5.2004\n",
            "Epoch 41 has loss 5.0968\n",
            "Epoch 42 has loss 5.0752\n",
            "Epoch 43 has loss 4.9695\n",
            "Epoch 44 has loss 5.0281\n",
            "Epoch 45 has loss 4.9668\n",
            "Epoch 46 has loss 4.9608\n",
            "Epoch 47 has loss 4.9047\n",
            "Epoch 48 has loss 4.8184\n",
            "Epoch 49 has loss 4.7874\n",
            "  1316 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2551 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2886 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3873 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4516 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (como input v3):\n",
            "Hits@10: 0.0099\n",
            "Hits@20: 0.0191\n",
            "Hits@30: 0.0216\n",
            "Hits@40: 0.0290\n",
            "Hits@50: 0.0338\n",
            "Accuracy: 0.8497\n"
          ]
        }
      ],
      "source": [
        "aug_emb = torch.cat([emb, embedding_biobert_tensor_v4.unsqueeze(0).expand(emb.size(0), -1)], dim=1)\n",
        "\n",
        "model = PostProcessSAGE(\n",
        "    in_channels=aug_emb.size(1),\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "predictor = NeuralLinkPredictor(\n",
        "    in_channels=hidden_dimension,\n",
        "    hidden_channels=hidden_dimension,\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "predictor.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()), lr=0.01\n",
        ")\n",
        "\n",
        "train(model, predictor, aug_emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "results_post_bio_input_v4 = test(model, predictor, aug_emb, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (como input v3):\")\n",
        "for k, v in results_post_bio_input_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 has loss 11.9557\n",
            "Epoch 1 has loss 11.8539\n",
            "Epoch 2 has loss 11.7855\n",
            "Epoch 3 has loss 12.3018\n",
            "Epoch 4 has loss 12.0865\n",
            "Epoch 5 has loss 11.4796\n",
            "Epoch 6 has loss 11.0789\n",
            "Epoch 7 has loss 10.37\n",
            "Epoch 8 has loss 10.2192\n",
            "Epoch 9 has loss 12.3417\n",
            "Epoch 10 has loss 10.404\n",
            "Epoch 11 has loss 9.41\n",
            "Epoch 12 has loss 10.0842\n",
            "Epoch 13 has loss 9.9925\n",
            "Epoch 14 has loss 11.3709\n",
            "Epoch 15 has loss 12.4451\n",
            "Epoch 16 has loss 11.7413\n",
            "Epoch 17 has loss 9.3836\n",
            "Epoch 18 has loss 9.5237\n",
            "Epoch 19 has loss 9.6501\n",
            "Epoch 20 has loss 9.6119\n",
            "Epoch 21 has loss 9.8585\n",
            "Epoch 22 has loss 9.647\n",
            "Epoch 23 has loss 9.5776\n",
            "Epoch 24 has loss 10.3781\n",
            "Epoch 25 has loss 10.5863\n",
            "Epoch 26 has loss 10.5048\n",
            "Epoch 27 has loss 10.0075\n",
            "Epoch 28 has loss 10.1961\n",
            "Epoch 29 has loss 11.0304\n",
            "Epoch 30 has loss 10.7362\n",
            "Epoch 31 has loss 11.8991\n",
            "Epoch 32 has loss 11.0224\n",
            "Epoch 33 has loss 10.4991\n",
            "Epoch 34 has loss 11.0092\n",
            "Epoch 35 has loss 11.848\n",
            "Epoch 36 has loss 14.5264\n",
            "Epoch 37 has loss 15.1523\n",
            "Epoch 38 has loss 14.8424\n",
            "Epoch 39 has loss 14.6401\n",
            "Epoch 40 has loss 18.0534\n",
            "Epoch 41 has loss 15.2807\n",
            "Epoch 42 has loss 13.1909\n",
            "Epoch 43 has loss 10.2964\n",
            "Epoch 44 has loss 10.1634\n",
            "Epoch 45 has loss 9.718\n",
            "Epoch 46 has loss 9.5302\n",
            "Epoch 47 has loss 9.7471\n",
            "Epoch 48 has loss 9.9635\n",
            "Epoch 49 has loss 10.0221\n",
            "📉 Epoch 0, Loss: 19.9697\n",
            "📉 Epoch 1, Loss: 17.0899\n",
            "📉 Epoch 2, Loss: 16.4740\n",
            "📉 Epoch 3, Loss: 15.9728\n",
            "📉 Epoch 4, Loss: 15.8646\n",
            "📉 Epoch 5, Loss: 15.8048\n",
            "📉 Epoch 6, Loss: 15.8105\n",
            "📉 Epoch 7, Loss: 15.8001\n",
            "📉 Epoch 8, Loss: 15.7988\n",
            "📉 Epoch 9, Loss: 15.7913\n",
            "📉 Epoch 10, Loss: 15.7421\n",
            "📉 Epoch 11, Loss: 15.7500\n",
            "📉 Epoch 12, Loss: 15.7213\n",
            "📉 Epoch 13, Loss: 15.7400\n",
            "📉 Epoch 14, Loss: 15.7058\n",
            "📉 Epoch 15, Loss: 15.7051\n",
            "📉 Epoch 16, Loss: 15.6953\n",
            "📉 Epoch 17, Loss: 15.6838\n",
            "📉 Epoch 18, Loss: 15.7063\n",
            "📉 Epoch 19, Loss: 15.6855\n",
            "📉 Epoch 20, Loss: 15.6656\n",
            "📉 Epoch 21, Loss: 15.6614\n",
            "📉 Epoch 22, Loss: 15.6856\n",
            "📉 Epoch 23, Loss: 15.6597\n",
            "📉 Epoch 24, Loss: 15.7054\n",
            "📉 Epoch 25, Loss: 15.6476\n",
            "📉 Epoch 26, Loss: 15.7594\n",
            "📉 Epoch 27, Loss: 15.8266\n",
            "📉 Epoch 28, Loss: 15.7965\n",
            "📉 Epoch 29, Loss: 15.6843\n",
            "📉 Epoch 30, Loss: 15.6801\n",
            "📉 Epoch 31, Loss: 15.6611\n",
            "📉 Epoch 32, Loss: 15.6470\n",
            "📉 Epoch 33, Loss: 15.6975\n",
            "📉 Epoch 34, Loss: 15.6014\n",
            "📉 Epoch 35, Loss: 15.5971\n",
            "📉 Epoch 36, Loss: 15.6504\n",
            "📉 Epoch 37, Loss: 15.6438\n",
            "📉 Epoch 38, Loss: 15.6669\n",
            "📉 Epoch 39, Loss: 15.6440\n",
            "📉 Epoch 40, Loss: 15.6513\n",
            "📉 Epoch 41, Loss: 15.6836\n",
            "📉 Epoch 42, Loss: 15.6595\n",
            "📉 Epoch 43, Loss: 15.6999\n",
            "📉 Epoch 44, Loss: 15.6390\n",
            "📉 Epoch 45, Loss: 15.6614\n",
            "📉 Epoch 46, Loss: 15.6543\n",
            "📉 Epoch 47, Loss: 15.6542\n",
            "📉 Epoch 48, Loss: 16.1892\n",
            "📉 Epoch 49, Loss: 15.9164\n",
            "  1898 positive edges (out of 133489) are ranked into the 10 (out of 101882) top-ranked negative edges\n",
            "  2548 positive edges (out of 133489) are ranked into the 20 (out of 101882) top-ranked negative edges\n",
            "  2973 positive edges (out of 133489) are ranked into the 30 (out of 101882) top-ranked negative edges\n",
            "  3597 positive edges (out of 133489) are ranked into the 40 (out of 101882) top-ranked negative edges\n",
            "  4121 positive edges (out of 133489) are ranked into the 50 (out of 101882) top-ranked negative edges\n",
            "\n",
            "📘 Resultados PostProcessSAGE + BioBERT (GPT final v3):\n",
            "Hits@10: 0.0142\n",
            "Hits@20: 0.0191\n",
            "Hits@30: 0.0223\n",
            "Hits@40: 0.0269\n",
            "Hits@50: 0.0309\n",
            "Accuracy: 0.7910\n"
          ]
        }
      ],
      "source": [
        "model = PostProcessSAGE(\n",
        "    in_channels=1,\n",
        "    hidden_dimension=hidden_dimension,\n",
        "    out_channels=hidden_dimension,\n",
        "    num_conv_layers=7,\n",
        "    num_linear_layers=4,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "train(model, DotProductLinkPredictor().to(device), emb, adj_t, split_edge, torch.nn.BCELoss(), optimizer, 64 * 1024, 50)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z_post_bio_v4 = model(emb, adj_t)\n",
        "\n",
        "predictor_post_bio_final_v4 = NeuralLinkPredictorWithGPT(\n",
        "    in_channels=z_post_bio_v4.size(1),\n",
        "    hidden_channels=z_post_bio_v4.size(1),\n",
        "    out_channels=1,\n",
        "    num_layers=4,\n",
        "    dropout=0.5,\n",
        "    gpt_emb_dim=embedding_biobert_tensor_v4.shape[0]\n",
        ").to(device)\n",
        "predictor_post_bio_final_v4.reset_parameters()\n",
        "optimizer = torch.optim.Adam(predictor_post_bio_final_v4.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "predictor_post_bio_final_v4.train()\n",
        "\n",
        "pos_edges = split_edge[\"train\"][\"edge\"]\n",
        "neg_edges = split_edge[\"train\"][\"edge_neg\"]\n",
        "\n",
        "edges = torch.cat([pos_edges, neg_edges], dim=0)\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_edges.size(0), device=device),\n",
        "    torch.zeros(neg_edges.size(0), device=device)\n",
        "], dim=0)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for perm in DataLoader(range(edges.size(0)), batch_size=64 * 1024, shuffle=True):\n",
        "        edge = edges[perm].t().to(device)\n",
        "        label = labels[perm]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = predictor_post_bio_final_v4(\n",
        "            z_post_bio_v4[edge[0]], z_post_bio_v4[edge[1]],\n",
        "            embedding_biobert_tensor_v4.unsqueeze(0).expand(edge.size(1), -1)\n",
        "        )\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"📉 Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "wrapped_predictor = WrappedPredictor(predictor_post_bio_final_v4, embedding_biobert_tensor_v4)\n",
        "dummy_model = DummyModel()\n",
        "\n",
        "results_post_bio_final_v4 = test(dummy_model, wrapped_predictor, z_post_bio_v4, adj_t, split_edge[\"valid\"], eval, 64 * 1024)\n",
        "\n",
        "print(\"\\n📘 Resultados PostProcessSAGE + BioBERT (GPT final v3):\")\n",
        "for k, v in results_post_bio_final_v4.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'display'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     22\u001b[39m column_order = [\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEmbedding Source\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrompt Type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGPT Injection\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPredictor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHits@10\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHits@20\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHits@30\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHits@40\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHits@50\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m ]\n\u001b[32m     26\u001b[39m df_summary = df_summary[column_order]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mdisplay\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m(df_summary)\n",
            "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'display'"
          ]
        }
      ],
      "source": [
        "third_prompt_type = \"structural descriptive expansion\"\n",
        "new_rows = [\n",
        "    # SAGEConv + BioBERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_bio_base_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sage_bio_input_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_sage_bio_final_v4},\n",
        "\n",
        "    # SkipConnSAGE + BioBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_base_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skipconn_bio_input_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_skipconn_bio_final_v4},\n",
        "\n",
        "    # PostProcessSAGE + BioBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_base_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + BioBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_bio_input_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + BioBERT\", \"Epochs\": 50, **results_post_bio_final_v4},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "# Aseguramos el orden de columnas\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "import IPython.display as display\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Embedding Source</th>\n",
              "      <th>Prompt Type</th>\n",
              "      <th>GPT Injection</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Hits@10</th>\n",
              "      <th>Hits@20</th>\n",
              "      <th>Hits@30</th>\n",
              "      <th>Hits@40</th>\n",
              "      <th>Hits@50</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.225854</td>\n",
              "      <td>0.250096</td>\n",
              "      <td>0.273229</td>\n",
              "      <td>0.9010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.117703</td>\n",
              "      <td>0.176794</td>\n",
              "      <td>0.197260</td>\n",
              "      <td>0.208849</td>\n",
              "      <td>0.219119</td>\n",
              "      <td>0.8920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAGEConv</td>\n",
              "      <td>classic</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + GPT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.032752</td>\n",
              "      <td>0.044678</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.072718</td>\n",
              "      <td>0.8618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.7420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + GPT</td>\n",
              "      <td>reasoning-based summary</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.7520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.018241</td>\n",
              "      <td>0.029823</td>\n",
              "      <td>0.037494</td>\n",
              "      <td>0.046296</td>\n",
              "      <td>0.051053</td>\n",
              "      <td>0.7711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>SkipConnSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>0.011649</td>\n",
              "      <td>0.017043</td>\n",
              "      <td>0.022818</td>\n",
              "      <td>0.7878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.7848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic + SciBERT</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>input</td>\n",
              "      <td>Neural</td>\n",
              "      <td>50</td>\n",
              "      <td>0.191836</td>\n",
              "      <td>0.270509</td>\n",
              "      <td>0.302190</td>\n",
              "      <td>0.324581</td>\n",
              "      <td>0.339279</td>\n",
              "      <td>0.8819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>PostProcessSAGE</td>\n",
              "      <td>classic</td>\n",
              "      <td>structural descriptive expansion</td>\n",
              "      <td>final</td>\n",
              "      <td>Neural + SciBERT</td>\n",
              "      <td>50</td>\n",
              "      <td>0.039441</td>\n",
              "      <td>0.173685</td>\n",
              "      <td>0.198293</td>\n",
              "      <td>0.216587</td>\n",
              "      <td>0.222790</td>\n",
              "      <td>0.8473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Model   Embedding Source                       Prompt Type  \\\n",
              "0          SAGEConv            classic                                 -   \n",
              "1          SAGEConv      classic + GPT           reasoning-based summary   \n",
              "2          SAGEConv            classic           reasoning-based summary   \n",
              "3      SkipConnSAGE            classic                                 -   \n",
              "4      SkipConnSAGE      classic + GPT           reasoning-based summary   \n",
              "..              ...                ...                               ...   \n",
              "76     SkipConnSAGE  classic + SciBERT  structural descriptive expansion   \n",
              "77     SkipConnSAGE            classic  structural descriptive expansion   \n",
              "78  PostProcessSAGE            classic                                 -   \n",
              "79  PostProcessSAGE  classic + SciBERT  structural descriptive expansion   \n",
              "80  PostProcessSAGE            classic  structural descriptive expansion   \n",
              "\n",
              "   GPT Injection         Predictor  Epochs   Hits@10   Hits@20   Hits@30  \\\n",
              "0              -            Neural      50  0.132565  0.171662  0.225854   \n",
              "1          input            Neural      50  0.117703  0.176794  0.197260   \n",
              "2          final      Neural + GPT      50  0.032752  0.044678  0.057555   \n",
              "3              -            Neural     100  0.000225  0.000494  0.000802   \n",
              "4          input            Neural     100  0.000809  0.001049  0.001288   \n",
              "..           ...               ...     ...       ...       ...       ...   \n",
              "76         input            Neural      50  0.018241  0.029823  0.037494   \n",
              "77         final  Neural + SciBERT      50  0.003274  0.007042  0.011649   \n",
              "78             -            Neural      50  0.000000  0.000000  0.000000   \n",
              "79         input            Neural      50  0.191836  0.270509  0.302190   \n",
              "80         final  Neural + SciBERT      50  0.039441  0.173685  0.198293   \n",
              "\n",
              "     Hits@40   Hits@50  Accuracy  \n",
              "0   0.250096  0.273229    0.9010  \n",
              "1   0.208849  0.219119    0.8920  \n",
              "2   0.062597  0.072718    0.8618  \n",
              "3   0.001041  0.001326    0.7420  \n",
              "4   0.001858  0.002277    0.7520  \n",
              "..       ...       ...       ...  \n",
              "76  0.046296  0.051053    0.7711  \n",
              "77  0.017043  0.022818    0.7878  \n",
              "78  0.000000  0.000000    0.7848  \n",
              "79  0.324581  0.339279    0.8819  \n",
              "80  0.216587  0.222790    0.8473  \n",
              "\n",
              "[81 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "new_rows = [\n",
        "    # SAGEConv + SciBERT\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sci_base_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_sci_input_v4},\n",
        "    {\"Model\": \"SAGEConv\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_sci_final_v4},\n",
        "\n",
        "    # SkipConnSAGE + SciBERT\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_sci_base_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_skip_sci_inpu_v4},\n",
        "    {\"Model\": \"SkipConnSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_skip_sci_final_v4},\n",
        "\n",
        "    # PostProcessSAGE + SciBERT\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": \"-\", \"GPT Injection\": \"-\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_base_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic + SciBERT\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"input\", \"Predictor\": \"Neural\", \"Epochs\": 50, **results_post_sci_input_v4},\n",
        "    {\"Model\": \"PostProcessSAGE\", \"Embedding Source\": \"classic\", \"Prompt Type\": third_prompt_type, \"GPT Injection\": \"final\", \"Predictor\": \"Neural + SciBERT\", \"Epochs\": 50, **results_post_sci_final_v4},\n",
        "]\n",
        "\n",
        "df_summary = pd.concat([df_summary, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "# Aseguramos el orden de columnas\n",
        "column_order = [\n",
        "    \"Model\", \"Embedding Source\", \"Prompt Type\", \"GPT Injection\",\n",
        "    \"Predictor\", \"Epochs\", \"Hits@10\", \"Hits@20\", \"Hits@30\", \"Hits@40\", \"Hits@50\", \"Accuracy\"\n",
        "]\n",
        "df_summary = df_summary[column_order]\n",
        "\n",
        "display.display(df_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary.to_csv(\"results_summary.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
